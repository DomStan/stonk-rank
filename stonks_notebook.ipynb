{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dstan\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:61: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import Int64Index as NumericIndex\n"
     ]
    }
   ],
   "source": [
    "# Utils\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from itertools import combinations\n",
    "from itertools import permutations\n",
    "from functools import partial\n",
    "\n",
    "# Data management\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Data fetching\n",
    "import yfinance as yf\n",
    "\n",
    "# Spread generation\n",
    "# from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stonk price data download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input ticker names by industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tickers_by_industry(industries=None, data_dir=None, filename=None):\n",
    "    '''\n",
    "    Read the CSV file containing all tickers and their subindustries and return tickers from the selected subindustries in a list.\n",
    "    \n",
    "    -Args:\n",
    "        industries (List(string)): if not given, return all tickers; else the list can contain:\n",
    "            'technology_hardware_and_equipment'\n",
    "            'software_and_services'\n",
    "            'media_and_entertainment'\n",
    "            'retailing'\n",
    "            'automobiles_and_components'\n",
    "            'semiconductors_and_semiconductor_equipment'\n",
    "            'health_care_equipment_and_services'\n",
    "            'banks'\n",
    "            'pharmaceuticals_biotechnology_and_life_sciences'\n",
    "            'food_and_staples_retailing'\n",
    "            'oil_gas_and_consumable_fuels'\n",
    "            'food_beverage_and_tobacco'\n",
    "            'telecommunication_services'\n",
    "            'consumer_durables_and_apparel'\n",
    "            'consumer_services'\n",
    "            'transportation'\n",
    "            'diversified_financials'\n",
    "            'utilities'\n",
    "            'capital_goods'\n",
    "            'insurance'\n",
    "            'chemicals'\n",
    "            'metals_and_mining'\n",
    "            'commercial_and_professional_services'\n",
    "            'containers_and_packaging'\n",
    "            'energy_equipment_and_services'\n",
    "            'construction_materials'\n",
    "            'paper_and_forest_products'\n",
    "    \n",
    "    -Returns:\n",
    "        tickers (pandas Series): list of selected ticker names\n",
    "    '''\n",
    "    filename = 'stonk_list.csv' if filename is None else filename\n",
    "    data_dir = 'data' if data_dir is None else data_dir\n",
    "    \n",
    "    path_to_csv = os.path.join(data_dir, filename)\n",
    "    stonk_list = pd.read_csv(path_to_csv)\n",
    "    return stonk_list.set_index('ticker') if industries is None else stonk_list[stonk_list['subindustry'].isin(industries)].set_index('ticker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_stonk_prices(stonk_list, period_years=3, date_from=None, date_to=None, interval='1d', source='yfinance', data_dir='data', proxy=False):    \n",
    "    '''\n",
    "    Returns historical price data for the selected stonks.\n",
    "\n",
    "    -Args:\n",
    "        stonk_list (List(string)): List of stonk identifiers as strings, case unsensitive\n",
    "        period_years (float): How many years of data to download until date_to, can be a floating point number\n",
    "    -Optional:\n",
    "        date_from (datetime): Start date for stonk data (use instead of period_years)\n",
    "        date_to (datetime): End date for stonk data\n",
    "        interval (string): Valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
    "        source (string): Where to source data from. Valid sources: yfinance\n",
    "        data_dir (string): Folder name where to output downloaded data\n",
    "        file_prefix (string): Prefix of CSV file containing downloaded data inside data_dir\n",
    "        proxy (boolean): Whether to use a proxy connection to avoid API limits/blocks\n",
    "                \n",
    "    -Returns:\n",
    "        stonks (Pandas Dataframe): Pandas Dataframe containing requested ticker prices\n",
    "    '''\n",
    "    \n",
    "    date_to = datetime.now() if date_to is None else date_to\n",
    "    date_from = date_to-(timedelta(days=int(365*period_years))) if date_from is None else date_from\n",
    "    \n",
    "    if source.lower() == 'yfinance':\n",
    "        stonks = yf.download(list(stonk_list), start=date_from, end=date_to, interval=interval, group_by='column', threads=True, rounding=True)['Adj Close']\n",
    "        stonks.dropna(axis=0, how='all', inplace=True)\n",
    "        stonks.sort_values(by='Date', inplace=True)\n",
    "        \n",
    "        stonks.index = pd.to_datetime(stonks.index).date\n",
    "        stonks.index.name = 'date'\n",
    "\n",
    "        clean_stonks = stonks.dropna(axis=1, how='all', thresh=int(len(stonks.index) * 0.99)).copy()\n",
    "        clean_stonks.dropna(axis=0, how='all', thresh=int(len(clean_stonks.columns) * 0.99), inplace=True)\n",
    "        \n",
    "        # Forward fill ticker columns (axis=0 for columns)\n",
    "        clean_stonks.fillna(axis=0, method='ffill', inplace=True)\n",
    "        \n",
    "        clean_stonks.dropna(axis=1, how='any', inplace=True)\n",
    "        \n",
    "        # Must be no NA values left\n",
    "        assert clean_stonks.isna().sum().sum() == 0\n",
    "    else:\n",
    "        raise ValueError('Unsupported data source')\n",
    "        \n",
    "    def stonks_to_csv(stonks, clean):\n",
    "        from_date_string = stonks.index[0]\n",
    "        to_date_string = stonks.index[-1]\n",
    "\n",
    "        filename = 'stonks_{from_date}_to_{to_date}.csv'.format(from_date=from_date_string, to_date=to_date_string)\n",
    "        \n",
    "        if clean:\n",
    "            filename = 'clean_' + filename\n",
    "            \n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "\n",
    "        stonks.to_csv(path_or_buf=file_path, header=True, index=True, na_rep='NaN')\n",
    "    \n",
    "    stonks_to_csv(stonks, clean=False)\n",
    "    stonks_to_csv(clean_stonks, clean=True)\n",
    "    \n",
    "    return (stonks, clean_stonks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock price data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stonk_data(date_from, date_to, clean=True, date_index=False, data_dir=None):\n",
    "    data_dir = 'data' if data_dir is None else data_dir\n",
    "    data_prefix = 'clean_stonks' if clean else 'stonks'\n",
    "    \n",
    "    path = os.path.join(data_dir, '{}_{}_to_{}.csv'.format(data_prefix, date_from, date_to))\n",
    "    stonks = pd.read_csv(path, header=0, index_col=0)\n",
    "    \n",
    "    if clean:\n",
    "        assert stonks.isna().sum().sum() == 0\n",
    "    \n",
    "    if date_index:\n",
    "        return stonks\n",
    "    else:\n",
    "        return stonks.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stonk_data_by_industry(date_from, date_to, clean=True, date_index=False, industries=None, stonk_list_filename=None, data_dir=None):\n",
    "    '''\n",
    "    Read the CSV file containing all stonk price data and return the tickers from the selected subindustries.\n",
    "    \n",
    "    -Args:\n",
    "        industries (List(string)): if not given, return all tickers; else the list can contain:\n",
    "            'technology_hardware_and_equipment'\n",
    "            'software_and_services'\n",
    "            'media_and_entertainment'\n",
    "            'retailing'\n",
    "            'automobiles_and_components'\n",
    "            'semiconductors_and_semiconductor_equipment'\n",
    "            'health_care_equipment_and_services'\n",
    "            'banks'\n",
    "            'pharmaceuticals_biotechnology_and_life_sciences'\n",
    "            'food_and_staples_retailing'\n",
    "            'oil_gas_and_consumable_fuels'\n",
    "            'food_beverage_and_tobacco'\n",
    "            'telecommunication_services'\n",
    "            'consumer_durables_and_apparel'\n",
    "            'consumer_services'\n",
    "            'transportation'\n",
    "            'diversified_financials'\n",
    "            'utilities'\n",
    "            'capital_goods'\n",
    "            'insurance'\n",
    "            'chemicals'\n",
    "            'metals_and_mining'\n",
    "            'commercial_and_professional_services'\n",
    "            'containers_and_packaging'\n",
    "            'energy_equipment_and_services'\n",
    "            'construction_materials'\n",
    "            'paper_and_forest_products'\n",
    "    \n",
    "    -Returns:\n",
    "        stonks (pandas DataFrame): list of selected tickers' price data\n",
    "    '''\n",
    "    all_stonks = read_stonk_data(date_from, date_to, date_index=date_index, data_dir=data_dir, clean=clean)\n",
    "    \n",
    "    if industries is None or not industries:\n",
    "        return all_stonks\n",
    "    else: \n",
    "        all_tickers = get_tickers_by_industry(industries=None, data_dir=data_dir, filename=stonk_list_filename)\n",
    "        all_stonks = all_stonks.join(all_tickers, how='inner')\n",
    "        return all_stonks[all_stonks['subindustry'].isin(industries)].drop(columns='subindustry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make combinations with numpy\n",
    "def combine_stonk_pairs(stonks_prices):\n",
    "    # All ticker names must be unique\n",
    "    assert all(stonks_prices.index.unique() == stonks_prices.index)\n",
    "    assert(len(stonks_prices) < 300)\n",
    "    \n",
    "    combs = np.asarray(list(combinations(stonks_prices.index.unique(), 2)))\n",
    "    \n",
    "    return stonks_prices.loc[combs[:, 0]], stonks_prices.loc[combs[:, 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Linear regression residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_residuals_many(X, Y):\n",
    "    '''\n",
    "    Vectorized calculation of residuals from many univariate linear regressions.\n",
    "        Args:\n",
    "        - X (numpy array of shape (n_pairs, d_time)): matrix of LR inputs X, each row represents a different regression, corresponding to the same rows in Y\n",
    "        - Y (numpy array of shape (n_pairs, d_time)): matrix of LR inputs Y, each row represents a different regression, corresponding to the same rows in X\n",
    "        Returns:\n",
    "        - residuals (numpy array of shape (n_pairs, d_time)): matrix of resulting residuals between vectorized pairs of X and Y\n",
    "        - betas (numpy array of shape (n_pairs, 1)): beta coefficients for each linear regression\n",
    "        - Y_hat (numpy array of shape (n_pairs, d_time)): predictions using X\n",
    "    '''\n",
    "    # Stack 2D matrices into 3D matrices\n",
    "    X = X.reshape(np.shape(X)[0], np.shape(X)[1], -1)\n",
    "    Y = Y.reshape(np.shape(Y)[0], np.shape(Y)[1], -1)\n",
    "    \n",
    "    # Add bias/intercept in the form (Xi, 1)\n",
    "    Z = np.concatenate([X, np.ones((np.shape(X)[0], np.shape(X)[1], 1))], axis=2)\n",
    "    \n",
    "    # Save the transpose as it's used a couple of times\n",
    "    Z_t = Z.transpose(0, 2, 1)\n",
    "    \n",
    "    # Linear Regression equation solutions w.r.t. weight matrix\n",
    "    # W contains (beta_coef, a_intercept) for each regression\n",
    "    W = np.matmul(np.linalg.inv(np.matmul(Z_t, Z)),  np.matmul(Z_t, Y))\n",
    "    \n",
    "    # Predictions and residuals\n",
    "    # Y_hat = np.matmul(Z, W).round(2)\n",
    "    residuals = (Y - np.matmul(Z, W)).round(2)\n",
    "    \n",
    "    # Y_hat returned for debugging purposes\n",
    "    # return (residuals[:, :, 0], W[:, 0, 0], Y_hat[:, :, 0])\n",
    "    return (residuals[:, :, 0], W[:, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rolling_residuals(X, Y, l_reg, l_roll, dt, write_csv=False, data_dir='data'):\n",
    "    '''\n",
    "    Calculates rolling window residuals in vectorized form. Returns the result as an array that repeats each ticker for the number of regressions calculated.\n",
    "    For example, if the inputs are (Pair A, Pair B, Pair C) and l_roll / dt = 3, then the returned results will have the form as follows:\n",
    "    (Pair A, Pair A, Pair A, Pair B, Pair B, Pair B, Pair C, Pair C, Pair C)\n",
    "    Works best when l_reg and l_roll are integers.\n",
    "        Args:\n",
    "        - X, Y (DataFrame of shape (n_pairs, >= l_reg + l_roll)): matrix of LR inputs X, Y; each row containing at least the complete data period for rolling regressions (can be longer)\n",
    "        - l_reg (float): length of each LR to calculate residuals, in years; will be multiplied by the adjusted number of days in a trading year\n",
    "        - l_roll (float): length of rolling window, in years; will be multipled by the adjusted number of days in a trading year\n",
    "        - dt (int): rolling window step size, in trading days; total trading year days will be reduced to be divisible by dt (by not more than the value of dt)\n",
    "        Returns:\n",
    "        - residuals (numpy array of shape (n_pairs * (l_roll/dt)+1, l_reg + l_roll)): matrix of resulting residuals between vectorized pairs of X and Y\n",
    "        - betas (numpy array of shape (n_pairs * (l_roll/dt)+1, 1)): beta coefficients for each linear regression\n",
    "        - Y_hat (numpy array of shape (n_pairs * (l_roll/dt)+1, l_reg + l_roll)): predictions using X\n",
    "    '''\n",
    "    \n",
    "    _DAYS_IN_TRADING_YEAR = 252\n",
    "    \n",
    "    # Adjust days in a year so that the number is divisible by dt\n",
    "    _DAYS_IN_TRADING_YEAR = _DAYS_IN_TRADING_YEAR - (_DAYS_IN_TRADING_YEAR % dt)\n",
    "    l_reg_days = int(_DAYS_IN_TRADING_YEAR * l_reg)\n",
    "    l_roll_days = int(_DAYS_IN_TRADING_YEAR * l_roll)\n",
    "    total_days = l_reg_days + l_roll_days\n",
    "    \n",
    "    # Number of regressions for each ticker\n",
    "    n_windows = (l_roll_days // dt) + 1\n",
    "    \n",
    "    # Number of tickers\n",
    "    n_x = X.shape[0]\n",
    "    \n",
    "    # Take the dates, create an empty array for windowed dates\n",
    "    date_index = X.columns[-total_days:]\n",
    "    date_index_windowed = np.empty(shape=(n_x*n_windows, 2), dtype='O')\n",
    "    \n",
    "    # Repeat each ticker name times n_windows\n",
    "    X_index = np.repeat(X.index, n_windows)\n",
    "    Y_index = np.repeat(Y.index, n_windows)\n",
    "    \n",
    "    # X and Y must have the same dates\n",
    "    assert np.array_equal(X.columns, Y.columns)\n",
    "    \n",
    "    X = X.to_numpy(dtype=np.float32)\n",
    "    Y = Y.to_numpy(dtype=np.float32)\n",
    "    \n",
    "    # Rolling window length must be divisible by dt\n",
    "    assert (l_roll_days % dt) == 0\n",
    "    \n",
    "    # There has to be enough days' worth of data in X (and Y) and their shapes must match\n",
    "    assert X.shape == Y.shape and X.shape[1] >= total_days\n",
    "    \n",
    "    # Take the total_days from the end of the arrays (most recent days first, oldest days at the end are cut off)\n",
    "    X = X[:, -total_days:]\n",
    "    Y = Y[:, -total_days:]\n",
    "    \n",
    "    # Create empty arrays that will contain windowed slices of our data\n",
    "    X_windows = np.empty(shape=(n_x*n_windows, l_reg_days))\n",
    "    Y_windows = np.empty(shape=(n_x*n_windows, l_reg_days))\n",
    "    \n",
    "    # Take windowed slices and place them into the created empty arrays\n",
    "    for n in range(n_x):\n",
    "        for i in range(n_windows):\n",
    "            n_i = (n*n_windows)+i\n",
    "            t_i = i*dt\n",
    "            t_y = t_i + l_reg_days\n",
    "            \n",
    "            X_windows[n_i] = X[n, t_i:t_y]\n",
    "            Y_windows[n_i] = Y[n, t_i:t_y]\n",
    "            date_index_windowed[n_i, 0] = date_index[t_i]\n",
    "            date_index_windowed[n_i, 1] = date_index[t_y-1]\n",
    "    \n",
    "    # Make sure we've got the windowing dimensions right\n",
    "    assert X_windows.shape == (n_x*n_windows, l_reg_days) and Y_windows.shape == (n_x*n_windows, l_reg_days)\n",
    "    \n",
    "    # Sanity checks\n",
    "    assert all([\n",
    "        X[0, -1] == X_windows[n_windows-1, -1],\n",
    "        Y[0, -1] == Y_windows[n_windows-1, -1],\n",
    "        X[-1, -1] == X_windows[-1, -1],\n",
    "        Y[-1, -1] == Y_windows[-1, -1],\n",
    "    ])\n",
    "    \n",
    "    # Construct ticker pair index column\n",
    "    pair_index = np.array(pd.DataFrame(np.array([Y_index, X_index])).apply('_'.join, axis=0, raw=True))\n",
    "    \n",
    "    # Construct regression date range index column\n",
    "    date_index = np.array(pd.DataFrame(np.array([date_index_windowed[:, 0], date_index_windowed[:, 1]])).apply('_'.join, axis=0, raw=True))\n",
    "    \n",
    "    # Lengths of indexes must match\n",
    "    assert len(pair_index) == len(date_index)\n",
    "    \n",
    "    # Calculate and return the residuals\n",
    "    res, betas = get_residuals_many(X_windows, Y_windows)\n",
    "    \n",
    "    res = pd.DataFrame(res, index=pair_index)\n",
    "    res.insert(0, 'dates', date_index)\n",
    "    betas = pd.DataFrame(betas, index=pair_index)\n",
    "    betas.insert(0, 'dates', date_index)\n",
    "    \n",
    "    if write_csv:\n",
    "        time = datetime.now().time()\n",
    "        res.to_csv(os.path.join(data_dir, time.strftime('residuals_%H%M%S.csv')), header=False, index=True)\n",
    "        betas.to_csv(os.path.join(data_dir, time.strftime('betas_%H%M%S.csv')), header=False, index=True)\n",
    "        pd.Series(date_index).to_csv(os.path.join(data_dir, time.strftime('dates_%H%M%S.csv')), header=False, index=False)\n",
    "    \n",
    "    return res, betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADF testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adfs(residuals, adf_regression):\n",
    "    # Get ADF test p-values for each row of the residuals array. No autolag (maxlag always used)\n",
    "    return np.apply_along_axis(lambda x: adfuller(x, regression=adf_regression, autolag=None)[1], axis=1, arr=residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregate_adfs(residuals, betas=None, cutoff=0.1, adf_regression='c', write_csv=False, data_dir='data'):\n",
    "    # Get ADF p-values\n",
    "    adfs = get_adfs(residuals.drop(columns='dates').to_numpy(dtype=np.float32), adf_regression=adf_regression).reshape((-1, 1))\n",
    "    \n",
    "    # Add ones to ADF values where betas are negative, if betas are given\n",
    "    if betas is not None:\n",
    "        # Must be the same number of columns\n",
    "        assert adfs.shape[0] == betas.shape[0]\n",
    "        # Residuals and betas must have the same index names\n",
    "        assert np.all(residuals.index == betas.index)\n",
    "        # Add 1's to p-values where betas are negative\n",
    "        adfs = adfs + (betas[0].to_numpy() <= 0).reshape((-1, 1))\n",
    "        \n",
    "    # Make a copy for returning, CSV output\n",
    "    adfs_raw = pd.DataFrame(adfs.copy(), index=residuals.index)\n",
    "    \n",
    "    # All unique ticker pairs, in original order\n",
    "    unique_pairs = residuals.index.unique()\n",
    "    \n",
    "    # Number of regressions for one pair\n",
    "    pairs_per_index = len(residuals) // len(unique_pairs)\n",
    "    \n",
    "    # Reshape into a 3D array for averaging ADF values along the second axis\n",
    "    adfs = adfs.reshape((len(unique_pairs), pairs_per_index, 1))\n",
    "    \n",
    "    # Takes cutoff, averages along the pairs_per_index (second) axis\n",
    "    adfs = (adfs <= cutoff).mean(axis=1)\n",
    "    \n",
    "    # Probably always true, but just in case\n",
    "    assert adfs.shape[0] == len(unique_pairs)\n",
    "        \n",
    "    # Back to a DataFrame with named indexes\n",
    "    adfs = pd.DataFrame(adfs, index=unique_pairs)\n",
    "    \n",
    "    # Output to CSV\n",
    "    if write_csv:\n",
    "        time = datetime.now().time()\n",
    "        adfs.to_csv(os.path.join(data_dir, time.strftime('adfs_%H%M%S.csv')), header=False, index=True)\n",
    "        adfs_raw.to_csv(os.path.join(data_dir, time.strftime('adfs-raw_%H%M%S.csv')), header=False, index=True)\n",
    "        \n",
    "    return adfs, adfs_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardized residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_pairs(pairs):\n",
    "    # Get unique ticker pairs, in preserved order\n",
    "    unique_pairs = pairs.index.unique()\n",
    "    \n",
    "    # Number of samples per ticker pair\n",
    "    pairs_per_index = len(pairs) // len(unique_pairs)\n",
    "    \n",
    "    # Must be an equal number of pairs per index\n",
    "    assert pairs_per_index * len(unique_pairs) == len(pairs)\n",
    "    \n",
    "    # Slice taking only the last regression for each ticker pair\n",
    "    last_pairs = pairs.iloc[pairs_per_index-1:len(pairs):pairs_per_index].copy()\n",
    "    \n",
    "    # Make sure we got the slices right\n",
    "    assert np.all(last_pairs.index == unique_pairs) and np.all(pairs.iloc[-1] == last_pairs.iloc[-1])\n",
    "        \n",
    "    return last_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_standardized_residuals(residuals, write_csv=False, data_dir='data'):\n",
    "    # Dates aren't needed anymore, as we're using the latest regressions\n",
    "    residuals = residuals.drop(columns='dates')\n",
    "    \n",
    "    # Get the last regression for each spread\n",
    "    last_reg_pairs = get_last_pairs(residuals)\n",
    "    \n",
    "    # Get unique ticker pairs\n",
    "    unique_pairs = last_reg_pairs.index\n",
    "    \n",
    "    # Convert to numpy\n",
    "    last_reg_pairs = last_reg_pairs.to_numpy(dtype=np.float32)\n",
    "    \n",
    "    # Standardize\n",
    "    last_reg_pairs = (last_reg_pairs - last_reg_pairs.mean(axis=1, keepdims=True)) / last_reg_pairs.std(axis=1, keepdims=True)\n",
    "    \n",
    "    # Back to a DataFrame with named indexes\n",
    "    last_reg_pairs = pd.DataFrame(last_reg_pairs, index=unique_pairs)\n",
    "    \n",
    "    # Output to CSV\n",
    "    if write_csv:\n",
    "        time = datetime.now().time()\n",
    "        last_reg_pairs.to_csv(os.path.join(data_dir, time.strftime('std-residuals_%H%M%S.csv')), header=False, index=True)\n",
    "        \n",
    "    return last_reg_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean residual magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_residual_magnitude(std_residuals, dt):\n",
    "    # Assume there is enough days' worth of data for averaging over dt days\n",
    "    assert std_residuals.shape[1] >= dt\n",
    "    \n",
    "    # Select the last dt days from the right\n",
    "    std_residuals = std_residuals.to_numpy(dtype=np.float32)[:, -dt:]\n",
    "    \n",
    "    # Take the absolute maximum for each day, over all tickers, mean over the results\n",
    "    mean_magnitude = np.abs(std_residuals).max(axis=0).mean()\n",
    "    \n",
    "    return mean_magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trade returns test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spreads_returns(prices_X, prices_Y, betas_YX, buy_X):\n",
    "\n",
    "    # Save spread indexes for later\n",
    "    pairs_indexes = betas_YX.index\n",
    "    \n",
    "    # Take numpy betas\n",
    "    betas_YX = betas_YX.drop(columns='dates').to_numpy()\n",
    "\n",
    "    # Take numpy buy list for X\n",
    "    buy_X = buy_X.values\n",
    "\n",
    "    # Sanity checks\n",
    "    assert all([\n",
    "        prices_X.shape == prices_Y.shape,\n",
    "        len(buy_X) == betas_YX.shape[0]\n",
    "    ])\n",
    "\n",
    "    # Prices to numpy\n",
    "    prices_X = prices_X.to_numpy()\n",
    "    prices_Y = prices_Y.to_numpy()\n",
    "\n",
    "    # Save entering prices at t=0\n",
    "    initial_prices_X = prices_X[:, [0]].copy()\n",
    "    initial_prices_Y = prices_Y[:, [0]].copy()\n",
    "\n",
    "    # Initial proportional values of trades at t=0, X prices scaled by beta\n",
    "    initial_trade_values = (initial_prices_X * betas_YX) + initial_prices_Y\n",
    "\n",
    "    # Returns for X, Y trades each day. X prices scaled by beta\n",
    "    returns_X = betas_YX * (prices_X - initial_prices_X)\n",
    "    returns_Y = prices_Y - initial_prices_Y\n",
    "\n",
    "    # Negate short trades\n",
    "    returns_X[~buy_X] = -returns_X[~buy_X]\n",
    "    returns_Y[buy_X] = -returns_Y[buy_X]\n",
    "\n",
    "    # Add the trade returns for X, Y, divide by initial investment values to get profit/loss %\n",
    "    trade_returns = (returns_X + returns_Y) / initial_trade_values\n",
    "\n",
    "    # Back to dataframe with indexes\n",
    "    trade_returns = pd.DataFrame(trade_returns, index=pairs_indexes)\n",
    "    \n",
    "    return trade_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data collection pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collection(stonk_prices, industries, l_reg, l_roll, dt, adf_pass_cutoff=0.1, trade_length_months=3, trading_interval_months=1):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data collection pipeline step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collection_step(X, Y, l_reg, l_roll, dt, adf_pval_cutoff, adf_pass_rate_filter, trade_length_months):\n",
    "    assert X.shape == Y.shape\n",
    "\n",
    "    _DAYS_IN_TRADING_MONTH = 20\n",
    "    output = {}\n",
    "\n",
    "    trade_length_days = trade_length_months*_DAYS_IN_TRADING_MONTH\n",
    "\n",
    "    X_until_T = X.iloc[:, :-trade_length_days]\n",
    "    Y_until_T = Y.iloc[:, :-trade_length_days]\n",
    "\n",
    "    X_from_T = X.iloc[:, -trade_length_days-1:]\n",
    "    Y_from_T = Y.iloc[:, -trade_length_days-1:]\n",
    "\n",
    "    # X and Y dimensions must match\n",
    "    assert X_from_T.shape == Y_from_T.shape and X_until_T.shape == Y_until_T.shape\n",
    "\n",
    "    # Check whether enough data was given\n",
    "    assert X_from_T.shape[1] == trade_length_days+1 and X.shape[1] == X_until_T.shape[1] + X_from_T.shape[1] - 1\n",
    "\n",
    "    residuals, betas = get_rolling_residuals(X=X_until_T, Y=Y_until_T, l_reg=l_reg, l_roll=l_roll, dt=dt)\n",
    "\n",
    "    adfs, adfs_raw = get_aggregate_adfs(residuals, betas=betas, cutoff=adf_pval_cutoff)\n",
    "\n",
    "    std_residuals = get_standardized_residuals(residuals)\n",
    "\n",
    "    last_betas = get_last_pairs(betas)\n",
    "\n",
    "    assert np.all(std_residuals.index == last_betas.index)\n",
    "    assert np.all(adfs.index == std_residuals.index)\n",
    "\n",
    "    # Select spreads that are above the specified ADF pass rate\n",
    "    selected = (adfs >= adf_pass_rate_filter).values\n",
    "\n",
    "    selected_std_residuals = std_residuals[selected]\n",
    "\n",
    "    selected_residuals_max_mean = get_mean_residual_magnitude(selected_std_residuals, dt=trade_length_days)\n",
    "\n",
    "    selected_betas = last_betas[selected]\n",
    "\n",
    "    selected_buys_X = selected_std_residuals.iloc[:, -1] > 0\n",
    "\n",
    "    selected_X_from_T = X_from_T[selected]\n",
    "    selected_Y_from_T = Y_from_T[selected]\n",
    "\n",
    "    \n",
    "    selected_trade_returns = get_spreads_returns(prices_X=selected_X_from_T, prices_Y=selected_Y_from_T, betas_YX=selected_betas, buy_X=selected_buys_X)\n",
    "\n",
    "    output['adf_residuals'] = adfs[selected][0].values\n",
    "    output['last_residual'] = selected_std_residuals.iloc[:, -1].values\n",
    "    output['ticker_x'] = selected_X_from_T.index\n",
    "    output['ticker_y'] = selected_Y_from_T.index\n",
    "    output['return_one_month'] = selected_trade_returns.iloc[:, 20].values\n",
    "    output['beta'] = selected_betas[0].values\n",
    "    output['residual_mean_max'] = np.full(len(selected_std_residuals), selected_residuals_max_mean)\n",
    "    output['trade_date'] = np.full(len(selected_std_residuals), X_from_T.columns[0])\n",
    "\n",
    "    if trade_length_months > 1:\n",
    "        output['return_two_month'] = selected_trade_returns.iloc[:, 40].values\n",
    "    else:\n",
    "        output['return_two_month'] = np.full(len(selected_std_residuals.index), np.nan)\n",
    "\n",
    "    if trade_length_months > 2:\n",
    "        output['return_three_month'] = selected_trade_returns.iloc[:, 60].values\n",
    "    else:\n",
    "        output['return_three_month'] = np.full(len(selected_std_residuals.index), np.nan)\n",
    "\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = data_collection_step(X, Y, 3, 1, 20, 0.1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, 3, 1, 20, 0.1, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_reg, l_roll, dt, adf_pass_cutoff, trade_length_months = (3, 1, 20, 0.1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2017-04-17</th>\n",
       "      <th>2017-04-18</th>\n",
       "      <th>2017-04-19</th>\n",
       "      <th>2017-04-20</th>\n",
       "      <th>2017-04-21</th>\n",
       "      <th>2017-04-24</th>\n",
       "      <th>2017-04-25</th>\n",
       "      <th>2017-04-26</th>\n",
       "      <th>2017-04-27</th>\n",
       "      <th>2017-04-28</th>\n",
       "      <th>...</th>\n",
       "      <th>2022-01-03</th>\n",
       "      <th>2022-01-04</th>\n",
       "      <th>2022-01-05</th>\n",
       "      <th>2022-01-06</th>\n",
       "      <th>2022-01-07</th>\n",
       "      <th>2022-01-10</th>\n",
       "      <th>2022-01-11</th>\n",
       "      <th>2022-01-12</th>\n",
       "      <th>2022-01-13</th>\n",
       "      <th>2022-01-14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ADBE</th>\n",
       "      <td>129.99</td>\n",
       "      <td>129.81</td>\n",
       "      <td>130.22</td>\n",
       "      <td>131.42</td>\n",
       "      <td>131.52</td>\n",
       "      <td>132.89</td>\n",
       "      <td>133.49</td>\n",
       "      <td>132.79</td>\n",
       "      <td>133.38</td>\n",
       "      <td>133.74</td>\n",
       "      <td>...</td>\n",
       "      <td>564.37</td>\n",
       "      <td>554.00</td>\n",
       "      <td>514.43</td>\n",
       "      <td>514.12</td>\n",
       "      <td>510.70</td>\n",
       "      <td>525.83</td>\n",
       "      <td>529.89</td>\n",
       "      <td>532.37</td>\n",
       "      <td>516.90</td>\n",
       "      <td>520.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>91.66</td>\n",
       "      <td>91.72</td>\n",
       "      <td>91.85</td>\n",
       "      <td>92.72</td>\n",
       "      <td>92.41</td>\n",
       "      <td>93.71</td>\n",
       "      <td>94.49</td>\n",
       "      <td>94.36</td>\n",
       "      <td>94.63</td>\n",
       "      <td>94.03</td>\n",
       "      <td>...</td>\n",
       "      <td>242.80</td>\n",
       "      <td>243.93</td>\n",
       "      <td>241.85</td>\n",
       "      <td>240.19</td>\n",
       "      <td>237.66</td>\n",
       "      <td>231.73</td>\n",
       "      <td>234.11</td>\n",
       "      <td>233.47</td>\n",
       "      <td>230.78</td>\n",
       "      <td>227.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADS</th>\n",
       "      <td>223.10</td>\n",
       "      <td>222.63</td>\n",
       "      <td>225.10</td>\n",
       "      <td>243.78</td>\n",
       "      <td>242.75</td>\n",
       "      <td>244.02</td>\n",
       "      <td>246.02</td>\n",
       "      <td>245.38</td>\n",
       "      <td>243.71</td>\n",
       "      <td>233.50</td>\n",
       "      <td>...</td>\n",
       "      <td>68.43</td>\n",
       "      <td>70.79</td>\n",
       "      <td>68.81</td>\n",
       "      <td>69.14</td>\n",
       "      <td>70.20</td>\n",
       "      <td>71.60</td>\n",
       "      <td>72.08</td>\n",
       "      <td>71.73</td>\n",
       "      <td>74.66</td>\n",
       "      <td>72.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADSK</th>\n",
       "      <td>85.99</td>\n",
       "      <td>86.41</td>\n",
       "      <td>87.93</td>\n",
       "      <td>88.41</td>\n",
       "      <td>88.58</td>\n",
       "      <td>90.14</td>\n",
       "      <td>90.32</td>\n",
       "      <td>90.23</td>\n",
       "      <td>90.63</td>\n",
       "      <td>90.07</td>\n",
       "      <td>...</td>\n",
       "      <td>283.72</td>\n",
       "      <td>278.19</td>\n",
       "      <td>264.32</td>\n",
       "      <td>264.11</td>\n",
       "      <td>262.32</td>\n",
       "      <td>262.39</td>\n",
       "      <td>270.63</td>\n",
       "      <td>269.60</td>\n",
       "      <td>260.17</td>\n",
       "      <td>259.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AKAM</th>\n",
       "      <td>58.82</td>\n",
       "      <td>58.95</td>\n",
       "      <td>59.68</td>\n",
       "      <td>60.20</td>\n",
       "      <td>60.28</td>\n",
       "      <td>61.39</td>\n",
       "      <td>61.80</td>\n",
       "      <td>62.24</td>\n",
       "      <td>59.75</td>\n",
       "      <td>60.94</td>\n",
       "      <td>...</td>\n",
       "      <td>117.51</td>\n",
       "      <td>116.95</td>\n",
       "      <td>115.48</td>\n",
       "      <td>111.88</td>\n",
       "      <td>110.54</td>\n",
       "      <td>112.65</td>\n",
       "      <td>113.42</td>\n",
       "      <td>114.11</td>\n",
       "      <td>112.91</td>\n",
       "      <td>112.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEXT</th>\n",
       "      <td>13.55</td>\n",
       "      <td>14.01</td>\n",
       "      <td>14.04</td>\n",
       "      <td>13.81</td>\n",
       "      <td>13.44</td>\n",
       "      <td>13.74</td>\n",
       "      <td>13.72</td>\n",
       "      <td>13.81</td>\n",
       "      <td>14.63</td>\n",
       "      <td>14.84</td>\n",
       "      <td>...</td>\n",
       "      <td>10.00</td>\n",
       "      <td>9.67</td>\n",
       "      <td>8.95</td>\n",
       "      <td>9.00</td>\n",
       "      <td>9.03</td>\n",
       "      <td>9.07</td>\n",
       "      <td>9.44</td>\n",
       "      <td>9.33</td>\n",
       "      <td>8.95</td>\n",
       "      <td>8.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZEN</th>\n",
       "      <td>28.41</td>\n",
       "      <td>28.62</td>\n",
       "      <td>28.10</td>\n",
       "      <td>28.37</td>\n",
       "      <td>28.33</td>\n",
       "      <td>28.64</td>\n",
       "      <td>28.68</td>\n",
       "      <td>28.40</td>\n",
       "      <td>28.55</td>\n",
       "      <td>28.75</td>\n",
       "      <td>...</td>\n",
       "      <td>103.53</td>\n",
       "      <td>102.25</td>\n",
       "      <td>96.87</td>\n",
       "      <td>98.36</td>\n",
       "      <td>97.92</td>\n",
       "      <td>99.99</td>\n",
       "      <td>100.45</td>\n",
       "      <td>101.95</td>\n",
       "      <td>99.15</td>\n",
       "      <td>100.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEXT</th>\n",
       "      <td>13.55</td>\n",
       "      <td>14.01</td>\n",
       "      <td>14.04</td>\n",
       "      <td>13.81</td>\n",
       "      <td>13.44</td>\n",
       "      <td>13.74</td>\n",
       "      <td>13.72</td>\n",
       "      <td>13.81</td>\n",
       "      <td>14.63</td>\n",
       "      <td>14.84</td>\n",
       "      <td>...</td>\n",
       "      <td>10.00</td>\n",
       "      <td>9.67</td>\n",
       "      <td>8.95</td>\n",
       "      <td>9.00</td>\n",
       "      <td>9.03</td>\n",
       "      <td>9.07</td>\n",
       "      <td>9.44</td>\n",
       "      <td>9.33</td>\n",
       "      <td>8.95</td>\n",
       "      <td>8.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZEN</th>\n",
       "      <td>28.41</td>\n",
       "      <td>28.62</td>\n",
       "      <td>28.10</td>\n",
       "      <td>28.37</td>\n",
       "      <td>28.33</td>\n",
       "      <td>28.64</td>\n",
       "      <td>28.68</td>\n",
       "      <td>28.40</td>\n",
       "      <td>28.55</td>\n",
       "      <td>28.75</td>\n",
       "      <td>...</td>\n",
       "      <td>103.53</td>\n",
       "      <td>102.25</td>\n",
       "      <td>96.87</td>\n",
       "      <td>98.36</td>\n",
       "      <td>97.92</td>\n",
       "      <td>99.99</td>\n",
       "      <td>100.45</td>\n",
       "      <td>101.95</td>\n",
       "      <td>99.15</td>\n",
       "      <td>100.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZEN</th>\n",
       "      <td>28.41</td>\n",
       "      <td>28.62</td>\n",
       "      <td>28.10</td>\n",
       "      <td>28.37</td>\n",
       "      <td>28.33</td>\n",
       "      <td>28.64</td>\n",
       "      <td>28.68</td>\n",
       "      <td>28.40</td>\n",
       "      <td>28.55</td>\n",
       "      <td>28.75</td>\n",
       "      <td>...</td>\n",
       "      <td>103.53</td>\n",
       "      <td>102.25</td>\n",
       "      <td>96.87</td>\n",
       "      <td>98.36</td>\n",
       "      <td>97.92</td>\n",
       "      <td>99.99</td>\n",
       "      <td>100.45</td>\n",
       "      <td>101.95</td>\n",
       "      <td>99.15</td>\n",
       "      <td>100.41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5995 rows × 1198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      2017-04-17  2017-04-18  2017-04-19  2017-04-20  2017-04-21  2017-04-24  \\\n",
       "ADBE      129.99      129.81      130.22      131.42      131.52      132.89   \n",
       "ADP        91.66       91.72       91.85       92.72       92.41       93.71   \n",
       "ADS       223.10      222.63      225.10      243.78      242.75      244.02   \n",
       "ADSK       85.99       86.41       87.93       88.41       88.58       90.14   \n",
       "AKAM       58.82       58.95       59.68       60.20       60.28       61.39   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "YEXT       13.55       14.01       14.04       13.81       13.44       13.74   \n",
       "ZEN        28.41       28.62       28.10       28.37       28.33       28.64   \n",
       "YEXT       13.55       14.01       14.04       13.81       13.44       13.74   \n",
       "ZEN        28.41       28.62       28.10       28.37       28.33       28.64   \n",
       "ZEN        28.41       28.62       28.10       28.37       28.33       28.64   \n",
       "\n",
       "      2017-04-25  2017-04-26  2017-04-27  2017-04-28  ...  2022-01-03  \\\n",
       "ADBE      133.49      132.79      133.38      133.74  ...      564.37   \n",
       "ADP        94.49       94.36       94.63       94.03  ...      242.80   \n",
       "ADS       246.02      245.38      243.71      233.50  ...       68.43   \n",
       "ADSK       90.32       90.23       90.63       90.07  ...      283.72   \n",
       "AKAM       61.80       62.24       59.75       60.94  ...      117.51   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "YEXT       13.72       13.81       14.63       14.84  ...       10.00   \n",
       "ZEN        28.68       28.40       28.55       28.75  ...      103.53   \n",
       "YEXT       13.72       13.81       14.63       14.84  ...       10.00   \n",
       "ZEN        28.68       28.40       28.55       28.75  ...      103.53   \n",
       "ZEN        28.68       28.40       28.55       28.75  ...      103.53   \n",
       "\n",
       "      2022-01-04  2022-01-05  2022-01-06  2022-01-07  2022-01-10  2022-01-11  \\\n",
       "ADBE      554.00      514.43      514.12      510.70      525.83      529.89   \n",
       "ADP       243.93      241.85      240.19      237.66      231.73      234.11   \n",
       "ADS        70.79       68.81       69.14       70.20       71.60       72.08   \n",
       "ADSK      278.19      264.32      264.11      262.32      262.39      270.63   \n",
       "AKAM      116.95      115.48      111.88      110.54      112.65      113.42   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "YEXT        9.67        8.95        9.00        9.03        9.07        9.44   \n",
       "ZEN       102.25       96.87       98.36       97.92       99.99      100.45   \n",
       "YEXT        9.67        8.95        9.00        9.03        9.07        9.44   \n",
       "ZEN       102.25       96.87       98.36       97.92       99.99      100.45   \n",
       "ZEN       102.25       96.87       98.36       97.92       99.99      100.45   \n",
       "\n",
       "      2022-01-12  2022-01-13  2022-01-14  \n",
       "ADBE      532.37      516.90      520.60  \n",
       "ADP       233.47      230.78      227.62  \n",
       "ADS        71.73       74.66       72.16  \n",
       "ADSK      269.60      260.17      259.10  \n",
       "AKAM      114.11      112.91      112.62  \n",
       "...          ...         ...         ...  \n",
       "YEXT        9.33        8.95        8.67  \n",
       "ZEN       101.95       99.15      100.41  \n",
       "YEXT        9.33        8.95        8.67  \n",
       "ZEN       101.95       99.15      100.41  \n",
       "ZEN       101.95       99.15      100.41  \n",
       "\n",
       "[5995 rows x 1198 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_until_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2022-01-14</th>\n",
       "      <th>2022-01-18</th>\n",
       "      <th>2022-01-19</th>\n",
       "      <th>2022-01-20</th>\n",
       "      <th>2022-01-21</th>\n",
       "      <th>2022-01-24</th>\n",
       "      <th>2022-01-25</th>\n",
       "      <th>2022-01-26</th>\n",
       "      <th>2022-01-27</th>\n",
       "      <th>2022-01-28</th>\n",
       "      <th>...</th>\n",
       "      <th>2022-03-30</th>\n",
       "      <th>2022-03-31</th>\n",
       "      <th>2022-04-01</th>\n",
       "      <th>2022-04-04</th>\n",
       "      <th>2022-04-05</th>\n",
       "      <th>2022-04-06</th>\n",
       "      <th>2022-04-07</th>\n",
       "      <th>2022-04-08</th>\n",
       "      <th>2022-04-11</th>\n",
       "      <th>2022-04-12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ADBE</th>\n",
       "      <td>520.60</td>\n",
       "      <td>513.34</td>\n",
       "      <td>516.58</td>\n",
       "      <td>510.85</td>\n",
       "      <td>499.91</td>\n",
       "      <td>519.66</td>\n",
       "      <td>502.72</td>\n",
       "      <td>500.81</td>\n",
       "      <td>493.05</td>\n",
       "      <td>518.16</td>\n",
       "      <td>...</td>\n",
       "      <td>460.06</td>\n",
       "      <td>455.62</td>\n",
       "      <td>458.19</td>\n",
       "      <td>468.81</td>\n",
       "      <td>458.58</td>\n",
       "      <td>444.33</td>\n",
       "      <td>452.72</td>\n",
       "      <td>445.34</td>\n",
       "      <td>434.44</td>\n",
       "      <td>426.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>227.62</td>\n",
       "      <td>225.32</td>\n",
       "      <td>223.71</td>\n",
       "      <td>219.22</td>\n",
       "      <td>216.05</td>\n",
       "      <td>217.16</td>\n",
       "      <td>215.80</td>\n",
       "      <td>196.48</td>\n",
       "      <td>195.45</td>\n",
       "      <td>198.28</td>\n",
       "      <td>...</td>\n",
       "      <td>227.76</td>\n",
       "      <td>227.54</td>\n",
       "      <td>233.50</td>\n",
       "      <td>235.18</td>\n",
       "      <td>238.00</td>\n",
       "      <td>237.79</td>\n",
       "      <td>238.50</td>\n",
       "      <td>237.71</td>\n",
       "      <td>230.96</td>\n",
       "      <td>231.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADS</th>\n",
       "      <td>72.16</td>\n",
       "      <td>68.86</td>\n",
       "      <td>65.50</td>\n",
       "      <td>65.35</td>\n",
       "      <td>64.43</td>\n",
       "      <td>66.98</td>\n",
       "      <td>66.43</td>\n",
       "      <td>66.27</td>\n",
       "      <td>65.86</td>\n",
       "      <td>67.41</td>\n",
       "      <td>...</td>\n",
       "      <td>57.05</td>\n",
       "      <td>56.15</td>\n",
       "      <td>56.80</td>\n",
       "      <td>55.83</td>\n",
       "      <td>54.89</td>\n",
       "      <td>52.61</td>\n",
       "      <td>51.83</td>\n",
       "      <td>52.70</td>\n",
       "      <td>52.90</td>\n",
       "      <td>53.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADSK</th>\n",
       "      <td>259.10</td>\n",
       "      <td>252.03</td>\n",
       "      <td>254.35</td>\n",
       "      <td>252.41</td>\n",
       "      <td>239.19</td>\n",
       "      <td>245.35</td>\n",
       "      <td>233.78</td>\n",
       "      <td>233.28</td>\n",
       "      <td>228.66</td>\n",
       "      <td>239.54</td>\n",
       "      <td>...</td>\n",
       "      <td>221.00</td>\n",
       "      <td>214.35</td>\n",
       "      <td>213.04</td>\n",
       "      <td>218.77</td>\n",
       "      <td>211.55</td>\n",
       "      <td>203.94</td>\n",
       "      <td>204.77</td>\n",
       "      <td>202.25</td>\n",
       "      <td>199.03</td>\n",
       "      <td>196.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AKAM</th>\n",
       "      <td>112.62</td>\n",
       "      <td>112.08</td>\n",
       "      <td>113.17</td>\n",
       "      <td>113.27</td>\n",
       "      <td>112.61</td>\n",
       "      <td>114.19</td>\n",
       "      <td>112.56</td>\n",
       "      <td>109.94</td>\n",
       "      <td>110.19</td>\n",
       "      <td>112.17</td>\n",
       "      <td>...</td>\n",
       "      <td>121.11</td>\n",
       "      <td>119.39</td>\n",
       "      <td>120.51</td>\n",
       "      <td>120.46</td>\n",
       "      <td>120.41</td>\n",
       "      <td>119.59</td>\n",
       "      <td>119.34</td>\n",
       "      <td>118.36</td>\n",
       "      <td>118.01</td>\n",
       "      <td>117.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEXT</th>\n",
       "      <td>8.67</td>\n",
       "      <td>8.50</td>\n",
       "      <td>8.47</td>\n",
       "      <td>8.22</td>\n",
       "      <td>7.81</td>\n",
       "      <td>7.94</td>\n",
       "      <td>7.92</td>\n",
       "      <td>7.59</td>\n",
       "      <td>7.41</td>\n",
       "      <td>7.57</td>\n",
       "      <td>...</td>\n",
       "      <td>7.09</td>\n",
       "      <td>6.89</td>\n",
       "      <td>6.83</td>\n",
       "      <td>7.07</td>\n",
       "      <td>6.77</td>\n",
       "      <td>6.50</td>\n",
       "      <td>6.45</td>\n",
       "      <td>6.25</td>\n",
       "      <td>6.21</td>\n",
       "      <td>6.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZEN</th>\n",
       "      <td>100.41</td>\n",
       "      <td>96.88</td>\n",
       "      <td>97.57</td>\n",
       "      <td>97.69</td>\n",
       "      <td>94.74</td>\n",
       "      <td>95.77</td>\n",
       "      <td>93.72</td>\n",
       "      <td>92.03</td>\n",
       "      <td>90.52</td>\n",
       "      <td>94.51</td>\n",
       "      <td>...</td>\n",
       "      <td>119.69</td>\n",
       "      <td>120.29</td>\n",
       "      <td>123.81</td>\n",
       "      <td>125.17</td>\n",
       "      <td>124.76</td>\n",
       "      <td>122.58</td>\n",
       "      <td>123.25</td>\n",
       "      <td>122.50</td>\n",
       "      <td>121.99</td>\n",
       "      <td>123.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEXT</th>\n",
       "      <td>8.67</td>\n",
       "      <td>8.50</td>\n",
       "      <td>8.47</td>\n",
       "      <td>8.22</td>\n",
       "      <td>7.81</td>\n",
       "      <td>7.94</td>\n",
       "      <td>7.92</td>\n",
       "      <td>7.59</td>\n",
       "      <td>7.41</td>\n",
       "      <td>7.57</td>\n",
       "      <td>...</td>\n",
       "      <td>7.09</td>\n",
       "      <td>6.89</td>\n",
       "      <td>6.83</td>\n",
       "      <td>7.07</td>\n",
       "      <td>6.77</td>\n",
       "      <td>6.50</td>\n",
       "      <td>6.45</td>\n",
       "      <td>6.25</td>\n",
       "      <td>6.21</td>\n",
       "      <td>6.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZEN</th>\n",
       "      <td>100.41</td>\n",
       "      <td>96.88</td>\n",
       "      <td>97.57</td>\n",
       "      <td>97.69</td>\n",
       "      <td>94.74</td>\n",
       "      <td>95.77</td>\n",
       "      <td>93.72</td>\n",
       "      <td>92.03</td>\n",
       "      <td>90.52</td>\n",
       "      <td>94.51</td>\n",
       "      <td>...</td>\n",
       "      <td>119.69</td>\n",
       "      <td>120.29</td>\n",
       "      <td>123.81</td>\n",
       "      <td>125.17</td>\n",
       "      <td>124.76</td>\n",
       "      <td>122.58</td>\n",
       "      <td>123.25</td>\n",
       "      <td>122.50</td>\n",
       "      <td>121.99</td>\n",
       "      <td>123.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZEN</th>\n",
       "      <td>100.41</td>\n",
       "      <td>96.88</td>\n",
       "      <td>97.57</td>\n",
       "      <td>97.69</td>\n",
       "      <td>94.74</td>\n",
       "      <td>95.77</td>\n",
       "      <td>93.72</td>\n",
       "      <td>92.03</td>\n",
       "      <td>90.52</td>\n",
       "      <td>94.51</td>\n",
       "      <td>...</td>\n",
       "      <td>119.69</td>\n",
       "      <td>120.29</td>\n",
       "      <td>123.81</td>\n",
       "      <td>125.17</td>\n",
       "      <td>124.76</td>\n",
       "      <td>122.58</td>\n",
       "      <td>123.25</td>\n",
       "      <td>122.50</td>\n",
       "      <td>121.99</td>\n",
       "      <td>123.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5995 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      2022-01-14  2022-01-18  2022-01-19  2022-01-20  2022-01-21  2022-01-24  \\\n",
       "ADBE      520.60      513.34      516.58      510.85      499.91      519.66   \n",
       "ADP       227.62      225.32      223.71      219.22      216.05      217.16   \n",
       "ADS        72.16       68.86       65.50       65.35       64.43       66.98   \n",
       "ADSK      259.10      252.03      254.35      252.41      239.19      245.35   \n",
       "AKAM      112.62      112.08      113.17      113.27      112.61      114.19   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "YEXT        8.67        8.50        8.47        8.22        7.81        7.94   \n",
       "ZEN       100.41       96.88       97.57       97.69       94.74       95.77   \n",
       "YEXT        8.67        8.50        8.47        8.22        7.81        7.94   \n",
       "ZEN       100.41       96.88       97.57       97.69       94.74       95.77   \n",
       "ZEN       100.41       96.88       97.57       97.69       94.74       95.77   \n",
       "\n",
       "      2022-01-25  2022-01-26  2022-01-27  2022-01-28  ...  2022-03-30  \\\n",
       "ADBE      502.72      500.81      493.05      518.16  ...      460.06   \n",
       "ADP       215.80      196.48      195.45      198.28  ...      227.76   \n",
       "ADS        66.43       66.27       65.86       67.41  ...       57.05   \n",
       "ADSK      233.78      233.28      228.66      239.54  ...      221.00   \n",
       "AKAM      112.56      109.94      110.19      112.17  ...      121.11   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "YEXT        7.92        7.59        7.41        7.57  ...        7.09   \n",
       "ZEN        93.72       92.03       90.52       94.51  ...      119.69   \n",
       "YEXT        7.92        7.59        7.41        7.57  ...        7.09   \n",
       "ZEN        93.72       92.03       90.52       94.51  ...      119.69   \n",
       "ZEN        93.72       92.03       90.52       94.51  ...      119.69   \n",
       "\n",
       "      2022-03-31  2022-04-01  2022-04-04  2022-04-05  2022-04-06  2022-04-07  \\\n",
       "ADBE      455.62      458.19      468.81      458.58      444.33      452.72   \n",
       "ADP       227.54      233.50      235.18      238.00      237.79      238.50   \n",
       "ADS        56.15       56.80       55.83       54.89       52.61       51.83   \n",
       "ADSK      214.35      213.04      218.77      211.55      203.94      204.77   \n",
       "AKAM      119.39      120.51      120.46      120.41      119.59      119.34   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "YEXT        6.89        6.83        7.07        6.77        6.50        6.45   \n",
       "ZEN       120.29      123.81      125.17      124.76      122.58      123.25   \n",
       "YEXT        6.89        6.83        7.07        6.77        6.50        6.45   \n",
       "ZEN       120.29      123.81      125.17      124.76      122.58      123.25   \n",
       "ZEN       120.29      123.81      125.17      124.76      122.58      123.25   \n",
       "\n",
       "      2022-04-08  2022-04-11  2022-04-12  \n",
       "ADBE      445.34      434.44      426.77  \n",
       "ADP       237.71      230.96      231.06  \n",
       "ADS        52.70       52.90       53.28  \n",
       "ADSK      202.25      199.03      196.14  \n",
       "AKAM      118.36      118.01      117.93  \n",
       "...          ...         ...         ...  \n",
       "YEXT        6.25        6.21        6.19  \n",
       "ZEN       122.50      121.99      123.38  \n",
       "YEXT        6.25        6.21        6.19  \n",
       "ZEN       122.50      121.99      123.38  \n",
       "ZEN       122.50      121.99      123.38  \n",
       "\n",
       "[5995 rows x 61 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_from_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X.shape == Y.shape\n",
    "    \n",
    "_DAYS_IN_TRADING_MONTH = 20\n",
    "output = {}\n",
    "    \n",
    "trade_length_days = trade_length_months*_DAYS_IN_TRADING_MONTH\n",
    "    \n",
    "X_until_T = X.iloc[:, :-trade_length_days]\n",
    "Y_until_T = Y.iloc[:, :-trade_length_days]\n",
    "    \n",
    "X_from_T = X.iloc[:, -trade_length_days-1:]\n",
    "Y_from_T = Y.iloc[:, -trade_length_days-1:]\n",
    "    \n",
    "# X and Y dimensions must match\n",
    "assert X_from_T.shape == Y_from_T.shape and X_until_T.shape == Y_until_T.shape\n",
    "    \n",
    "# Check whether enough data was given\n",
    "assert X_from_T.shape[1] == trade_length_days+1 and X.shape[1] == X_until_T.shape[1] + X_from_T.shape[1] - 1\n",
    "    \n",
    "residuals, betas = get_rolling_residuals(X=X_until_T, Y=Y_until_T, l_reg=l_reg, l_roll=l_roll, dt=dt)\n",
    "    \n",
    "adfs, adfs_raw = get_aggregate_adfs(residuals, betas=betas, cutoff=adf_pass_cutoff)\n",
    "    \n",
    "std_residuals = get_standardized_residuals(residuals)\n",
    "    \n",
    "last_betas = get_last_pairs(betas)\n",
    "    \n",
    "assert np.all(std_residuals.index == last_betas.index)\n",
    "assert np.all(adfs.index == std_residuals.index)\n",
    "    \n",
    "# Select spreads with >50% ADF pass rate\n",
    "selected = (adfs > 0.5).values\n",
    "    \n",
    "selected_std_residuals = std_residuals[selected]\n",
    "    \n",
    "selected_residuals_max_mean = get_mean_residual_magnitude(selected_std_residuals, dt=trade_length_days)\n",
    "    \n",
    "selected_betas = last_betas[selected]\n",
    "    \n",
    "selected_buys_X = selected_std_residuals.iloc[:, -1] > 0\n",
    "    \n",
    "# selected_separated_pairs = separate_pair_index(selected_std_residuals.index)\n",
    "selected_X_from_T = X_from_T[selected]\n",
    "selected_Y_from_T = Y_from_T[selected]\n",
    "    \n",
    "selected_trade_returns = get_spreads_returns(prices_X=selected_X_from_T, prices_Y=selected_Y_from_T, betas_YX=selected_betas, buy_X=selected_buys_X)\n",
    "    \n",
    "output['adf_residuals'] = adfs[selected][0].values\n",
    "output['last_residual'] = selected_std_residuals.iloc[:, -1].values\n",
    "output['ticker_x'] = selected_X_from_T.index\n",
    "output['ticker_y'] = selected_Y_from_T.index\n",
    "output['return_one_month'] = selected_trade_returns.iloc[:, 20].values\n",
    "output['beta'] = selected_betas[0].values\n",
    "output['residual_mean_max'] = np.full(len(selected_std_residuals), selected_residuals_max_mean)\n",
    "output['trade_date'] = np.full(len(selected_std_residuals), X_from_T.columns[0])\n",
    "\n",
    "if trade_length_months > 1:\n",
    "    output['return_two_month'] = selected_trade_returns.iloc[:, 40].values\n",
    "else:\n",
    "    output['return_two_month'] = np.full(len(selected_std_residuals.index), np.nan)\n",
    "        \n",
    "if trade_length_months > 2:\n",
    "    output['return_three_month'] = selected_trade_returns.iloc[:, 60].values\n",
    "else:\n",
    "    output['return_three_month'] = np.full(len(selected_std_residuals.index), np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "542"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['last_residual'].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010174763797510648"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['last_residual'].abs() > 3]['return_two_month'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002693945187356014"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['last_residual'].abs() < 3]['return_two_month'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_time(func):\n",
    "    t1 = time.time()\n",
    "    ret = func()\n",
    "    t2 = time.time()\n",
    "    print(\"Time: \" + str(int(t2-t1)) + 's')\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_pair_index(indexes):\n",
    "    indexes = pd.Series(indexes)\n",
    "    y = np.array(indexes.apply(lambda x: x.split('_')[0]))\n",
    "    x = np.array(indexes.apply(lambda x: x.split('_')[1]))\n",
    "    return {'y':y, 'x':x}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline example tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Import tickers from given custom list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'data/selected_spreads.csv'\n",
    "# ticker_pairs = pd.read_csv(filename, header=0)\n",
    "# ticker_pairs.set_index('spreads', inplace=True)\n",
    "\n",
    "# separated_indexes = separate_pair_index(ticker_pairs.index)\n",
    "# ticker_pairs['x'] = separated_indexes['x']\n",
    "# ticker_pairs['y'] = separated_indexes['y']\n",
    "\n",
    "# ticker_pairs = ticker_pairs.loc[~ticker_pairs.index.str.contains(r'CIT|LORL|ENBL|MDP')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Download stock daily prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets all ticker names (no argument given)\n",
    "ticker_list = get_tickers_by_industry(['software_and_services'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ticker_list = set(list(ticker_pairs['x']) + list(ticker_pairs['y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific date - 3rd of March 2022 (Y, M, D)\n",
    "# date_to = datetime(2022, 3, 1)\n",
    "# Date of today\n",
    "date_to = datetime.today()\n",
    "# How many years' of data to download (going backwards from date_end). Year can be a floating point number\n",
    "period_years = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  219 of 219 completed\n"
     ]
    }
   ],
   "source": [
    "# Download ticker price data for the tickers selected above (saved to .csv automatically)\n",
    "df, df_clean = download_stonk_prices(ticker_list.index, period_years=period_years, date_to=date_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Read stock data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'technology_hardware_and_equipment'\n",
    "'software_and_services'\n",
    "'media_and_entertainment'\n",
    "'retailing'\n",
    "'automobiles_and_components'\n",
    "'semiconductors_and_semiconductor_equipment'\n",
    "'health_care_equipment_and_services'\n",
    "'banks'\n",
    "'pharmaceuticals_biotechnology_and_life_sciences'\n",
    "'food_and_staples_retailing'\n",
    "'oil_gas_and_consumable_fuels'\n",
    "'food_beverage_and_tobacco'\n",
    "'telecommunication_services'\n",
    "'consumer_durables_and_apparel'\n",
    "'consumer_services'\n",
    "'transportation'\n",
    "'diversified_financials'\n",
    "'utilities'\n",
    "'capital_goods'\n",
    "'insurance'\n",
    "'chemicals'\n",
    "'metals_and_mining'\n",
    "'commercial_and_professional_services'\n",
    "'containers_and_packaging'\n",
    "'energy_equipment_and_services'\n",
    "'construction_materials'\n",
    "'paper_and_forest_products'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stock data of ALL industries (all tickers) - no arguments specified\n",
    "# stonks = get_stonk_data_by_industry('2018-04-26', '2022-04-12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stock data from selected industries only\n",
    "stonks = get_stonk_data_by_industry('2017-04-17', '2022-04-12', industries=['software_and_services'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = combine_stonk_pairs(stonks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Select spreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = stonks.loc[ticker_pairs['x']]\n",
    "# Y = stonks.loc[ticker_pairs['y']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1. (IF PREVIOUS CELL FAILED) Remove ticker pairs with failed downloads and retry previous operation (CTRL + / to uncomment lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# failed_list = '|'.join(['SITE', 'INFO', 'RRD', 'HWM', 'OR', 'NGVT', 'VRS', 'TWLO', 'NUAN', 'PI', 'RRR', 'TPB', 'USFD', 'GMS', 'ENIC'])\n",
    "# ticker_pairs = ticker_pairs[~ticker_pairs.index.str.contains(failed_list)]\n",
    "# X = stonks.loc[ticker_pairs['x']]\n",
    "# Y = stonks.loc[ticker_pairs['y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "_WRITE_RESULTS_TO_CSV = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Calculate rolling residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 37s\n"
     ]
    }
   ],
   "source": [
    "residuals, betas = measure_time(partial(get_rolling_residuals, X=X, Y=Y, l_reg=3, l_roll=1, dt=5, write_csv=_WRITE_RESULTS_TO_CSV))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Calculate ADF test results using the residuals returned above. Betas are optionally given to invalidate ADF test results where betas are negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 615s\n"
     ]
    }
   ],
   "source": [
    "adfs, adfs_raw = measure_time(partial(get_aggregate_adfs, residuals, betas=betas, write_csv=_WRITE_RESULTS_TO_CSV))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Calculate the standardized residuals of the regression from the last time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_residuals = get_standardized_residuals(residuals, write_csv=_WRITE_RESULTS_TO_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 8. Calculate selected trade returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_std_residuals = std_residuals[(adfs > 0.5).values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 7. Calculate the mean residual trade making magnitude cutoff over the last *dt* days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_residual_magnitude = get_mean_residual_magnitude(selected_std_residuals, dt=30)\n",
    "# mean_residual_magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.7123723"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean_residual_magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 8.2. Select which trades to make based on the last standardized residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trade_YX = selected_std_residuals[selected_std_residuals.iloc[:, -1].abs() >= mean_residual_magnitude]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 8.2. Get betas for the last regressions and for the selected pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_betas = get_last_pairs(betas)\n",
    "# betas_YX = last_betas.loc[trade_YX.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 8.3. Select long/short stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buy_X = trade_YX.iloc[:, -1].apply(lambda x: x > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 8.4. Separate spread pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separated_pairs = separate_pair_index(trade_YX.index)\n",
    "# tickers_X = separated_pairs['x']\n",
    "# tickers_Y = separated_pairs['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 8.5. Calculate returns for the trades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Slow residual functions (for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_rolling_slow_residuals(X, Y, l_reg, l_roll, dt):\n",
    "#     _DAYS_IN_TRADING_YEAR = (252) - (252 % dt)\n",
    "#     l_reg_days = _DAYS_IN_TRADING_YEAR * l_reg\n",
    "#     l_roll_days = _DAYS_IN_TRADING_YEAR * l_roll\n",
    "#     total_days = l_reg_days + l_roll_days\n",
    "#     n_windows = l_roll_days // dt\n",
    "#     n_x = X.shape[0]\n",
    "    \n",
    "#     assert (l_roll_days % dt) == 0\n",
    "#     assert X.shape[1] >= total_days and Y.shape[1] >= total_days\n",
    "    \n",
    "#     X = X[:, -total_days:]\n",
    "#     Y = Y[:, -total_days:]\n",
    "    \n",
    "#     # First window\n",
    "#     X_windows = np.empty(shape=(n_x*n_windows, l_reg_days))\n",
    "#     Y_windows = np.empty(shape=(n_x*n_windows, l_reg_days))\n",
    "    \n",
    "#     for n in range(n_x):\n",
    "#         for i in range(n_windows):\n",
    "#             X_windows = np.concatenate(( X_windows, X[n, i*dt:l_reg_days+(i*dt)] ))\n",
    "#             Y_windows = np.concatenate(( Y_windows, Y[n, i*dt:l_reg_days+(i*dt)] ))\n",
    "    \n",
    "#     assert X_windows.shape == (n_x*n_windows, l_reg_days) and Y_windows.shape == (n_x*n_windows, l_reg_days)\n",
    "    \n",
    "#     return get_slow_residuals_many(X_windows, Y_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_slow_residuals_many(X, Y, n_jobs=-1):\n",
    "#     lr = LinearRegression(n_jobs=n_jobs, fit_intercept=True)\n",
    "#     X = X.reshape((X.shape[0], X.shape[1], -1))\n",
    "#     Y = Y.reshape((Y.shape[0], Y.shape[1], -1))\n",
    "    \n",
    "#     preds = []\n",
    "#     res = []\n",
    "#     betas = []\n",
    "#     for i in range(X.shape[0]):\n",
    "#         lr.fit(X[i], Y[i])\n",
    "#         preds.append(lr.predict(X[i]).round(2))\n",
    "#         res.append(Y[i]-preds[-1])\n",
    "#         betas.append(lr.coef_[0][0])\n",
    "#     return (np.asarray(res)[:,:,0], np.asarray(preds)[:,:,0], np.asarray(betas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1_fast = time.time()\n",
    "# res, betas, preds  = get_rolling_residuals(X, Y, l_reg=2, l_roll=1, dt=5)\n",
    "# t2_fast = time.time()\n",
    "\n",
    "# t1_slow = time.time()\n",
    "# res_slow, preds_slow = get_rolling_slow_residuals(X, Y, l_reg=2, l_roll=1, dt=5)\n",
    "# t2_slow = time.time()\n",
    "\n",
    "# print(\"Time slow: \" + str(t2_slow-t1_slow))\n",
    "# print(\"Time fast: \" + str(t2_fast-t1_fast))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1_fast = time.time()\n",
    "# res, preds, betas = get_residuals_many(X, Y)\n",
    "# t2_fast = time.time()\n",
    "\n",
    "# t1_slow = time.time()\n",
    "# res_slow, preds_slow, betas_slow = get_slow_residuals_many(X, Y)\n",
    "# t2_slow = time.time()\n",
    "\n",
    "# print(\"Time slow: \" + str(t2_slow-t1_slow))\n",
    "# print(\"Time fast: \" + str(t2_fast-t1_fast))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Stock list preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_stock_list(raw_data_path='data/raw_stonk_list.xls', output_path='data/stonk_list.csv'):\n",
    "    '''\n",
    "    Parses a raw excel file from CapitalIQ containing ticker names and their subindustries, validates\n",
    "    unusual ticker names with Yahoo Finance, saving the processed data in CSV format.\n",
    "\n",
    "        Parameters:\n",
    "            Required:\n",
    "                raw_data_path (string):\n",
    "                    Path to the raw excel file.\n",
    "                output_path (string):\n",
    "                    Path where to save the parsed data.\n",
    "                \n",
    "        Returns:\n",
    "            Nothing\n",
    "    '''\n",
    "    \n",
    "    df = pd.read_excel(io=raw_data_path)\n",
    "    \n",
    "    # Drop NA rows\n",
    "    df.dropna(axis=0, inplace=True)\n",
    "    \n",
    "    # Reset index and drop the first row\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    df.drop(index=0, axis=0, inplace=True)\n",
    "    \n",
    "    # Drop unwanted columns\n",
    "    df.drop(columns=df.columns[[1, 2, 3, 4, 5, 7, 8, 9]], inplace=True)\n",
    "    \n",
    "    # Rename remaining columns\n",
    "    df.columns = ['ticker', 'subindustry']\n",
    "    \n",
    "    # Remove the '(Primary)' tag from subindustries\n",
    "    df['subindustry'] = df['subindustry'].str.replace(r' \\(Primary\\)', '')\n",
    "    \n",
    "    # Remove everything until (and including) the semicolon for tickers\n",
    "    df['ticker'] = df['ticker'].str.replace(r'(.*:)', '')\n",
    "    \n",
    "    df['ticker'] = df['ticker'].str.replace(r' WI', '.VI')\n",
    "    df['ticker'] = df['ticker'].str.replace(r'\\.WI', '.VI')\n",
    "    \n",
    "    # Replace the ticker endings for a Yahoo finance supported format\n",
    "    df['ticker'] = df['ticker'].str.replace(r'\\.PR', '-P')\n",
    "    # df['ticker'] = df['ticker'].str.replace(r' PR', '-P')\n",
    "    \n",
    "    # Take all remaining tickers that have a dot\n",
    "    dotted = df[df['ticker'].str.fullmatch(r'[A-Z]*\\.[A-Z]')]\n",
    "    \n",
    "    # Replace the dots with dashes\n",
    "    dashed = dotted.copy()\n",
    "    dashed['ticker'] = dashed['ticker'].str.replace(r'\\.', '-')\n",
    "    \n",
    "    # Remove the dots\n",
    "    undotted = dotted.copy()\n",
    "    undotted['ticker'] = undotted['ticker'].str.replace(r'\\.', '')\n",
    "\n",
    "    # Combine all variantas together\n",
    "    all_variants = pd.concat([dotted, dashed, undotted])\n",
    "    \n",
    "    # Run all of these through Yahoo finance, get last day's price\n",
    "    stonks = yf.download(list(all_variants['ticker'].astype('string').values), period='1m', interval='1d', group_by='column')\n",
    "    \n",
    "    # Drop all NA tickers (that failed to download)\n",
    "    valid_tickers = stonks['Adj Close'].iloc[-1].dropna(axis=0).to_frame().reset_index()\n",
    "    \n",
    "    # Rename columns\n",
    "    valid_tickers.columns = ['ticker', 'price']\n",
    "    \n",
    "    # Add subindustries to the remaining valid tickers\n",
    "    valid_tickers = valid_tickers.join(all_variants.set_index('ticker'), on='ticker')\n",
    "    \n",
    "    # Drop the price column\n",
    "    valid_tickers.drop(columns=valid_tickers.columns[[1]], inplace=True)\n",
    "    \n",
    "    # Remove all tickers that have a dot from main dataframe\n",
    "    df = df[~df['ticker'].str.fullmatch(r'[A-Z]*\\.[A-Z]')]\n",
    "    \n",
    "    # Add the validated tickers back\n",
    "    df = pd.concat([df, valid_tickers], axis=0, ignore_index=True)\n",
    "    \n",
    "    # Make the subindustry strings more code friendly\n",
    "    df['subindustry'] = df['subindustry'].str.replace(' ', '_')\n",
    "    df['subindustry'] = df['subindustry'].str.lower()\n",
    "    df['subindustry'] = df['subindustry'].str.replace(',', '')\n",
    "    \n",
    "    df.to_csv(path_or_buf=output_path, header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
