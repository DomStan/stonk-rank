{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from itertools import combinations\n",
    "from functools import partial\n",
    "\n",
    "# Data management\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Data fetching\n",
    "# !pip install yfinance\n",
    "import yfinance as yf\n",
    "from numba import jit\n",
    "\n",
    "# Spread generation\n",
    "# from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "_DAYS_IN_TRADING_YEAR = 252\n",
    "_DAYS_IN_TRADING_MONTH = 21\n",
    "_DAYS_IN_TRADING_WEEK = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stonk price data download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input ticker names by industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tickers_by_industry(market_cap_min_mm, industries, data_dir=None, filename=None):\n",
    "    '''\n",
    "    Read the CSV file containing all tickers and their subindustries and return tickers from the selected subindustries in a list.\n",
    "    \n",
    "    -Args:\n",
    "        industries (List(string)): if not given, return all tickers.\n",
    "    -Returns:\n",
    "        tickers (pandas Series): list of selected ticker names\n",
    "    '''\n",
    "    filename = 'stonk_list.csv' if filename is None else filename\n",
    "    data_dir = 'data' if data_dir is None else data_dir\n",
    "    \n",
    "    path_to_csv = os.path.join(data_dir, filename)\n",
    "    stonk_list = pd.read_csv(path_to_csv)\n",
    "    stonk_list = stonk_list[stonk_list['market_cap'] >= market_cap_min_mm]\n",
    "    return stonk_list.set_index('ticker') if not industries else stonk_list[stonk_list['subindustry'].isin(industries)].set_index('ticker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_stonk_prices(stonk_list, period_years=3, date_from=None, date_to=None, interval='1d', source='yfinance', data_dir='data', proxy=False):    \n",
    "    '''\n",
    "    Returns historical price data for the selected stonks.\n",
    "\n",
    "    -Args:\n",
    "        stonk_list (List(string)): List of stonk identifiers as strings, case unsensitive\n",
    "        period_years (float): How many years of data to download until date_to, can be a floating point number\n",
    "    -Optional:\n",
    "        date_from (datetime): Start date for stonk data (use instead of period_years)\n",
    "        date_to (datetime): End date for stonk data\n",
    "        interval (string): Valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
    "        source (string): Where to source data from. Valid sources: yfinance\n",
    "        data_dir (string): Folder name where to output downloaded data\n",
    "        file_prefix (string): Prefix of CSV file containing downloaded data inside data_dir\n",
    "        proxy (boolean): Whether to use a proxy connection to avoid API limits/blocks\n",
    "                \n",
    "    -Returns:\n",
    "        stonks (Pandas Dataframe): Pandas Dataframe containing requested ticker prices\n",
    "    '''\n",
    "    \n",
    "    date_to = datetime.now() if date_to is None else date_to\n",
    "    date_from = date_to-(timedelta(days=int(365*period_years))) if date_from is None else date_from\n",
    "    \n",
    "    if source.lower() == 'yfinance':\n",
    "        stonks = yf.download(list(stonk_list), start=date_from, end=date_to, interval=interval, group_by='column', threads=True, rounding=True)['Adj Close']\n",
    "        stonks.dropna(axis=0, how='all', inplace=True)\n",
    "        stonks.sort_values(by='Date', inplace=True)\n",
    "        \n",
    "        stonks.index = pd.to_datetime(stonks.index).date\n",
    "        stonks.index.name = 'date'\n",
    "\n",
    "        clean_stonks = stonks.dropna(axis=1, how='all', thresh=int(len(stonks.index) * 0.99)).copy()\n",
    "        clean_stonks.dropna(axis=0, how='all', thresh=int(len(clean_stonks.columns) * 0.99), inplace=True)\n",
    "        \n",
    "        # Forward fill ticker columns (axis=0 for columns)\n",
    "        clean_stonks.fillna(axis=0, method='ffill', inplace=True)\n",
    "        \n",
    "        clean_stonks.dropna(axis=1, how='any', inplace=True)\n",
    "        \n",
    "        # Must be no NA values left\n",
    "        assert clean_stonks.isna().sum().sum() == 0\n",
    "    else:\n",
    "        raise ValueError('Unsupported data source')\n",
    "        \n",
    "    def stonks_to_csv(stonks, clean):\n",
    "        from_date_string = stonks.index[0]\n",
    "        to_date_string = stonks.index[-1]\n",
    "\n",
    "        filename = 'stonks_{from_date}_to_{to_date}.csv'.format(from_date=from_date_string, to_date=to_date_string)\n",
    "        \n",
    "        if clean:\n",
    "            filename = 'clean_' + filename\n",
    "            \n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "\n",
    "        stonks.to_csv(path_or_buf=file_path, header=True, index=True, na_rep='NaN')\n",
    "    \n",
    "    stonks_to_csv(stonks, clean=False)\n",
    "    stonks_to_csv(clean_stonks, clean=True)\n",
    "    \n",
    "    return (stonks, clean_stonks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock price data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stonk_data(date_from, date_to, clean=True, date_index=False, data_dir=None):\n",
    "    data_dir = 'data' if data_dir is None else data_dir\n",
    "    data_prefix = 'clean_stonks' if clean else 'stonks'\n",
    "    \n",
    "    path = os.path.join(data_dir, '{}_{}_to_{}.csv'.format(data_prefix, date_from, date_to))\n",
    "    stonks = pd.read_csv(path, header=0, index_col=0).astype(np.float32)\n",
    "    \n",
    "    if clean:\n",
    "        assert stonks.isna().sum().sum() == 0\n",
    "    \n",
    "    if date_index:\n",
    "        return stonks\n",
    "    else:\n",
    "        return stonks.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stonk_data_by_industry(date_from, date_to, market_cap_min_mm=1000, clean=True, date_index=False, industries=None, stonk_list_filename=None, data_dir=None):\n",
    "    '''\n",
    "    Read the CSV file containing all stonk price data and return the tickers from the selected subindustries.\n",
    "    \n",
    "    -Args:\n",
    "        industries (List(string)): if not given, return all tickers\n",
    "    \n",
    "    -Returns:\n",
    "        stonks (pandas DataFrame): list of selected tickers' price data\n",
    "    '''\n",
    "    all_stonks = read_stonk_data(date_from, date_to, date_index=date_index, data_dir=data_dir, clean=clean)\n",
    "    selected_tickers = get_tickers_by_industry(market_cap_min_mm, industries=industries, data_dir=data_dir, filename=stonk_list_filename)\n",
    "    return all_stonks[all_stonks.index.isin(selected_tickers.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make combinations with numpy\n",
    "def combine_stonk_pairs(stonks_prices):\n",
    "    # All ticker names must be unique\n",
    "    assert all(stonks_prices.index.unique() == stonks_prices.index)\n",
    "    \n",
    "    # Check that there isn't too many combinations\n",
    "    assert(len(stonks_prices) < 400)\n",
    "    \n",
    "    combs = np.asarray(list(combinations(stonks_prices.index.unique(), 2)))\n",
    "    \n",
    "    return stonks_prices.loc[combs[:, 0]], stonks_prices.loc[combs[:, 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Linear regression residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_residuals_many(X, Y):\n",
    "    '''\n",
    "    Vectorized calculation of residuals from many univariate linear regressions.\n",
    "        Args:\n",
    "        - X (numpy array of shape (n_pairs, d_time)): matrix of LR inputs X, each row represents a different regression, corresponding to the same rows in Y\n",
    "        - Y (numpy array of shape (n_pairs, d_time)): matrix of LR inputs Y, each row represents a different regression, corresponding to the same rows in X\n",
    "        Returns:\n",
    "        - residuals (numpy array of shape (n_pairs, d_time)): matrix of resulting residuals between vectorized pairs of X and Y\n",
    "        - betas (numpy array of shape (n_pairs, 1)): beta coefficients for each linear regression\n",
    "        - Y_hat (numpy array of shape (n_pairs, d_time)): predictions using X\n",
    "    '''\n",
    "    # Stack 2D matrices into 3D matrices\n",
    "    X = X.reshape(np.shape(X)[0], np.shape(X)[1], -1).astype(np.float32)\n",
    "    Y = Y.reshape(np.shape(Y)[0], np.shape(Y)[1], -1).astype(np.float32)\n",
    "    \n",
    "    # Add bias/intercept in the form (Xi, 1)\n",
    "    Z = np.concatenate([X, np.ones((np.shape(X)[0], np.shape(X)[1], 1), dtype=np.float32)], axis=2)\n",
    "    del X\n",
    "    \n",
    "    # Save the transpose as it's used a couple of times\n",
    "    Z_t = Z.transpose(0, 2, 1)\n",
    "    \n",
    "    # Linear Regression equation solutions w.r.t. weight matrix\n",
    "    # W contains (beta_coef, a_intercept) for each regression\n",
    "    W = np.matmul(np.linalg.inv(np.matmul(Z_t, Z)),  np.matmul(Z_t, Y))\n",
    "    del Z_t\n",
    "    \n",
    "    # Predictions and residuals\n",
    "    # Y_hat = np.matmul(Z, W).round(2)\n",
    "    residuals = (Y - np.matmul(Z, W)).round(2)\n",
    "    del Y\n",
    "    del Z\n",
    "    assert residuals.dtype == np.float32\n",
    "    \n",
    "    # Y_hat returned for debugging purposes\n",
    "    # return (residuals[:, :, 0], W[:, 0, 0], Y_hat[:, :, 0])\n",
    "    return (residuals[:, :, 0], W[:, 0, 0],  W[:, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rolling_residuals(X, Y, l_reg, l_roll, dt, write_csv=False, data_dir='data'):\n",
    "    '''\n",
    "    Calculates rolling window residuals in vectorized form. Returns the result as an array that repeats each ticker for the number of regressions calculated.\n",
    "    For example, if the inputs are (Pair A, Pair B, Pair C) and l_roll / dt = 3, then the returned results will have the form as follows:\n",
    "    (Pair A, Pair A, Pair A, Pair B, Pair B, Pair B, Pair C, Pair C, Pair C)\n",
    "    Works best when l_reg and l_roll are integers.\n",
    "        Args:\n",
    "        - X, Y (DataFrame of shape (n_pairs, >= l_reg + l_roll)): matrix of LR inputs X, Y; each row containing at least the complete data period for rolling regressions (can be longer)\n",
    "        - l_reg (float): length of each LR to calculate residuals, in years; will be multiplied by the adjusted number of days in a trading year\n",
    "        - l_roll (float): length of rolling window, in years; will be multipled by the adjusted number of days in a trading year\n",
    "        - dt (int): rolling window step size, in trading days; total trading year days will be reduced to be divisible by dt (by not more than the value of dt)\n",
    "        Returns:\n",
    "        - residuals (numpy array of shape (n_pairs * (l_roll/dt)+1, l_reg + l_roll)): matrix of resulting residuals between vectorized pairs of X and Y\n",
    "        - betas (numpy array of shape (n_pairs * (l_roll/dt)+1, 1)): beta coefficients for each linear regression\n",
    "        - Y_hat (numpy array of shape (n_pairs * (l_roll/dt)+1, l_reg + l_roll)): predictions using X\n",
    "    '''\n",
    "    \n",
    "    # Adjust days so that they are divisible by dt\n",
    "    l_reg_days = int(_DAYS_IN_TRADING_YEAR * l_reg)\n",
    "    l_reg_days-= l_reg_days % dt\n",
    "    l_roll_days = int(_DAYS_IN_TRADING_YEAR * l_roll)\n",
    "    l_roll_days-= l_roll_days % dt\n",
    "    \n",
    "    total_days = l_reg_days + l_roll_days\n",
    "    \n",
    "    # Number of regressions for each ticker\n",
    "    n_windows = (l_roll_days // dt) + 1\n",
    "    \n",
    "    # Number of tickers\n",
    "    n_x = X.shape[0]\n",
    "    \n",
    "    # Take the dates, create an empty array for windowed dates\n",
    "    date_index = X.columns[-total_days:]\n",
    "    date_index_windowed = np.empty(shape=(n_x*n_windows, 2), dtype='O')\n",
    "    \n",
    "    # Repeat each ticker name times n_windows\n",
    "    X_index = np.repeat(X.index, n_windows)\n",
    "    Y_index = np.repeat(Y.index, n_windows)\n",
    "    \n",
    "    # X and Y must have the same dates\n",
    "    assert np.array_equal(X.columns, Y.columns)\n",
    "    \n",
    "    X = X.to_numpy(dtype=np.float32)\n",
    "    Y = Y.to_numpy(dtype=np.float32)\n",
    "    \n",
    "    # Rolling window length must be divisible by dt\n",
    "    assert (l_roll_days % dt) == 0\n",
    "    \n",
    "    # There has to be enough days' worth of data in X (and Y) and their shapes must match\n",
    "    assert X.shape == Y.shape and X.shape[1] >= total_days\n",
    "    \n",
    "    # Take the total_days from the end of the arrays (most recent days first, oldest days at the end are cut off)\n",
    "    X = X[:, -total_days:]\n",
    "    Y = Y[:, -total_days:]\n",
    "    \n",
    "    # Create empty arrays that will contain windowed slices of our data\n",
    "    X_windows = np.empty(shape=(n_x*n_windows, l_reg_days))\n",
    "    Y_windows = np.empty(shape=(n_x*n_windows, l_reg_days))\n",
    "    \n",
    "    # Take windowed slices and place them into the created empty arrays\n",
    "    for n in range(n_x):\n",
    "        for i in range(n_windows):\n",
    "            n_i = (n*n_windows)+i\n",
    "            t_i = i*dt\n",
    "            t_y = t_i + l_reg_days\n",
    "            \n",
    "            X_windows[n_i] = X[n, t_i:t_y]\n",
    "            Y_windows[n_i] = Y[n, t_i:t_y]\n",
    "            date_index_windowed[n_i, 0] = date_index[t_i]\n",
    "            date_index_windowed[n_i, 1] = date_index[t_y-1]\n",
    "    \n",
    "    # Make sure we've got the windowing dimensions right\n",
    "    assert X_windows.shape == (n_x*n_windows, l_reg_days) and Y_windows.shape == (n_x*n_windows, l_reg_days)\n",
    "    \n",
    "    # Sanity checks\n",
    "    assert all([\n",
    "        X[0, -1] == X_windows[n_windows-1, -1],\n",
    "        Y[0, -1] == Y_windows[n_windows-1, -1],\n",
    "        X[-1, -1] == X_windows[-1, -1],\n",
    "        Y[-1, -1] == Y_windows[-1, -1],\n",
    "    ])\n",
    "    \n",
    "    # Construct ticker pair index column\n",
    "    pair_index = np.array(pd.DataFrame(np.array([Y_index, X_index])).apply('_'.join, axis=0, raw=True))\n",
    "    \n",
    "    # Construct regression date range index column\n",
    "    date_index = np.array(pd.DataFrame(np.array([date_index_windowed[:, 0], date_index_windowed[:, 1]])).apply('_'.join, axis=0, raw=True))\n",
    "    \n",
    "    # Lengths of indexes must match\n",
    "    assert len(pair_index) == len(date_index)\n",
    "    \n",
    "    # Calculate and return the residuals\n",
    "    res, betas, intercepts = get_residuals_many(X_windows, Y_windows)\n",
    "    \n",
    "    res = pd.DataFrame(res, index=pair_index)\n",
    "    res.insert(0, 'dates', date_index)\n",
    "    \n",
    "    betas = pd.DataFrame(betas, index=pair_index)\n",
    "    betas.insert(0, 'dates', date_index)\n",
    "    \n",
    "    intercepts = pd.DataFrame(intercepts, index=pair_index)\n",
    "    \n",
    "    if write_csv:\n",
    "        time = datetime.now().time()\n",
    "        res.to_csv(os.path.join(data_dir, time.strftime('residuals_%H%M%S.csv')), header=False, index=True)\n",
    "        betas.to_csv(os.path.join(data_dir, time.strftime('betas_%H%M%S.csv')), header=False, index=True)\n",
    "        pd.Series(date_index).to_csv(os.path.join(data_dir, time.strftime('dates_%H%M%S.csv')), header=False, index=False)\n",
    "    \n",
    "    return res, betas, intercepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADF testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adfs(residuals, adf_regression):\n",
    "    # Get ADF test p-values for each row of the residuals array. No autolag (maxlag always used)\n",
    "    assert residuals.dtype == np.float32\n",
    "    return np.apply_along_axis(lambda x: adfuller(x, regression=adf_regression, autolag=None)[1], axis=1, arr=residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregate_adfs(residuals, betas=None, cutoff=0.1, adf_regression='c', write_csv=False, data_dir='data'):\n",
    "    # Get ADF p-values\n",
    "    adfs = get_adfs(residuals.drop(columns='dates').to_numpy(dtype=np.float32), adf_regression=adf_regression).reshape((-1, 1))\n",
    "    \n",
    "    # Add ones to ADF values where betas are negative, if betas are given\n",
    "    if betas is not None:\n",
    "        # Must be the same number of columns\n",
    "        assert adfs.shape[0] == betas.shape[0]\n",
    "        # Residuals and betas must have the same index names\n",
    "        assert np.all(residuals.index == betas.index)\n",
    "        # Add 1's to p-values where betas are negative\n",
    "        adfs = adfs + (betas[0].to_numpy() <= 0).reshape((-1, 1))\n",
    "        \n",
    "    # Make a copy for returning, CSV output\n",
    "    adfs_raw = pd.DataFrame(adfs.copy(), index=residuals.index)\n",
    "    \n",
    "    # All unique ticker pairs, in original order\n",
    "    unique_pairs = residuals.index.unique()\n",
    "    \n",
    "    # Number of regressions for one pair\n",
    "    pairs_per_index = len(residuals) // len(unique_pairs)\n",
    "    \n",
    "    # Reshape into a 3D array for averaging ADF values along the second axis\n",
    "    adfs = adfs.reshape((len(unique_pairs), pairs_per_index, 1))\n",
    "    \n",
    "    # Takes cutoff, averages along the pairs_per_index (second) axis\n",
    "    adfs = (adfs <= cutoff).mean(axis=1)\n",
    "    \n",
    "    # Probably always true, but just in case\n",
    "    assert adfs.shape[0] == len(unique_pairs)\n",
    "        \n",
    "    # Back to a DataFrame with named indexes\n",
    "    adfs = pd.DataFrame(adfs, index=unique_pairs)\n",
    "    \n",
    "    # Output to CSV\n",
    "    if write_csv:\n",
    "        time = datetime.now().time()\n",
    "        adfs.to_csv(os.path.join(data_dir, time.strftime('adfs_%H%M%S.csv')), header=False, index=True)\n",
    "        adfs_raw.to_csv(os.path.join(data_dir, time.strftime('adfs-raw_%H%M%S.csv')), header=False, index=True)\n",
    "        \n",
    "    return adfs, adfs_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardized residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_pairs(pairs):\n",
    "    # Get unique ticker pairs, in preserved order\n",
    "    unique_pairs = pairs.index.unique()\n",
    "    \n",
    "    # Number of samples per ticker pair\n",
    "    pairs_per_index = len(pairs) // len(unique_pairs)\n",
    "    \n",
    "    # Must be an equal number of pairs per index\n",
    "    assert pairs_per_index * len(unique_pairs) == len(pairs)\n",
    "    \n",
    "    # Slice taking only the last regression for each ticker pair\n",
    "    last_pairs = pairs.iloc[pairs_per_index-1:len(pairs):pairs_per_index].copy()\n",
    "    \n",
    "    # Make sure we got the slices right\n",
    "    assert np.all(last_pairs.index == unique_pairs) and np.all(pairs.iloc[-1] == last_pairs.iloc[-1])\n",
    "        \n",
    "    return last_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_standardized_residuals(residuals, write_csv=False, data_dir='data'):\n",
    "    # Dates aren't needed anymore, as we're using the latest regressions\n",
    "    residuals = residuals.drop(columns='dates')\n",
    "    \n",
    "    # Get the last regression for each spread\n",
    "    last_reg_pairs = get_last_pairs(residuals)\n",
    "    \n",
    "    # Get unique ticker pairs\n",
    "    unique_pairs = last_reg_pairs.index\n",
    "    \n",
    "    # Convert to numpy\n",
    "    last_reg_pairs = last_reg_pairs.to_numpy(dtype=np.float32)\n",
    "    \n",
    "    # Standardize\n",
    "    means = last_reg_pairs.mean(axis=1, keepdims=True)\n",
    "    stds = last_reg_pairs.std(axis=1, keepdims=True)\n",
    "    last_reg_pairs = (last_reg_pairs - means) / stds\n",
    "    \n",
    "    # Back to a DataFrame with named indexes\n",
    "    last_reg_pairs = pd.DataFrame(last_reg_pairs, index=unique_pairs)\n",
    "    means = pd.DataFrame(means, index=unique_pairs)\n",
    "    stds = pd.DataFrame(stds, index=unique_pairs)\n",
    "    \n",
    "    # Output to CSV\n",
    "    if write_csv:\n",
    "        time = datetime.now().time()\n",
    "        last_reg_pairs.to_csv(os.path.join(data_dir, time.strftime('std-residuals_%H%M%S.csv')), header=False, index=True)\n",
    "        \n",
    "    return (last_reg_pairs, means, stds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean residual magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_residual_magnitude(std_residuals, dt):\n",
    "    # Assume there is enough days' worth of data for averaging over dt days\n",
    "    assert std_residuals.shape[1] >= dt\n",
    "    \n",
    "    # Select the last dt days from the right\n",
    "    std_residuals = std_residuals.to_numpy(dtype=np.float32)[:, -dt:]\n",
    "    \n",
    "    # Take the absolute maximum for each day, over all tickers, mean over the results\n",
    "    mean_magnitude = np.round(np.abs(std_residuals).max(axis=0).mean(), 2)\n",
    "    \n",
    "    return mean_magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trade returns and future residuals calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trades_returns(prices_X, prices_Y, betas_YX, buy_X):\n",
    "\n",
    "    # Save spread indexes for later\n",
    "    pairs_indexes = betas_YX.index.copy()\n",
    "    \n",
    "    # Take numpy betas\n",
    "    betas_YX = betas_YX.drop(columns='dates').to_numpy().copy()\n",
    "\n",
    "    # Take numpy buy list for X\n",
    "    buy_X = buy_X.values.copy()\n",
    "\n",
    "    # Sanity checks\n",
    "    assert all([\n",
    "        prices_X.shape == prices_Y.shape,\n",
    "        len(buy_X) == betas_YX.shape[0]\n",
    "    ])\n",
    "\n",
    "    # Prices to numpy\n",
    "    prices_X = prices_X.to_numpy().copy()\n",
    "    prices_Y = prices_Y.to_numpy().copy()\n",
    "\n",
    "    # Save entering prices at t=0\n",
    "    initial_prices_X = prices_X[:, [0]].copy()\n",
    "    initial_prices_Y = prices_Y[:, [0]].copy()\n",
    "\n",
    "    # Initial proportional values of trades at t=0, X prices scaled by beta\n",
    "    initial_trade_values = (initial_prices_X * betas_YX) + initial_prices_Y\n",
    "\n",
    "    # Returns for X, Y trades each day. X prices scaled by beta\n",
    "    returns_X = betas_YX * (prices_X - initial_prices_X)\n",
    "    returns_Y = prices_Y - initial_prices_Y\n",
    "\n",
    "    # Negate short trades\n",
    "    returns_X[~buy_X] = -returns_X[~buy_X]\n",
    "    returns_Y[buy_X] = -returns_Y[buy_X]\n",
    "\n",
    "    # Add the trade returns for X, Y, divide by initial investment values to get profit/loss %\n",
    "    trade_returns = ((returns_X + returns_Y) / initial_trade_values).round(2)\n",
    "\n",
    "    # Back to dataframe with indexes\n",
    "    trade_returns = pd.DataFrame(trade_returns, index=pairs_indexes)\n",
    "    \n",
    "    return trade_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trades_residuals(prices_X, prices_Y, betas_YX, intercepts_YX, means_YX, stds_YX):\n",
    "\n",
    "    # Sanity checks\n",
    "    assert all([\n",
    "        np.all(betas_YX.index == intercepts_YX.index),\n",
    "        np.all(means_YX.index == stds_YX.index),\n",
    "        np.all(betas_YX.index == stds_YX.index)\n",
    "    ])\n",
    "    \n",
    "    # Save indexes for later\n",
    "    indexes = betas_YX.index.copy()\n",
    "    \n",
    "    betas_YX = betas_YX.drop(columns='dates').to_numpy().copy()\n",
    "    intercepts_YX = intercepts_YX.to_numpy().copy()\n",
    "    \n",
    "    means_YX = means_YX.to_numpy().copy()\n",
    "    stds_YX = stds_YX.to_numpy().copy()\n",
    "    \n",
    "    prices_X = prices_X.to_numpy().copy()\n",
    "    prices_Y = prices_Y.to_numpy().copy()\n",
    "\n",
    "    # Sanity checks\n",
    "    assert all([\n",
    "        prices_X.shape == prices_Y.shape,\n",
    "        betas_YX.shape[0] == intercepts_YX.shape[0] == means_YX.shape[0] == stds_YX.shape[0] == prices_X.shape[0],\n",
    "        betas_YX.shape[1] == intercepts_YX.shape[1] == means_YX.shape[1] == stds_YX.shape[1] == 1,\n",
    "    ])\n",
    "    \n",
    "    # Calculate residuals from regression form: Y = bX + a\n",
    "    trade_residuals = prices_Y - ((betas_YX * prices_X) + intercepts_YX)\n",
    "    \n",
    "    # Standardize the residuals\n",
    "    trade_residuals = (trade_residuals - means_YX) / stds_YX \n",
    "\n",
    "    # Back to dataframe with indexes\n",
    "    trade_residuals = pd.DataFrame(trade_residuals, index=indexes)\n",
    "    \n",
    "    return trade_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5671, 1)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means.to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5671, 1)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stds.to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.45177376],\n",
       "       [0.42660797],\n",
       "       [0.3905766 ],\n",
       "       ...,\n",
       "       [0.4283793 ],\n",
       "       [0.4290197 ],\n",
       "       [0.425498  ]], dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas.drop(columns='dates').to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data collection pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collection_rolling_pipeline(stonk_prices, industries, l_reg, l_roll, dt, market_cap_min_mm=1000, adf_pval_cutoff=0.1, adf_pass_rate_filter=0.5, trade_length_months=3, trading_interval_weeks=4, data_dir='data'):\n",
    "    _DAYS_IN_TRADING_YEAR = 252\n",
    "    \n",
    "    # Adjust days so that they are divisible by dt\n",
    "    l_reg_days = int(_DAYS_IN_TRADING_YEAR * l_reg)\n",
    "    l_reg_days-= l_reg_days % dt\n",
    "    l_roll_days = int(_DAYS_IN_TRADING_YEAR * l_roll)\n",
    "    l_roll_days-= l_roll_days % dt\n",
    "    \n",
    "    total_days = l_reg_days + l_roll_days\n",
    "    trade_length_days = _DAYS_IN_TRADING_MONTH * trade_length_months\n",
    "    trading_interval_days = _DAYS_IN_TRADING_WEEK * trading_interval_weeks\n",
    "    total_backtest_days = total_days + trade_length_days\n",
    "    \n",
    "    # Get tickers from selected industries\n",
    "    tickers = get_tickers_by_industry(market_cap_min_mm, industries, data_dir=data_dir)\n",
    "    \n",
    "    data_range = range(stonk_prices.shape[1], total_backtest_days, -trading_interval_days)\n",
    "    \n",
    "    total_data_windows = len(list(data_range))\n",
    "    total_industries = len(industries)\n",
    "    \n",
    "    print('Total data windows: ' + str(total_data_windows))\n",
    "    \n",
    "    for index_end in data_range:\n",
    "        index_start = index_end - total_backtest_days\n",
    "        data_window = stonk_prices.iloc[:, index_start:index_end]\n",
    "        assert data_window.shape[1] == total_backtest_days\n",
    "        \n",
    "        print('Period ' + str(data_window.columns[0]) + ' to ' +  str(data_window.columns[-1]))\n",
    "        \n",
    "        data_all_industries = []\n",
    "        for industry in industries:\n",
    "            tickers_by_industry = tickers[tickers['subindustry'] == industry]\n",
    "            data_window_by_industry = data_window[data_window.index.isin(tickers_by_industry.index)]\n",
    "            X, Y = combine_stonk_pairs(data_window_by_industry)\n",
    "            \n",
    "            data = pd.DataFrame(data_collection_step(X, Y, l_reg, l_roll, dt, adf_pval_cutoff, adf_pass_rate_filter, trade_length_months))\n",
    "            \n",
    "            data['data_window_start'] = np.full(data.shape[0], X.columns[0])\n",
    "            data['subindustry'] = np.full(data.shape[0], industry)\n",
    "            \n",
    "            data_all_industries.append(data)\n",
    "            \n",
    "            print('Industries ' + str(len(data_all_industries)) + '/' + str(total_industries))\n",
    "        \n",
    "        data_all_industries = pd.concat(data_all_industries, ignore_index=True)\n",
    "        \n",
    "        filename = '_'.join([\n",
    "            str(data_window.columns[0]),\n",
    "            str(data_window.columns[-1]),\n",
    "            str(l_reg),\n",
    "            str(l_roll),\n",
    "            str(dt),\n",
    "            str(market_cap_min_mm),\n",
    "            str(adf_pval_cutoff),\n",
    "            str(adf_pass_rate_filter)\n",
    "        ]) + '.csv'\n",
    "        \n",
    "        data_output_path = os.path.join(data_dir, 'trades', filename)\n",
    "        \n",
    "        data_all_industries.to_csv(data_output_path, header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data collection pipeline step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collection_step(X, Y, l_reg, l_roll, dt, adf_pval_cutoff, adf_pass_rate_filter, trade_length_months):\n",
    "    assert X.shape == Y.shape\n",
    "\n",
    "    _DAYS_IN_TRADING_MONTH = 21\n",
    "    output = {}\n",
    "\n",
    "    trade_length_days = trade_length_months*_DAYS_IN_TRADING_MONTH\n",
    "\n",
    "    X_until_T = X.iloc[:, :-trade_length_days].copy()\n",
    "    Y_until_T = Y.iloc[:, :-trade_length_days].copy()\n",
    "\n",
    "    X_from_T = X.iloc[:, -trade_length_days-1:].reset_index().drop_duplicates(subset='index').set_index('index').copy()\n",
    "    Y_from_T = Y.iloc[:, -trade_length_days-1:].reset_index().drop_duplicates(subset='index').set_index('index').copy()\n",
    "\n",
    "    # X and Y dimensions must match\n",
    "    assert X_from_T.shape == Y_from_T.shape and X_until_T.shape == Y_until_T.shape\n",
    "\n",
    "    # Check whether enough days' worth of data was given\n",
    "    assert X_from_T.shape[1] == trade_length_days+1 and X.shape[1] == X_until_T.shape[1] + X_from_T.shape[1] - 1\n",
    "    \n",
    "    del X\n",
    "    del Y\n",
    "\n",
    "    residuals, betas, intercepts = get_rolling_residuals(X=X_until_T, Y=Y_until_T, l_reg=l_reg, l_roll=l_roll, dt=dt)\n",
    "    std_residuals, means, stds = get_standardized_residuals(residuals)\n",
    "    \n",
    "    # Filter residuals to only select relevant trades that fit the theoretical assumptions\n",
    "    std_residuals = std_residuals[std_residuals.iloc[:, -1].abs() >= 2]\n",
    "    \n",
    "    if len(std_residuals) == 0:\n",
    "        return {}\n",
    "    \n",
    "    residuals = residuals.loc[std_residuals.index]\n",
    "    betas = betas.loc[std_residuals.index]\n",
    "    intercepts = intercepts.loc[std_residuals.index]\n",
    "    \n",
    "    adfs, adfs_raw = get_aggregate_adfs(residuals, betas=betas, cutoff=adf_pval_cutoff)\n",
    "    del adfs_raw\n",
    "\n",
    "    # Select betas and intercepts from the most recent regressions\n",
    "    betas = get_last_pairs(betas)\n",
    "    intercepts = get_last_pairs(intercepts)\n",
    "\n",
    "    assert np.all(std_residuals.index == betas.index)\n",
    "    assert np.all(adfs.index == std_residuals.index)\n",
    "    assert np.all(intercepts.index == betas.index)\n",
    "\n",
    "    # Select trades that are above the specified ADF pass rate\n",
    "    selected_by_adf = (adfs >= adf_pass_rate_filter).values\n",
    "\n",
    "    std_residuals = std_residuals[selected_by_adf]\n",
    "    \n",
    "    if len(std_residuals) == 0:\n",
    "        return {}\n",
    "\n",
    "    residuals_max_mean = get_mean_residual_magnitude(std_residuals, dt=trade_length_days)\n",
    "\n",
    "    betas = betas.loc[std_residuals.index]\n",
    "    intercepts = intercepts.loc[std_residuals.index]\n",
    "    \n",
    "    means = means.loc[std_residuals.index]\n",
    "    stds = stds.loc[std_residuals.index]\n",
    "\n",
    "    # True for trades where we buy X and short Y\n",
    "    buys_X = std_residuals.iloc[:, -1] > 0\n",
    "\n",
    "    trades_tickers = separate_pair_index(std_residuals.index)\n",
    "    X_from_T = X_from_T.loc[trades_tickers['x']]\n",
    "    Y_from_T = Y_from_T.loc[trades_tickers['y']]\n",
    "\n",
    "    trade_returns = get_trades_returns(prices_X=X_from_T, prices_Y=Y_from_T, betas_YX=betas, buy_X=buys_X)\n",
    "    trade_residuals = get_trades_residuals(prices_X=X_from_T, prices_Y=Y_from_T, betas_YX=betas, intercepts_YX=intercepts, means_YX=means, stds_YX=stds)\n",
    "\n",
    "    output_length = len(std_residuals)\n",
    "    output['ticker_x'] = X_from_T.index\n",
    "    output['ticker_y'] = Y_from_T.index\n",
    "    output['trade_date'] = np.full(output_length, X_from_T.columns[0])\n",
    "    \n",
    "    output['adf_pass_rate'] = adfs[selected_by_adf][0].values.round(2)\n",
    "    output['last_residual'] = std_residuals.iloc[:, -1].values.round(2)\n",
    "    output['beta'] = betas[0].values.round(2)\n",
    "    output['intercept'] = intercepts[0].values.round(2)\n",
    "    output['residual_mean_max'] = np.full(output_length, residuals_max_mean)\n",
    "\n",
    "    output['return_one_month'] = trade_returns.iloc[:, 21].values\n",
    "    output['residual_one_month'] = trade_residuals.iloc[:, 21].values\n",
    "    if trade_length_months > 1:\n",
    "        output['return_two_month'] = trade_returns.iloc[:, 42].values\n",
    "        output['residual_two_month'] = trade_residuals.iloc[:, 42].values\n",
    "    else:\n",
    "        output['return_two_month'] = np.full(output_length, np.nan)\n",
    "        output['residual_two_month'] = np.full(output_length, np.nan)\n",
    "\n",
    "    if trade_length_months > 2:\n",
    "        output['return_three_month'] = trade_returns.iloc[:, 63].values\n",
    "        output['residual_three_month'] = trade_residuals.iloc[:, 63].values\n",
    "    else:\n",
    "        output['return_three_month'] = np.full(output_length, np.nan)\n",
    "        output['residual_three_month'] = np.full(output_length, np.nan)\n",
    "\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_trade_data(data_dir='data/trades'):\n",
    "    data_list = []\n",
    "    for file in os.listdir(data_dir):\n",
    "            if file.endswith('csv'):\n",
    "                data_list.append(pd.read_csv(os.path.join(data_dir, file)))\n",
    "    df = pd.concat(data_list, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_list = get_tickers_by_industry(market_cap_min_mm=1000, industries=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collection_rolling_pipeline(stonks, industries=list(ticker_list['subindustry'].unique()), l_reg=3, l_roll=1, dt=20, market_cap_min_mm=1000, adf_pval_cutoff=0.1, adf_pass_rate_filter=0.5, trade_length_months=3, trading_interval_weeks=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_time(func):\n",
    "    t1 = time.time()\n",
    "    ret = func()\n",
    "    t2 = time.time()\n",
    "    print(\"Time: \" + str(int(t2-t1)) + 's')\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_pair_index(indexes):\n",
    "    indexes = pd.Series(indexes)\n",
    "    splits = indexes.apply(lambda x: pd.Series(x.split('_')))\n",
    "    y = splits[0].values\n",
    "    x = splits[1].values\n",
    "    return {'y':y, 'x':x}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline example tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Import tickers from given custom list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'data/selected_spreads.csv'\n",
    "# ticker_pairs = pd.read_csv(filename, header=0)\n",
    "# ticker_pairs.set_index('spreads', inplace=True)\n",
    "\n",
    "# separated_indexes = separate_pair_index(ticker_pairs.index)\n",
    "# ticker_pairs['x'] = separated_indexes['x']\n",
    "# ticker_pairs['y'] = separated_indexes['y']\n",
    "\n",
    "# ticker_pairs = ticker_pairs.loc[~ticker_pairs.index.str.contains(r'CIT|LORL|ENBL|MDP')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Download stock daily prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets all ticker names (no argument given)\n",
    "ticker_list = get_tickers_by_industry(market_cap_min_mm=1000, industries=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ticker_list = set(list(ticker_pairs['x']) + list(ticker_pairs['y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific date - 3rd of March 2022 (Y, M, D)\n",
    "# date_to = datetime(2022, 3, 1)\n",
    "# Date of today\n",
    "date_to = datetime.today()\n",
    "# How many years' of data to download (going backwards from date_end). Year can be a floating point number\n",
    "period_years = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  2815 of 2815 completed\n",
      "\n",
      "10 Failed downloads:\n",
      "- PFE.WI: No data found, symbol may be delisted\n",
      "- BIP.PRA: No data found, symbol may be delisted\n",
      "- O.WI: No data found, symbol may be delisted\n",
      "- POST WI: No data found, symbol may be delisted\n",
      "- BIP.PRB: No data found, symbol may be delisted\n",
      "- SNX.WI: No data found, symbol may be delisted\n",
      "- MRK.WI: No data found, symbol may be delisted\n",
      "- T WD: No data found, symbol may be delisted\n",
      "- RXN WI: No data found, symbol may be delisted\n",
      "- DELL WI: No data found, symbol may be delisted\n"
     ]
    }
   ],
   "source": [
    "# Download ticker price data for the tickers selected above (saved to .csv automatically)\n",
    "df, df_clean = download_stonk_prices(ticker_list.index, period_years=period_years, date_to=date_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Read stock data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'health_care_equipment_and_services'\n",
    "'software_and_services'\n",
    "'retailing'\n",
    "'telecommunication_services'\n",
    "'capital_goods'\n",
    "'energy'\n",
    "'pharmaceuticals_biotechnology_and_life_sciences'\n",
    "'consumer_staples'\n",
    "'banks'\n",
    "'diversified_financials'\n",
    "'metals_and_mining'\n",
    "'technology_hardware_and_equipment'\n",
    "'utilities'\n",
    "'chemicals'\n",
    "'automobiles_and_components'\n",
    "'semiconductors_and_semiconductor_equipment'\n",
    "'media_and_entertainment'\n",
    "'real_estate'\n",
    "'consumer_services'\n",
    "'consumer_durables_and_apparel'\n",
    "'insurance'\n",
    "'transportation'\n",
    "'commercial_and_professional_services'\n",
    "'paper_and_forest_products'\n",
    "'containers_and_packaging'\n",
    "'construction_materials'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stock data of ALL industries (all tickers) - no arguments specified\n",
    "# stonks = get_stonk_data_by_industry('2018-04-26', '2022-04-12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stock data from selected industries only\n",
    "stonks = get_stonk_data_by_industry('2017-04-20', '2022-04-18', industries=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = combine_stonk_pairs(stonks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Select spreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = stonks.loc[ticker_pairs['x']]\n",
    "# Y = stonks.loc[ticker_pairs['y']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1. (IF PREVIOUS CELL FAILED) Remove ticker pairs with failed downloads and retry previous operation (CTRL + / to uncomment lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# failed_list = '|'.join(['SITE', 'INFO', 'RRD', 'HWM', 'OR', 'NGVT', 'VRS', 'TWLO', 'NUAN', 'PI', 'RRR', 'TPB', 'USFD', 'GMS', 'ENIC'])\n",
    "# ticker_pairs = ticker_pairs[~ticker_pairs.index.str.contains(failed_list)]\n",
    "# X = stonks.loc[ticker_pairs['x']]\n",
    "# Y = stonks.loc[ticker_pairs['y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "_WRITE_RESULTS_TO_CSV = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Calculate rolling residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 7s\n"
     ]
    }
   ],
   "source": [
    "residuals, betas, intercepts = measure_time(partial(get_rolling_residuals, X=X, Y=Y, l_reg=3, l_roll=1, dt=20, write_csv=_WRITE_RESULTS_TO_CSV))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Calculate ADF test results using the residuals returned above. Betas are optionally given to invalidate ADF test results where betas are negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 168s\n"
     ]
    }
   ],
   "source": [
    "adfs, adfs_raw = measure_time(partial(get_aggregate_adfs, residuals, betas=betas, write_csv=_WRITE_RESULTS_TO_CSV))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Calculate the standardized residuals of the regression from the last time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_residuals, means, stds = get_standardized_residuals(residuals, write_csv=_WRITE_RESULTS_TO_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 8. Calculate selected trade returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_std_residuals = std_residuals[(adfs > 0.5).values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 7. Calculate the mean residual trade making magnitude cutoff over the last *dt* days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_residual_magnitude = get_mean_residual_magnitude(selected_std_residuals, dt=30)\n",
    "# mean_residual_magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 8.2. Select which trades to make based on the last standardized residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trade_YX = selected_std_residuals[selected_std_residuals.iloc[:, -1].abs() >= mean_residual_magnitude]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 8.2. Get betas for the last regressions and for the selected pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_betas = get_last_pairs(betas)\n",
    "# betas_YX = last_betas.loc[trade_YX.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Stock list preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_stock_list(write_csv, raw_data_path='data/raw_stonk_list.xls', output_path='data/stonk_list.csv'):\n",
    "    '''\n",
    "    Parses a raw excel file from CapitalIQ containing ticker names and their subindustries, validates\n",
    "    unusual ticker names with Yahoo Finance, saving the processed data in CSV format.\n",
    "\n",
    "        Parameters:\n",
    "            Required:\n",
    "                raw_data_path (string):\n",
    "                    Path to the raw excel file.\n",
    "                output_path (string):\n",
    "                    Path where to save the parsed data.\n",
    "                \n",
    "        Returns:\n",
    "            Nothing\n",
    "    '''\n",
    "    \n",
    "    df = pd.read_excel(io=raw_data_path)\n",
    "\n",
    "    # Drop NA rows\n",
    "    df.dropna(axis=0, inplace=True)\n",
    "\n",
    "    # Reset index and drop the first row\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    df.columns = df.iloc[0]\n",
    "    df.drop(index=0, axis=0, inplace=True)\n",
    "\n",
    "    # Drop unwanted columns\n",
    "    drop_columns = [\n",
    "        'Security Name',\n",
    "        'Company Name',\n",
    "        'Trading Status',\n",
    "        'Most Recent Trade Price ($USD, Historical rate)',\n",
    "        'Equity Security Type',\n",
    "        'Exchange Country/Region',\n",
    "        'Exchanges'\n",
    "    ]\n",
    "    df.drop(columns=drop_columns, inplace=True)\n",
    "\n",
    "    # Rename remaining columns\n",
    "    df.columns = ['ticker', 'subindustry', 'market_cap']\n",
    "    \n",
    "    # Remove the '(Primary)' tag from subindustries\n",
    "    df['subindustry'] = df['subindustry'].str.replace(r' \\(Primary\\)', '')\n",
    "    \n",
    "    # Remove everything until (and including) the semicolon for tickers\n",
    "    df['ticker'] = df['ticker'].str.replace(r'(.*:)', '')\n",
    "    \n",
    "    # Take all remaining tickers that have a dot\n",
    "    dotted = df[df['ticker'].str.fullmatch(r'[A-Z]*\\.[A-Z]')]\n",
    "    \n",
    "    # Replace the dots with dashes\n",
    "    dashed = dotted.copy()\n",
    "    dashed['ticker'] = dashed['ticker'].str.replace(r'\\.', '-')\n",
    "    \n",
    "    # Remove the dots\n",
    "    undotted = dotted.copy()\n",
    "    undotted['ticker'] = undotted['ticker'].str.replace(r'\\.', '')\n",
    "\n",
    "    # Combine all variantas together\n",
    "    all_variants = pd.concat([dotted, dashed, undotted])\n",
    "    \n",
    "    # Run all of these through Yahoo finance, get last day's price\n",
    "    stonks = yf.download(list(all_variants['ticker'].astype('string').values), period='1m', interval='1d', group_by='column')\n",
    "    \n",
    "    # Drop all NA tickers (that failed to download)\n",
    "    valid_tickers = stonks['Adj Close'].iloc[-1].dropna(axis=0, how='all').to_frame().reset_index()\n",
    "    \n",
    "    # Rename columns\n",
    "    valid_tickers.columns = ['ticker', 'price']\n",
    "    \n",
    "    # Add subindustries to the remaining valid tickers\n",
    "    valid_tickers = valid_tickers.join(all_variants.set_index('ticker'), on='ticker')\n",
    "    \n",
    "    # Drop the price column\n",
    "    valid_tickers.drop(columns=valid_tickers.columns[[1]], inplace=True)\n",
    "    \n",
    "    # Remove all tickers that have a dot from main dataframe\n",
    "    df = df[~df['ticker'].str.fullmatch(r'[A-Z]*\\.[A-Z]')]\n",
    "    \n",
    "    # Add the validated tickers back\n",
    "    df = pd.concat([df, valid_tickers], axis=0, ignore_index=True)\n",
    "    \n",
    "    # Make the subindustry strings more code friendly\n",
    "    df['subindustry'] = df['subindustry'].str.replace(' ', '_')\n",
    "    df['subindustry'] = df['subindustry'].str.lower()\n",
    "    df['subindustry'] = df['subindustry'].str.replace(',', '')\n",
    "    \n",
    "    if write_csv:\n",
    "        df.to_csv(path_or_buf=output_path, header=True, index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ingest_trade_data(data_dir='data/trades')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker_x</th>\n",
       "      <th>ticker_y</th>\n",
       "      <th>trade_date</th>\n",
       "      <th>adf_residuals</th>\n",
       "      <th>last_residual</th>\n",
       "      <th>beta</th>\n",
       "      <th>residual_mean_max</th>\n",
       "      <th>return_one_month</th>\n",
       "      <th>return_two_month</th>\n",
       "      <th>return_three_month</th>\n",
       "      <th>data_window_start</th>\n",
       "      <th>subindustry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABC</td>\n",
       "      <td>AMEH</td>\n",
       "      <td>2021-03-04</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.63</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>2017-05-11</td>\n",
       "      <td>health_care_equipment_and_services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABC</td>\n",
       "      <td>ENOV</td>\n",
       "      <td>2021-03-04</td>\n",
       "      <td>0.77</td>\n",
       "      <td>2.82</td>\n",
       "      <td>0.55</td>\n",
       "      <td>3.63</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.09</td>\n",
       "      <td>2017-05-11</td>\n",
       "      <td>health_care_equipment_and_services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABC</td>\n",
       "      <td>LH</td>\n",
       "      <td>2021-03-04</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1.95</td>\n",
       "      <td>3.63</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>2017-05-11</td>\n",
       "      <td>health_care_equipment_and_services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABMD</td>\n",
       "      <td>ALGN</td>\n",
       "      <td>2021-03-04</td>\n",
       "      <td>0.62</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.58</td>\n",
       "      <td>3.63</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>2017-05-11</td>\n",
       "      <td>health_care_equipment_and_services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABMD</td>\n",
       "      <td>AMEH</td>\n",
       "      <td>2021-03-04</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.26</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.63</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>2017-05-11</td>\n",
       "      <td>health_care_equipment_and_services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87026</th>\n",
       "      <td>RSG</td>\n",
       "      <td>TRI</td>\n",
       "      <td>2022-01-14</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-1.87</td>\n",
       "      <td>1.06</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>2018-03-27</td>\n",
       "      <td>commercial_and_professional_services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87027</th>\n",
       "      <td>RSG</td>\n",
       "      <td>TRU</td>\n",
       "      <td>2022-01-14</td>\n",
       "      <td>0.77</td>\n",
       "      <td>-1.78</td>\n",
       "      <td>0.84</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>2018-03-27</td>\n",
       "      <td>commercial_and_professional_services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87028</th>\n",
       "      <td>RSG</td>\n",
       "      <td>WCN</td>\n",
       "      <td>2022-01-14</td>\n",
       "      <td>0.77</td>\n",
       "      <td>-2.26</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2018-03-27</td>\n",
       "      <td>commercial_and_professional_services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87029</th>\n",
       "      <td>CCK</td>\n",
       "      <td>PKG</td>\n",
       "      <td>2022-01-14</td>\n",
       "      <td>0.77</td>\n",
       "      <td>-1.50</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>2018-03-27</td>\n",
       "      <td>containers_and_packaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87030</th>\n",
       "      <td>SEE</td>\n",
       "      <td>SON</td>\n",
       "      <td>2022-01-14</td>\n",
       "      <td>0.62</td>\n",
       "      <td>-2.36</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>2018-03-27</td>\n",
       "      <td>containers_and_packaging</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87031 rows  12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker_x ticker_y  trade_date  adf_residuals  last_residual  beta  \\\n",
       "0          ABC     AMEH  2021-03-04           0.62           1.91  0.03   \n",
       "1          ABC     ENOV  2021-03-04           0.77           2.82  0.55   \n",
       "2          ABC       LH  2021-03-04           1.00           2.10  1.95   \n",
       "3         ABMD     ALGN  2021-03-04           0.62           2.29  0.58   \n",
       "4         ABMD     AMEH  2021-03-04           1.00           2.26  0.02   \n",
       "...        ...      ...         ...            ...            ...   ...   \n",
       "87026      RSG      TRI  2022-01-14           1.00          -1.87  1.06   \n",
       "87027      RSG      TRU  2022-01-14           0.77          -1.78  0.84   \n",
       "87028      RSG      WCN  2022-01-14           0.77          -2.26  0.89   \n",
       "87029      CCK      PKG  2022-01-14           0.77          -1.50  1.08   \n",
       "87030      SEE      SON  2022-01-14           0.62          -2.36  0.44   \n",
       "\n",
       "       residual_mean_max  return_one_month  return_two_month  \\\n",
       "0                   3.63             -0.13             -0.21   \n",
       "1                   3.63              0.09              0.15   \n",
       "2                   3.63              0.02              0.01   \n",
       "3                   3.63             -0.02             -0.08   \n",
       "4                   3.63             -0.12             -0.21   \n",
       "...                  ...               ...               ...   \n",
       "87026               2.98              0.02             -0.02   \n",
       "87027               2.98              0.01             -0.03   \n",
       "87028               2.98              0.02              0.05   \n",
       "87029               1.81              0.01              0.02   \n",
       "87030               1.81              0.00              0.00   \n",
       "\n",
       "       return_three_month data_window_start  \\\n",
       "0                   -0.41        2017-05-11   \n",
       "1                    0.09        2017-05-11   \n",
       "2                   -0.01        2017-05-11   \n",
       "3                   -0.08        2017-05-11   \n",
       "4                   -0.40        2017-05-11   \n",
       "...                   ...               ...   \n",
       "87026               -0.03        2018-03-27   \n",
       "87027               -0.08        2018-03-27   \n",
       "87028                0.04        2018-03-27   \n",
       "87029                0.07        2018-03-27   \n",
       "87030                0.07        2018-03-27   \n",
       "\n",
       "                                subindustry  \n",
       "0        health_care_equipment_and_services  \n",
       "1        health_care_equipment_and_services  \n",
       "2        health_care_equipment_and_services  \n",
       "3        health_care_equipment_and_services  \n",
       "4        health_care_equipment_and_services  \n",
       "...                                     ...  \n",
       "87026  commercial_and_professional_services  \n",
       "87027  commercial_and_professional_services  \n",
       "87028  commercial_and_professional_services  \n",
       "87029              containers_and_packaging  \n",
       "87030              containers_and_packaging  \n",
       "\n",
       "[87031 rows x 12 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['residual_diff'] = df['residual_mean_max'] - df['last_residual'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "return_one_month     -0.000979\n",
       "return_two_month      0.060180\n",
       "return_three_month    0.030309\n",
       "dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['last_residual'].abs() > 5][['return_one_month', 'return_two_month', 'return_three_month']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>return_one_month</th>\n",
       "      <th>return_two_month</th>\n",
       "      <th>return_three_month</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subindustry</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>automobiles_and_components</th>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>banks</th>\n",
       "      <td>-0.010714</td>\n",
       "      <td>-0.022143</td>\n",
       "      <td>-0.007857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital_goods</th>\n",
       "      <td>0.006512</td>\n",
       "      <td>0.009535</td>\n",
       "      <td>-0.021395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chemicals</th>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>commercial_and_professional_services</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>consumer_services</th>\n",
       "      <td>-0.040000</td>\n",
       "      <td>-0.030000</td>\n",
       "      <td>-0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>consumer_staples</th>\n",
       "      <td>-0.010000</td>\n",
       "      <td>-0.020000</td>\n",
       "      <td>-0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diversified_financials</th>\n",
       "      <td>0.003455</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>-0.005818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>energy</th>\n",
       "      <td>-0.008235</td>\n",
       "      <td>0.065294</td>\n",
       "      <td>-0.012353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health_care_equipment_and_services</th>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>-0.052143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insurance</th>\n",
       "      <td>-0.020000</td>\n",
       "      <td>-0.020000</td>\n",
       "      <td>-0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>media_and_entertainment</th>\n",
       "      <td>-0.118421</td>\n",
       "      <td>-0.076316</td>\n",
       "      <td>-0.061053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metals_and_mining</th>\n",
       "      <td>-0.003750</td>\n",
       "      <td>-0.005000</td>\n",
       "      <td>0.032500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pharmaceuticals_biotechnology_and_life_sciences</th>\n",
       "      <td>0.015479</td>\n",
       "      <td>-0.034795</td>\n",
       "      <td>-0.133973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>real_estate</th>\n",
       "      <td>0.047143</td>\n",
       "      <td>2.905714</td>\n",
       "      <td>2.791429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>retailing</th>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.066250</td>\n",
       "      <td>0.138750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>semiconductors_and_semiconductor_equipment</th>\n",
       "      <td>0.012963</td>\n",
       "      <td>0.022963</td>\n",
       "      <td>0.051111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>software_and_services</th>\n",
       "      <td>0.030612</td>\n",
       "      <td>0.079592</td>\n",
       "      <td>0.082857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>technology_hardware_and_equipment</th>\n",
       "      <td>0.177273</td>\n",
       "      <td>0.257273</td>\n",
       "      <td>0.018182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>telecommunication_services</th>\n",
       "      <td>-0.152222</td>\n",
       "      <td>-0.108889</td>\n",
       "      <td>-0.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transportation</th>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.076667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 return_one_month  \\\n",
       "subindustry                                                         \n",
       "automobiles_and_components                               0.020000   \n",
       "banks                                                   -0.010714   \n",
       "capital_goods                                            0.006512   \n",
       "chemicals                                                0.040000   \n",
       "commercial_and_professional_services                     0.010000   \n",
       "consumer_services                                       -0.040000   \n",
       "consumer_staples                                        -0.010000   \n",
       "diversified_financials                                   0.003455   \n",
       "energy                                                  -0.008235   \n",
       "health_care_equipment_and_services                      -0.050000   \n",
       "insurance                                               -0.020000   \n",
       "media_and_entertainment                                 -0.118421   \n",
       "metals_and_mining                                       -0.003750   \n",
       "pharmaceuticals_biotechnology_and_life_sciences          0.015479   \n",
       "real_estate                                              0.047143   \n",
       "retailing                                                0.065000   \n",
       "semiconductors_and_semiconductor_equipment               0.012963   \n",
       "software_and_services                                    0.030612   \n",
       "technology_hardware_and_equipment                        0.177273   \n",
       "telecommunication_services                              -0.152222   \n",
       "transportation                                           0.041667   \n",
       "\n",
       "                                                 return_two_month  \\\n",
       "subindustry                                                         \n",
       "automobiles_and_components                               0.040000   \n",
       "banks                                                   -0.022143   \n",
       "capital_goods                                            0.009535   \n",
       "chemicals                                                0.035000   \n",
       "commercial_and_professional_services                     0.000000   \n",
       "consumer_services                                       -0.030000   \n",
       "consumer_staples                                        -0.020000   \n",
       "diversified_financials                                   0.000182   \n",
       "energy                                                   0.065294   \n",
       "health_care_equipment_and_services                       0.002143   \n",
       "insurance                                               -0.020000   \n",
       "media_and_entertainment                                 -0.076316   \n",
       "metals_and_mining                                       -0.005000   \n",
       "pharmaceuticals_biotechnology_and_life_sciences         -0.034795   \n",
       "real_estate                                              2.905714   \n",
       "retailing                                                0.066250   \n",
       "semiconductors_and_semiconductor_equipment               0.022963   \n",
       "software_and_services                                    0.079592   \n",
       "technology_hardware_and_equipment                        0.257273   \n",
       "telecommunication_services                              -0.108889   \n",
       "transportation                                           0.055000   \n",
       "\n",
       "                                                 return_three_month  \n",
       "subindustry                                                          \n",
       "automobiles_and_components                                 0.040000  \n",
       "banks                                                     -0.007857  \n",
       "capital_goods                                             -0.021395  \n",
       "chemicals                                                  0.055000  \n",
       "commercial_and_professional_services                      -0.040000  \n",
       "consumer_services                                         -0.025000  \n",
       "consumer_staples                                          -0.030000  \n",
       "diversified_financials                                    -0.005818  \n",
       "energy                                                    -0.012353  \n",
       "health_care_equipment_and_services                        -0.052143  \n",
       "insurance                                                 -0.040000  \n",
       "media_and_entertainment                                   -0.061053  \n",
       "metals_and_mining                                          0.032500  \n",
       "pharmaceuticals_biotechnology_and_life_sciences           -0.133973  \n",
       "real_estate                                                2.791429  \n",
       "retailing                                                  0.138750  \n",
       "semiconductors_and_semiconductor_equipment                 0.051111  \n",
       "software_and_services                                      0.082857  \n",
       "technology_hardware_and_equipment                          0.018182  \n",
       "telecommunication_services                                -0.090000  \n",
       "transportation                                             0.076667  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['last_residual'].abs() > 5][['subindustry', 'return_one_month', 'return_two_month', 'return_three_month']].groupby('subindustry').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['subindustry', 'return_one_month']].groupby('subindustry').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
