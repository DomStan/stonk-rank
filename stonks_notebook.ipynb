{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dstan\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:61: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import Int64Index as NumericIndex\n"
     ]
    }
   ],
   "source": [
    "# Utils\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from itertools import combinations\n",
    "from functools import partial\n",
    "\n",
    "# Data management\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Data fetching\n",
    "import yfinance as yf\n",
    "\n",
    "# Spread generation\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stonk price data download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input ticker names by industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tickers_by_industry(industries=None, data_dir=None, filename=None):\n",
    "    '''\n",
    "    Read the CSV file containing all tickers and their subindustries and return tickers from the selected subindustries in a list.\n",
    "    \n",
    "    -Args:\n",
    "        industries (List(string)): if not given, return all tickers; else the list can contain:\n",
    "            'technology_hardware_and_equipment'\n",
    "            'software_and_services'\n",
    "            'media_and_entertainment'\n",
    "            'retailing'\n",
    "            'automobiles_and_components'\n",
    "            'semiconductors_and_semiconductor_equipment'\n",
    "            'health_care_equipment_and_services'\n",
    "            'banks'\n",
    "            'pharmaceuticals_biotechnology_and_life_sciences'\n",
    "            'food_and_staples_retailing'\n",
    "            'oil_gas_and_consumable_fuels'\n",
    "            'food_beverage_and_tobacco'\n",
    "            'telecommunication_services'\n",
    "            'consumer_durables_and_apparel'\n",
    "            'consumer_services'\n",
    "            'transportation'\n",
    "            'diversified_financials'\n",
    "            'utilities'\n",
    "            'capital_goods'\n",
    "            'insurance'\n",
    "            'chemicals'\n",
    "            'metals_and_mining'\n",
    "            'commercial_and_professional_services'\n",
    "            'containers_and_packaging'\n",
    "            'energy_equipment_and_services'\n",
    "            'construction_materials'\n",
    "            'paper_and_forest_products'\n",
    "    \n",
    "    -Returns:\n",
    "        tickers (pandas Series): list of selected ticker names\n",
    "    '''\n",
    "    filename = 'stonk_list.csv' if filename is None else filename\n",
    "    data_dir = 'data' if data_dir is None else data_dir\n",
    "    \n",
    "    path_to_csv = os.path.join(data_dir, filename)\n",
    "    stonk_list = pd.read_csv(path_to_csv)\n",
    "    return stonk_list.set_index('ticker') if industries is None else stonk_list[stonk_list['subindustry'].isin(industries)].set_index('ticker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_stonk_prices(stonk_list, period_years=3, date_from=None, date_to=None, interval='1d', source='yfinance', data_dir='data', proxy=False):    \n",
    "    '''\n",
    "    Returns historical price data for the selected stonks.\n",
    "\n",
    "    -Args:\n",
    "        stonk_list (List(string)): List of stonk identifiers as strings, case unsensitive\n",
    "        period_years (float): How many years of data to download until date_to, can be a floating point number\n",
    "    -Optional:\n",
    "        date_from (datetime): Start date for stonk data (use instead of period_years)\n",
    "        date_to (datetime): End date for stonk data\n",
    "        interval (string): Valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
    "        source (string): Where to source data from. Valid sources: yfinance\n",
    "        data_dir (string): Folder name where to output downloaded data\n",
    "        file_prefix (string): Prefix of CSV file containing downloaded data inside data_dir\n",
    "        proxy (boolean): Whether to use a proxy connection to avoid API limits/blocks\n",
    "                \n",
    "    -Returns:\n",
    "        stonks (Pandas Dataframe): Pandas Dataframe containing requested ticker prices\n",
    "    '''\n",
    "    \n",
    "    date_to = datetime.now() if date_to is None else date_to\n",
    "    date_from = date_to-(timedelta(days=int(365*period_years))) if date_from is None else date_from\n",
    "    \n",
    "    if source.lower() == 'yfinance':\n",
    "        stonks = yf.download(list(stonk_list), start=date_from, end=date_to, interval=interval, group_by='column', threads=True, rounding=True)['Adj Close']\n",
    "        stonks.dropna(axis=0, how='all', inplace=True)\n",
    "        stonks.sort_values(by='Date', inplace=True)\n",
    "        \n",
    "        stonks.index = pd.to_datetime(stonks.index).date\n",
    "        stonks.index.name = 'date'\n",
    "\n",
    "        clean_stonks = stonks.dropna(axis=1, how='all', thresh=len(stonks.index) * 0.95).copy()\n",
    "        clean_stonks.dropna(axis=0, how='all', thresh=len(clean_stonks.columns) * 0.95, inplace=True)\n",
    "        clean_stonks.fillna(axis=1, method='ffill', inplace=True)\n",
    "        clean_stonks.dropna(axis=1, how='any', inplace=True)\n",
    "        \n",
    "        # Must be no NA values left\n",
    "        assert clean_stonks.isna().sum().sum() == 0\n",
    "    else:\n",
    "        raise ValueError('Unsupported data source')\n",
    "        \n",
    "    def stonks_to_csv(stonks, clean):\n",
    "        from_date_string = stonks.index[0]\n",
    "        to_date_string = stonks.index[-1]\n",
    "\n",
    "        filename = 'stonks_{from_date}_to_{to_date}.csv'.format(from_date=from_date_string, to_date=to_date_string)\n",
    "        \n",
    "        if clean:\n",
    "            filename = 'clean_' + filename\n",
    "            \n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "\n",
    "        stonks.to_csv(path_or_buf=file_path, header=True, index=True, na_rep='NaN')\n",
    "    \n",
    "    stonks_to_csv(stonks, clean=False)\n",
    "    stonks_to_csv(clean_stonks, clean=True)\n",
    "    \n",
    "    return (stonks, clean_stonks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock price data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stonk_data(date_from, date_to, clean=True, date_index=False, data_dir=None):\n",
    "    data_dir = 'data' if data_dir is None else data_dir\n",
    "    data_prefix = 'clean_stonks' if clean else 'stonks'\n",
    "    \n",
    "    path = os.path.join(data_dir, '{}_{}_to_{}.csv'.format(data_prefix, date_from, date_to))\n",
    "    stonks = pd.read_csv(path, header=0, index_col=0)\n",
    "    \n",
    "    if clean:\n",
    "        assert stonks.isna().sum().sum() == 0\n",
    "    \n",
    "    if date_index:\n",
    "        return stonks\n",
    "    else:\n",
    "        return stonks.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stonk_data_by_industry(date_from, date_to, clean=True, date_index=False, industries=None, stonk_list_filename=None, data_dir=None):\n",
    "    '''\n",
    "    Read the CSV file containing all stonk price data and return the tickers from the selected subindustries.\n",
    "    \n",
    "    -Args:\n",
    "        industries (List(string)): if not given, return all tickers; else the list can contain:\n",
    "            'technology_hardware_and_equipment'\n",
    "            'software_and_services'\n",
    "            'media_and_entertainment'\n",
    "            'retailing'\n",
    "            'automobiles_and_components'\n",
    "            'semiconductors_and_semiconductor_equipment'\n",
    "            'health_care_equipment_and_services'\n",
    "            'banks'\n",
    "            'pharmaceuticals_biotechnology_and_life_sciences'\n",
    "            'food_and_staples_retailing'\n",
    "            'oil_gas_and_consumable_fuels'\n",
    "            'food_beverage_and_tobacco'\n",
    "            'telecommunication_services'\n",
    "            'consumer_durables_and_apparel'\n",
    "            'consumer_services'\n",
    "            'transportation'\n",
    "            'diversified_financials'\n",
    "            'utilities'\n",
    "            'capital_goods'\n",
    "            'insurance'\n",
    "            'chemicals'\n",
    "            'metals_and_mining'\n",
    "            'commercial_and_professional_services'\n",
    "            'containers_and_packaging'\n",
    "            'energy_equipment_and_services'\n",
    "            'construction_materials'\n",
    "            'paper_and_forest_products'\n",
    "    \n",
    "    -Returns:\n",
    "        stonks (pandas DataFrame): list of selected tickers' price data\n",
    "    '''\n",
    "    all_stonks = read_stonk_data(date_from, date_to, date_index=date_index, data_dir=data_dir, clean=clean)\n",
    "    \n",
    "    if industries is None or not industries:\n",
    "        return all_stonks\n",
    "    else: \n",
    "        all_tickers = get_tickers_by_industry(industries=None, data_dir=data_dir, filename=stonk_list_filename)\n",
    "        all_stonks = all_stonks.join(all_tickers, how='inner')\n",
    "        return all_stonks[all_stonks['subindustry'].isin(industries)].drop(columns='subindustry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make combinations with numpy\n",
    "def combine_stonk_pairs(stonks_prices):\n",
    "    # All ticker names must be unique\n",
    "    assert all(stonks_prices.index.unique() == stonks_prices.index)\n",
    "    assert(len(stonks_prices) < 300)\n",
    "    \n",
    "    combs = np.asarray(list(combinations(stonks_prices.index.unique(), 2)))\n",
    "    \n",
    "    return stonks_prices.loc[combs[:, 0]], stonks_prices.loc[combs[:, 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Linear regression residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_residuals_many(X, Y):\n",
    "    '''\n",
    "    Vectorized calculation of residuals from many univariate linear regressions.\n",
    "        Args:\n",
    "        - X (numpy array of shape (n_pairs, d_time)): matrix of LR inputs X, each row represents a different regression, corresponding to the same rows in Y\n",
    "        - Y (numpy array of shape (n_pairs, d_time)): matrix of LR inputs Y, each row represents a different regression, corresponding to the same rows in X\n",
    "        Returns:\n",
    "        - residuals (numpy array of shape (n_pairs, d_time)): matrix of resulting residuals between vectorized pairs of X and Y\n",
    "        - betas (numpy array of shape (n_pairs, 1)): beta coefficients for each linear regression\n",
    "        - Y_hat (numpy array of shape (n_pairs, d_time)): predictions using X\n",
    "    '''\n",
    "    # Stack 2D matrices into 3D matrices\n",
    "    X = X.reshape(np.shape(X)[0], np.shape(X)[1], -1)\n",
    "    Y = Y.reshape(np.shape(Y)[0], np.shape(Y)[1], -1)\n",
    "    \n",
    "    # Add bias/intercept in the form (Xi, 1)\n",
    "    Z = np.concatenate([X, np.ones((np.shape(X)[0], np.shape(X)[1], 1))], axis=2)\n",
    "    \n",
    "    # Save the transpose as it's used a couple of times\n",
    "    Z_t = Z.transpose(0, 2, 1)\n",
    "    \n",
    "    # Linear Regression equation solutions w.r.t. weight matrix\n",
    "    # W contains (beta_coef, a_intercept) for each regression\n",
    "    W = np.matmul(np.linalg.inv(np.matmul(Z_t, Z)),  np.matmul(Z_t, Y))\n",
    "    \n",
    "    # Predictions and residuals\n",
    "    # Y_hat = np.matmul(Z, W).round(2)\n",
    "    residuals = (Y - np.matmul(Z, W)).round(2)\n",
    "    \n",
    "    # Y_hat returned for debugging purposes\n",
    "    # return (residuals[:, :, 0], W[:, 0, 0], Y_hat[:, :, 0])\n",
    "    return (residuals[:, :, 0], W[:, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rolling_residuals(X, Y, l_reg, l_roll, dt, write_csv=False, data_dir='data'):\n",
    "    '''\n",
    "    Calculates rolling window residuals in vectorized form. Returns the result as an array that repeats each ticker for the number of regressions calculated.\n",
    "    For example, if the inputs are (Pair A, Pair B, Pair C) and l_roll / dt = 3, then the returned results will have the form as follows:\n",
    "    (Pair A, Pair A, Pair A, Pair B, Pair B, Pair B, Pair C, Pair C, Pair C)\n",
    "    Works best when l_reg and l_roll are integers.\n",
    "        Args:\n",
    "        - X (DataFrame of shape (n_pairs, >= l_reg + l_roll)): matrix of LR inputs X, each row representing not less than complete data period for rolling regressions (can be longer)\n",
    "        - Y (DataFrame of shape (n_pairs, >= l_reg + l_roll)): matrix of LR inputs Y, each row representing not less than complete data period for rolling regressions (can be longer)\n",
    "        - l_reg (float): length of each LR to calculate residuals, in years; will be multiplied by the adjusted number of days in a trading year\n",
    "        - l_roll (float): length of rolling window, in years; will be multipled by the adjusted number of days in a trading year\n",
    "        - dt (int): rolling window step size, in trading days; total trading year days will be reduced to be divisible by dt (by not more than the value of dt)\n",
    "        Returns:\n",
    "        - residuals (numpy array of shape (n_pairs * (l_roll/dt)+1, l_reg + l_roll)): matrix of resulting residuals between vectorized pairs of X and Y\n",
    "        - betas (numpy array of shape (n_pairs * (l_roll/dt)+1, 1)): beta coefficients for each linear regression\n",
    "        - Y_hat (numpy array of shape (n_pairs * (l_roll/dt)+1, l_reg + l_roll)): predictions using X\n",
    "    '''\n",
    "    \n",
    "    _DAYS_IN_TRADING_YEAR = 252\n",
    "    \n",
    "    # Adjust days in a year so that the number is divisible by dt\n",
    "    _DAYS_IN_TRADING_YEAR = _DAYS_IN_TRADING_YEAR - (_DAYS_IN_TRADING_YEAR % dt)\n",
    "    l_reg_days = int(_DAYS_IN_TRADING_YEAR * l_reg)\n",
    "    l_roll_days = int(_DAYS_IN_TRADING_YEAR * l_roll)\n",
    "    total_days = l_reg_days + l_roll_days\n",
    "    \n",
    "    # Number of regressions for each ticker\n",
    "    n_windows = (l_roll_days // dt) + 1\n",
    "    \n",
    "    # Number of tickers\n",
    "    n_x = X.shape[0]\n",
    "    \n",
    "    # Take the dates, create an empty array for windowed dates\n",
    "    date_index = X.columns[-total_days:]\n",
    "    date_index_windowed = np.empty(shape=(n_x*n_windows, 2), dtype='O')\n",
    "    \n",
    "    # Repeat each ticker name times n_windows\n",
    "    X_index = np.repeat(X.index, n_windows)\n",
    "    Y_index = np.repeat(Y.index, n_windows)\n",
    "    \n",
    "    # X and Y must have the same dates\n",
    "    assert np.array_equal(X.columns, Y.columns)\n",
    "    \n",
    "    X = X.to_numpy().astype(np.float32)\n",
    "    Y = Y.to_numpy().astype(np.float32)\n",
    "    \n",
    "    # Rolling window length must be divisible by dt\n",
    "    assert (l_roll_days % dt) == 0\n",
    "    \n",
    "    # There has to be enough days' worth of data in X (and Y) and their shapes must match\n",
    "    assert X.shape == Y.shape and X.shape[1] >= total_days\n",
    "    \n",
    "    # Take the total_days from the end of the arrays (most recent days first, oldest days at the end are cut off)\n",
    "    X = X[:, -total_days:]\n",
    "    Y = Y[:, -total_days:]\n",
    "    \n",
    "    # Create empty arrays that will contain windowed slices of our data\n",
    "    X_windows = np.empty(shape=(n_x*n_windows, l_reg_days))\n",
    "    Y_windows = np.empty(shape=(n_x*n_windows, l_reg_days))\n",
    "    \n",
    "    # Take windowed slices and place them into the created empty arrays\n",
    "    for n in range(n_x):\n",
    "        for i in range(n_windows):\n",
    "            n_i = (n*n_windows)+i\n",
    "            t_i = i*dt\n",
    "            t_y = t_i + l_reg_days\n",
    "            \n",
    "            X_windows[n_i] = X[n, t_i:t_y]\n",
    "            Y_windows[n_i] = Y[n, t_i:t_y]\n",
    "            date_index_windowed[n_i, 0] = date_index[t_i]\n",
    "            date_index_windowed[n_i, 1] = date_index[t_y-1]\n",
    "    \n",
    "    # Make sure we've got the windowing dimensions right\n",
    "    assert X_windows.shape == (n_x*n_windows, l_reg_days) and Y_windows.shape == (n_x*n_windows, l_reg_days)\n",
    "    \n",
    "    # Sanity check\n",
    "    assert all([\n",
    "        X[0, -1] == X_windows[n_windows-1, -1],\n",
    "        Y[0, -1] == Y_windows[n_windows-1, -1],\n",
    "        X[-1, -1] == X_windows[-1, -1],\n",
    "        Y[-1, -1] == Y_windows[-1, -1],\n",
    "    ])\n",
    "    \n",
    "    # Construct ticker pair index column\n",
    "    pair_index = np.array(pd.DataFrame(np.array([X_index, Y_index])).apply('_'.join, axis=0, raw=True))\n",
    "    \n",
    "    # Construct regression date range index column\n",
    "    date_index = np.array(pd.DataFrame(np.array([date_index_windowed[:, 0], date_index_windowed[:, 1]])).apply('_'.join, axis=0, raw=True))\n",
    "    \n",
    "    # Lengths of indexes must match\n",
    "    assert len(pair_index) == len(date_index)\n",
    "    \n",
    "    # Calculate and return the residuals\n",
    "    res, betas = get_residuals_many(X_windows, Y_windows)\n",
    "    \n",
    "    res = pd.DataFrame(res, index=pair_index)\n",
    "    res.insert(0, 'dates', date_index)\n",
    "    betas = pd.DataFrame(betas, index=pair_index)\n",
    "    betas.insert(0, 'dates', date_index)\n",
    "    \n",
    "    if write_csv:\n",
    "        time = datetime.now().time()\n",
    "        res.to_csv(os.path.join(data_dir, time.strftime('residuals_%H%M%S.csv')), header=False, index=True)\n",
    "        betas.to_csv(os.path.join(data_dir, time.strftime('betas_%H%M%S.csv')), header=False, index=True)\n",
    "        pd.Series(date_index).to_csv(os.path.join(data_dir, time.strftime('dates_%H%M%S.csv')), header=False, index=False)\n",
    "    \n",
    "    return res, betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADF testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adfs(residuals, adf_regression):\n",
    "    adfres = np.empty(shape=(len(residuals),), dtype=np.float32)\n",
    "    adfres = np.apply_along_axis(lambda x: adfuller(x, regression=adf_regression, autolag=None)[1], axis=1, arr=residuals)\n",
    "    return adfres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregate_adfs(residuals, cutoff=0.1, adf_regression='c', write_csv=False, data_dir='data'):\n",
    "    adfs = pd.DataFrame(get_adfs(residuals.drop(columns='dates').to_numpy().astype(np.float32), adf_regression=adf_regression), index=residuals.index)\n",
    "    \n",
    "    unique_pairs = adfs.index.unique()\n",
    "    \n",
    "    adfs_aggregated = np.empty(shape=(len(unique_pairs),), dtype=np.float32)\n",
    "    \n",
    "    i = 0\n",
    "    for pair in unique_pairs:\n",
    "        pairs = adfs.loc[pair]\n",
    "        adfs_aggregated[i] = (pairs < cutoff).sum() / len(pairs)\n",
    "        i+= 1\n",
    "        \n",
    "    adfs_aggregated = pd.DataFrame(adfs_aggregated, index=unique_pairs)\n",
    "    \n",
    "    if write_csv:\n",
    "        time = datetime.now().time()\n",
    "        adfs_aggregated.to_csv(os.path.join(data_dir, time.strftime('adfs_%H%M%S.csv')), header=False, index=True)\n",
    "        \n",
    "    return adfs_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adfres = measure_time(partial(get_adfs, residuals, 'c'))\n",
    "# adfres.drop(columns=[4, 5], inplace=True)\n",
    "# adfres['rejected'] = (adfres[1] < 0.05).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "adfres_noauto = measure_time(partial(get_adfs, residuals, 'c'))\n",
    "# adfres_noauto.drop(columns=[4], inplace=True)\n",
    "adfres_noauto['rejected'] = (adfres_noauto[1] < 0.05).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 7s\n"
     ]
    }
   ],
   "source": [
    "adfres_noauto_nolag = measure_time(partial(get_adfs, residuals, 'c'))\n",
    "# adfres_noauto_nolag.drop(columns=[4], inplace=True)\n",
    "adfres_noauto_nolag['rejected'] = (adfres_noauto_nolag[1] < 0.05).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11538461538461539"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(adfres)['rejected'].sum() / len(adfres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.078"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(adfres_noauto)['rejected'].sum() / len(adfres_noauto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15169230769230768"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(adfres_noauto_nolag)['rejected'].sum() / len(adfres_noauto_nolag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06753846153846153"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((adfres)['rejected']-(adfres_noauto)['rejected']).abs().sum() / len(adfres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.062461538461538464"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((adfres)['rejected']-(adfres_noauto_nolag)['rejected']).abs().sum() / len(adfres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None,\n",
    "                       'display.precision', 6,\n",
    "                       ):\n",
    "    print(adfres-adfres_noauto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 20s\n"
     ]
    }
   ],
   "source": [
    "adfres_ct = measure_time(partial(get_adfs, residuals, 'ct'))\n",
    "adfres_n = measure_time(partial(get_aggregate_adfs, residuals, 'n'))\n",
    "adfres.insert(1, 'n', value=adfres_n[0])\n",
    "adfres.insert(1, 'ct', value=adfres_ct[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(adfres.index == residuals.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ACEL_ARMK</th>\n",
       "      <td>0.233818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACEL_ARMK</th>\n",
       "      <td>0.185407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACEL_ARMK</th>\n",
       "      <td>0.173262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACEL_ARMK</th>\n",
       "      <td>0.126130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACEL_ARMK</th>\n",
       "      <td>0.133973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BFAM_LOPE</th>\n",
       "      <td>0.337138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BFAM_LOPE</th>\n",
       "      <td>0.245955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BFAM_LOPE</th>\n",
       "      <td>0.149984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BFAM_LOPE</th>\n",
       "      <td>0.104871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BFAM_LOPE</th>\n",
       "      <td>0.025434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6500 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "ACEL_ARMK  0.233818\n",
       "ACEL_ARMK  0.185407\n",
       "ACEL_ARMK  0.173262\n",
       "ACEL_ARMK  0.126130\n",
       "ACEL_ARMK  0.133973\n",
       "...             ...\n",
       "BFAM_LOPE  0.337138\n",
       "BFAM_LOPE  0.245955\n",
       "BFAM_LOPE  0.149984\n",
       "BFAM_LOPE  0.104871\n",
       "BFAM_LOPE  0.025434\n",
       "\n",
       "[6500 rows x 1 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adfres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline example tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Import tickers from given custom list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/selected_spreads.csv'\n",
    "ticker_pairs = pd.read_csv(filename, header=0)\n",
    "ticker_pairs['x'] = ticker_pairs['spreads'].apply(lambda x: x.split('_')[0])\n",
    "ticker_pairs['y'] = ticker_pairs['spreads'].apply(lambda x: x.split('_')[1])\n",
    "ticker_pairs.set_index('spreads', inplace=True)\n",
    "\n",
    "ticker_pairs = ticker_pairs.loc[~ticker_pairs.index.str.contains(r'CIT|LORL|ENBL|MDP')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Download stock daily prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets all ticker names (no argument given)\n",
    "# ticker_list = get_tickers_by_industry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_list = set(list(ticker_pairs['x']) + list(ticker_pairs['y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific date - 3rd of March 2022 (Y, M, D)\n",
    "# date_to = datetime(2022, 3, 1)\n",
    "# Date of today\n",
    "date_to = datetime.today()\n",
    "# How many years' of data to download (going backwards from date_end). Year can be a floating point number\n",
    "period_years = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1041 of 1041 completed\n",
      "\n",
      "3 Failed downloads:\n",
      "- CIT: No data found, symbol may be delisted\n",
      "- LORL: No data found, symbol may be delisted\n",
      "- ENBL: No data found, symbol may be delisted\n"
     ]
    }
   ],
   "source": [
    "# Download ticker price data for the tickers selected above (saved to .csv automatically)\n",
    "df, df_clean = download_stonk_prices(ticker_list, period_years=period_years, date_to=date_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Read stock data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'technology_hardware_and_equipment'\n",
    "'software_and_services'\n",
    "'media_and_entertainment'\n",
    "'retailing'\n",
    "'automobiles_and_components'\n",
    "'semiconductors_and_semiconductor_equipment'\n",
    "'health_care_equipment_and_services'\n",
    "'banks'\n",
    "'pharmaceuticals_biotechnology_and_life_sciences'\n",
    "'food_and_staples_retailing'\n",
    "'oil_gas_and_consumable_fuels'\n",
    "'food_beverage_and_tobacco'\n",
    "'telecommunication_services'\n",
    "'consumer_durables_and_apparel'\n",
    "'consumer_services'\n",
    "'transportation'\n",
    "'diversified_financials'\n",
    "'utilities'\n",
    "'capital_goods'\n",
    "'insurance'\n",
    "'chemicals'\n",
    "'metals_and_mining'\n",
    "'commercial_and_professional_services'\n",
    "'containers_and_packaging'\n",
    "'energy_equipment_and_services'\n",
    "'construction_materials'\n",
    "'paper_and_forest_products'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stock data of ALL industries (all tickers) - no arguments specified\n",
    "stonks = get_stonk_data_by_industry('2018-03-16', '2022-03-14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stock data from selected industries only\n",
    "# stonks = get_stonk_data_by_industry('2018-03-12', '2022-03-11', industries=['consumer_durables_and_apparel', 'consumer_services'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, Y = combine_stonk_pairs(stonks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Select spreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = stonks.loc[ticker_pairs['x']]\n",
    "Y = stonks.loc[ticker_pairs['y']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Calculate residuals (exported to CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 9s\n"
     ]
    }
   ],
   "source": [
    "residuals, betas = measure_time(partial(get_rolling_residuals, X, Y, l_reg=3, l_roll=1, dt=20, write_csv=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 196s\n"
     ]
    }
   ],
   "source": [
    "adfs = measure_time(partial(get_aggregate_adfs, residuals, write_csv=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_time(func):\n",
    "    t1 = time.time()\n",
    "    ret = func()\n",
    "    t2 = time.time()\n",
    "    print(\"Time: \" + str(int(t2-t1)) + 's')\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Slow residual functions (for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_rolling_slow_residuals(X, Y, l_reg, l_roll, dt):\n",
    "#     _DAYS_IN_TRADING_YEAR = (252) - (252 % dt)\n",
    "#     l_reg_days = _DAYS_IN_TRADING_YEAR * l_reg\n",
    "#     l_roll_days = _DAYS_IN_TRADING_YEAR * l_roll\n",
    "#     total_days = l_reg_days + l_roll_days\n",
    "#     n_windows = l_roll_days // dt\n",
    "#     n_x = X.shape[0]\n",
    "    \n",
    "#     assert (l_roll_days % dt) == 0\n",
    "#     assert X.shape[1] >= total_days and Y.shape[1] >= total_days\n",
    "    \n",
    "#     X = X[:, -total_days:]\n",
    "#     Y = Y[:, -total_days:]\n",
    "    \n",
    "#     # First window\n",
    "#     X_windows = np.empty(shape=(n_x*n_windows, l_reg_days))\n",
    "#     Y_windows = np.empty(shape=(n_x*n_windows, l_reg_days))\n",
    "    \n",
    "#     for n in range(n_x):\n",
    "#         for i in range(n_windows):\n",
    "#             X_windows = np.concatenate(( X_windows, X[n, i*dt:l_reg_days+(i*dt)] ))\n",
    "#             Y_windows = np.concatenate(( Y_windows, Y[n, i*dt:l_reg_days+(i*dt)] ))\n",
    "    \n",
    "#     assert X_windows.shape == (n_x*n_windows, l_reg_days) and Y_windows.shape == (n_x*n_windows, l_reg_days)\n",
    "    \n",
    "#     return get_slow_residuals_many(X_windows, Y_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_slow_residuals_many(X, Y, n_jobs=-1):\n",
    "#     lr = LinearRegression(n_jobs=n_jobs, fit_intercept=True)\n",
    "#     X = X.reshape((X.shape[0], X.shape[1], -1))\n",
    "#     Y = Y.reshape((Y.shape[0], Y.shape[1], -1))\n",
    "    \n",
    "#     preds = []\n",
    "#     res = []\n",
    "#     betas = []\n",
    "#     for i in range(X.shape[0]):\n",
    "#         lr.fit(X[i], Y[i])\n",
    "#         preds.append(lr.predict(X[i]).round(2))\n",
    "#         res.append(Y[i]-preds[-1])\n",
    "#         betas.append(lr.coef_[0][0])\n",
    "#     return (np.asarray(res)[:,:,0], np.asarray(preds)[:,:,0], np.asarray(betas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1_fast = time.time()\n",
    "# res, betas, preds  = get_rolling_residuals(X, Y, l_reg=2, l_roll=1, dt=5)\n",
    "# t2_fast = time.time()\n",
    "\n",
    "# t1_slow = time.time()\n",
    "# res_slow, preds_slow = get_rolling_slow_residuals(X, Y, l_reg=2, l_roll=1, dt=5)\n",
    "# t2_slow = time.time()\n",
    "\n",
    "# print(\"Time slow: \" + str(t2_slow-t1_slow))\n",
    "# print(\"Time fast: \" + str(t2_fast-t1_fast))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1_fast = time.time()\n",
    "# res, preds, betas = get_residuals_many(X, Y)\n",
    "# t2_fast = time.time()\n",
    "\n",
    "# t1_slow = time.time()\n",
    "# res_slow, preds_slow, betas_slow = get_slow_residuals_many(X, Y)\n",
    "# t2_slow = time.time()\n",
    "\n",
    "# print(\"Time slow: \" + str(t2_slow-t1_slow))\n",
    "# print(\"Time fast: \" + str(t2_fast-t1_fast))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Stock list preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_stock_list(raw_data_path='data/raw_stonk_list.xls', output_path='data/stonk_list.csv'):\n",
    "    '''\n",
    "    Parses a raw excel file from CapitalIQ containing ticker names and their subindustries, validates\n",
    "    unusual ticker names with Yahoo Finance, saving the processed data in CSV format.\n",
    "\n",
    "        Parameters:\n",
    "            Required:\n",
    "                raw_data_path (string):\n",
    "                    Path to the raw excel file.\n",
    "                output_path (string):\n",
    "                    Path where to save the parsed data.\n",
    "                \n",
    "        Returns:\n",
    "            Nothing\n",
    "    '''\n",
    "    \n",
    "    df = pd.read_excel(io=raw_data_path)\n",
    "    \n",
    "    # Drop NA rows\n",
    "    df.dropna(axis=0, inplace=True)\n",
    "    \n",
    "    # Reset index and drop the first row\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    df.drop(index=0, axis=0, inplace=True)\n",
    "    \n",
    "    # Drop unwanted columns\n",
    "    df.drop(columns=df.columns[[1, 2, 3, 4, 5, 7, 8, 9]], inplace=True)\n",
    "    \n",
    "    # Rename remaining columns\n",
    "    df.columns = ['ticker', 'subindustry']\n",
    "    \n",
    "    # Remove the '(Primary)' tag from subindustries\n",
    "    df['subindustry'] = df['subindustry'].str.replace(r' \\(Primary\\)', '')\n",
    "    \n",
    "    # Remove everything until (and including) the semicolon for tickers\n",
    "    df['ticker'] = df['ticker'].str.replace(r'(.*:)', '')\n",
    "    \n",
    "    df['ticker'] = df['ticker'].str.replace(r' WI', '.VI')\n",
    "    df['ticker'] = df['ticker'].str.replace(r'\\.WI', '.VI')\n",
    "    \n",
    "    # Replace the ticker endings for a Yahoo finance supported format\n",
    "    df['ticker'] = df['ticker'].str.replace(r'\\.PR', '-P')\n",
    "    # df['ticker'] = df['ticker'].str.replace(r' PR', '-P')\n",
    "    \n",
    "    # Take all remaining tickers that have a dot\n",
    "    dotted = df[df['ticker'].str.fullmatch(r'[A-Z]*\\.[A-Z]')]\n",
    "    \n",
    "    # Replace the dots with dashes\n",
    "    dashed = dotted.copy()\n",
    "    dashed['ticker'] = dashed['ticker'].str.replace(r'\\.', '-')\n",
    "    \n",
    "    # Remove the dots\n",
    "    undotted = dotted.copy()\n",
    "    undotted['ticker'] = undotted['ticker'].str.replace(r'\\.', '')\n",
    "\n",
    "    # Combine all variantas together\n",
    "    all_variants = pd.concat([dotted, dashed, undotted])\n",
    "    \n",
    "    # Run all of these through Yahoo finance, get last day's price\n",
    "    stonks = yf.download(list(all_variants['ticker'].astype('string').values), period='1m', interval='1d', group_by='column')\n",
    "    \n",
    "    # Drop all NA tickers (that failed to download)\n",
    "    valid_tickers = stonks['Adj Close'].iloc[-1].dropna(axis=0).to_frame().reset_index()\n",
    "    \n",
    "    # Rename columns\n",
    "    valid_tickers.columns = ['ticker', 'price']\n",
    "    \n",
    "    # Add subindustries to the remaining valid tickers\n",
    "    valid_tickers = valid_tickers.join(all_variants.set_index('ticker'), on='ticker')\n",
    "    \n",
    "    # Drop the price column\n",
    "    valid_tickers.drop(columns=valid_tickers.columns[[1]], inplace=True)\n",
    "    \n",
    "    # Remove all tickers that have a dot from main dataframe\n",
    "    df = df[~df['ticker'].str.fullmatch(r'[A-Z]*\\.[A-Z]')]\n",
    "    \n",
    "    # Add the validated tickers back\n",
    "    df = pd.concat([df, valid_tickers], axis=0, ignore_index=True)\n",
    "    \n",
    "    # Make the subindustry strings more code friendly\n",
    "    df['subindustry'] = df['subindustry'].str.replace(' ', '_')\n",
    "    df['subindustry'] = df['subindustry'].str.lower()\n",
    "    df['subindustry'] = df['subindustry'].str.replace(',', '')\n",
    "    \n",
    "    df.to_csv(path_or_buf=output_path, header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
