{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dstan\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:61: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import Int64Index as NumericIndex\n"
     ]
    }
   ],
   "source": [
    "# Utils\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from itertools import combinations\n",
    "from itertools import permutations\n",
    "from functools import partial\n",
    "\n",
    "# Data management\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Data fetching\n",
    "import yfinance as yf\n",
    "\n",
    "# Spread generation\n",
    "# from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stonk price data download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input ticker names by industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tickers_by_industry(industries=None, data_dir=None, filename=None):\n",
    "    '''\n",
    "    Read the CSV file containing all tickers and their subindustries and return tickers from the selected subindustries in a list.\n",
    "    \n",
    "    -Args:\n",
    "        industries (List(string)): if not given, return all tickers; else the list can contain:\n",
    "            'technology_hardware_and_equipment'\n",
    "            'software_and_services'\n",
    "            'media_and_entertainment'\n",
    "            'retailing'\n",
    "            'automobiles_and_components'\n",
    "            'semiconductors_and_semiconductor_equipment'\n",
    "            'health_care_equipment_and_services'\n",
    "            'banks'\n",
    "            'pharmaceuticals_biotechnology_and_life_sciences'\n",
    "            'food_and_staples_retailing'\n",
    "            'oil_gas_and_consumable_fuels'\n",
    "            'food_beverage_and_tobacco'\n",
    "            'telecommunication_services'\n",
    "            'consumer_durables_and_apparel'\n",
    "            'consumer_services'\n",
    "            'transportation'\n",
    "            'diversified_financials'\n",
    "            'utilities'\n",
    "            'capital_goods'\n",
    "            'insurance'\n",
    "            'chemicals'\n",
    "            'metals_and_mining'\n",
    "            'commercial_and_professional_services'\n",
    "            'containers_and_packaging'\n",
    "            'energy_equipment_and_services'\n",
    "            'construction_materials'\n",
    "            'paper_and_forest_products'\n",
    "    \n",
    "    -Returns:\n",
    "        tickers (pandas Series): list of selected ticker names\n",
    "    '''\n",
    "    filename = 'stonk_list.csv' if filename is None else filename\n",
    "    data_dir = 'data' if data_dir is None else data_dir\n",
    "    \n",
    "    path_to_csv = os.path.join(data_dir, filename)\n",
    "    stonk_list = pd.read_csv(path_to_csv)\n",
    "    return stonk_list.set_index('ticker') if industries is None else stonk_list[stonk_list['subindustry'].isin(industries)].set_index('ticker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_stonk_prices(stonk_list, period_years=3, date_from=None, date_to=None, interval='1d', source='yfinance', data_dir='data', proxy=False):    \n",
    "    '''\n",
    "    Returns historical price data for the selected stonks.\n",
    "\n",
    "    -Args:\n",
    "        stonk_list (List(string)): List of stonk identifiers as strings, case unsensitive\n",
    "        period_years (float): How many years of data to download until date_to, can be a floating point number\n",
    "    -Optional:\n",
    "        date_from (datetime): Start date for stonk data (use instead of period_years)\n",
    "        date_to (datetime): End date for stonk data\n",
    "        interval (string): Valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
    "        source (string): Where to source data from. Valid sources: yfinance\n",
    "        data_dir (string): Folder name where to output downloaded data\n",
    "        file_prefix (string): Prefix of CSV file containing downloaded data inside data_dir\n",
    "        proxy (boolean): Whether to use a proxy connection to avoid API limits/blocks\n",
    "                \n",
    "    -Returns:\n",
    "        stonks (Pandas Dataframe): Pandas Dataframe containing requested ticker prices\n",
    "    '''\n",
    "    \n",
    "    date_to = datetime.now() if date_to is None else date_to\n",
    "    date_from = date_to-(timedelta(days=int(365*period_years))) if date_from is None else date_from\n",
    "    \n",
    "    if source.lower() == 'yfinance':\n",
    "        stonks = yf.download(list(stonk_list), start=date_from, end=date_to, interval=interval, group_by='column', threads=True, rounding=True)['Adj Close']\n",
    "        stonks.dropna(axis=0, how='all', inplace=True)\n",
    "        stonks.sort_values(by='Date', inplace=True)\n",
    "        \n",
    "        stonks.index = pd.to_datetime(stonks.index).date\n",
    "        stonks.index.name = 'date'\n",
    "\n",
    "        clean_stonks = stonks.dropna(axis=1, how='all', thresh=int(len(stonks.index) * 0.99)).copy()\n",
    "        clean_stonks.dropna(axis=0, how='all', thresh=int(len(clean_stonks.columns) * 0.99), inplace=True)\n",
    "        \n",
    "        # Forward fill ticker columns (axis=0 for columns)\n",
    "        clean_stonks.fillna(axis=0, method='ffill', inplace=True)\n",
    "        \n",
    "        clean_stonks.dropna(axis=1, how='any', inplace=True)\n",
    "        \n",
    "        # Must be no NA values left\n",
    "        assert clean_stonks.isna().sum().sum() == 0\n",
    "    else:\n",
    "        raise ValueError('Unsupported data source')\n",
    "        \n",
    "    def stonks_to_csv(stonks, clean):\n",
    "        from_date_string = stonks.index[0]\n",
    "        to_date_string = stonks.index[-1]\n",
    "\n",
    "        filename = 'stonks_{from_date}_to_{to_date}.csv'.format(from_date=from_date_string, to_date=to_date_string)\n",
    "        \n",
    "        if clean:\n",
    "            filename = 'clean_' + filename\n",
    "            \n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "\n",
    "        stonks.to_csv(path_or_buf=file_path, header=True, index=True, na_rep='NaN')\n",
    "    \n",
    "    stonks_to_csv(stonks, clean=False)\n",
    "    stonks_to_csv(clean_stonks, clean=True)\n",
    "    \n",
    "    return (stonks, clean_stonks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock price data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stonk_data(date_from, date_to, clean=True, date_index=False, data_dir=None):\n",
    "    data_dir = 'data' if data_dir is None else data_dir\n",
    "    data_prefix = 'clean_stonks' if clean else 'stonks'\n",
    "    \n",
    "    path = os.path.join(data_dir, '{}_{}_to_{}.csv'.format(data_prefix, date_from, date_to))\n",
    "    stonks = pd.read_csv(path, header=0, index_col=0)\n",
    "    \n",
    "    if clean:\n",
    "        assert stonks.isna().sum().sum() == 0\n",
    "    \n",
    "    if date_index:\n",
    "        return stonks\n",
    "    else:\n",
    "        return stonks.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stonk_data_by_industry(date_from, date_to, clean=True, date_index=False, industries=None, stonk_list_filename=None, data_dir=None):\n",
    "    '''\n",
    "    Read the CSV file containing all stonk price data and return the tickers from the selected subindustries.\n",
    "    \n",
    "    -Args:\n",
    "        industries (List(string)): if not given, return all tickers; else the list can contain:\n",
    "            'technology_hardware_and_equipment'\n",
    "            'software_and_services'\n",
    "            'media_and_entertainment'\n",
    "            'retailing'\n",
    "            'automobiles_and_components'\n",
    "            'semiconductors_and_semiconductor_equipment'\n",
    "            'health_care_equipment_and_services'\n",
    "            'banks'\n",
    "            'pharmaceuticals_biotechnology_and_life_sciences'\n",
    "            'food_and_staples_retailing'\n",
    "            'oil_gas_and_consumable_fuels'\n",
    "            'food_beverage_and_tobacco'\n",
    "            'telecommunication_services'\n",
    "            'consumer_durables_and_apparel'\n",
    "            'consumer_services'\n",
    "            'transportation'\n",
    "            'diversified_financials'\n",
    "            'utilities'\n",
    "            'capital_goods'\n",
    "            'insurance'\n",
    "            'chemicals'\n",
    "            'metals_and_mining'\n",
    "            'commercial_and_professional_services'\n",
    "            'containers_and_packaging'\n",
    "            'energy_equipment_and_services'\n",
    "            'construction_materials'\n",
    "            'paper_and_forest_products'\n",
    "    \n",
    "    -Returns:\n",
    "        stonks (pandas DataFrame): list of selected tickers' price data\n",
    "    '''\n",
    "    all_stonks = read_stonk_data(date_from, date_to, date_index=date_index, data_dir=data_dir, clean=clean)\n",
    "    \n",
    "    if industries is None or not industries:\n",
    "        return all_stonks\n",
    "    else: \n",
    "        all_tickers = get_tickers_by_industry(industries=None, data_dir=data_dir, filename=stonk_list_filename)\n",
    "        all_stonks = all_stonks.join(all_tickers, how='inner')\n",
    "        return all_stonks[all_stonks['subindustry'].isin(industries)].drop(columns='subindustry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make combinations with numpy\n",
    "def combine_stonk_pairs(stonks_prices):\n",
    "    # All ticker names must be unique\n",
    "    assert all(stonks_prices.index.unique() == stonks_prices.index)\n",
    "    assert(len(stonks_prices) < 300)\n",
    "    \n",
    "    combs = np.asarray(list(combinations(stonks_prices.index.unique(), 2)))\n",
    "    \n",
    "    return stonks_prices.loc[combs[:, 0]], stonks_prices.loc[combs[:, 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Linear regression residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_residuals_many(X, Y):\n",
    "    '''\n",
    "    Vectorized calculation of residuals from many univariate linear regressions.\n",
    "        Args:\n",
    "        - X (numpy array of shape (n_pairs, d_time)): matrix of LR inputs X, each row represents a different regression, corresponding to the same rows in Y\n",
    "        - Y (numpy array of shape (n_pairs, d_time)): matrix of LR inputs Y, each row represents a different regression, corresponding to the same rows in X\n",
    "        Returns:\n",
    "        - residuals (numpy array of shape (n_pairs, d_time)): matrix of resulting residuals between vectorized pairs of X and Y\n",
    "        - betas (numpy array of shape (n_pairs, 1)): beta coefficients for each linear regression\n",
    "        - Y_hat (numpy array of shape (n_pairs, d_time)): predictions using X\n",
    "    '''\n",
    "    # Stack 2D matrices into 3D matrices\n",
    "    X = X.reshape(np.shape(X)[0], np.shape(X)[1], -1)\n",
    "    Y = Y.reshape(np.shape(Y)[0], np.shape(Y)[1], -1)\n",
    "    \n",
    "    # Add bias/intercept in the form (Xi, 1)\n",
    "    Z = np.concatenate([X, np.ones((np.shape(X)[0], np.shape(X)[1], 1))], axis=2)\n",
    "    \n",
    "    # Save the transpose as it's used a couple of times\n",
    "    Z_t = Z.transpose(0, 2, 1)\n",
    "    \n",
    "    # Linear Regression equation solutions w.r.t. weight matrix\n",
    "    # W contains (beta_coef, a_intercept) for each regression\n",
    "    W = np.matmul(np.linalg.inv(np.matmul(Z_t, Z)),  np.matmul(Z_t, Y))\n",
    "    \n",
    "    # Predictions and residuals\n",
    "    # Y_hat = np.matmul(Z, W).round(2)\n",
    "    residuals = (Y - np.matmul(Z, W)).round(2)\n",
    "    \n",
    "    # Y_hat returned for debugging purposes\n",
    "    # return (residuals[:, :, 0], W[:, 0, 0], Y_hat[:, :, 0])\n",
    "    return (residuals[:, :, 0], W[:, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rolling_residuals(X, Y, l_reg, l_roll, dt, write_csv=False, data_dir='data'):\n",
    "    '''\n",
    "    Calculates rolling window residuals in vectorized form. Returns the result as an array that repeats each ticker for the number of regressions calculated.\n",
    "    For example, if the inputs are (Pair A, Pair B, Pair C) and l_roll / dt = 3, then the returned results will have the form as follows:\n",
    "    (Pair A, Pair A, Pair A, Pair B, Pair B, Pair B, Pair C, Pair C, Pair C)\n",
    "    Works best when l_reg and l_roll are integers.\n",
    "        Args:\n",
    "        - X, Y (DataFrame of shape (n_pairs, >= l_reg + l_roll)): matrix of LR inputs X, Y; each row containing at least the complete data period for rolling regressions (can be longer)\n",
    "        - l_reg (float): length of each LR to calculate residuals, in years; will be multiplied by the adjusted number of days in a trading year\n",
    "        - l_roll (float): length of rolling window, in years; will be multipled by the adjusted number of days in a trading year\n",
    "        - dt (int): rolling window step size, in trading days; total trading year days will be reduced to be divisible by dt (by not more than the value of dt)\n",
    "        Returns:\n",
    "        - residuals (numpy array of shape (n_pairs * (l_roll/dt)+1, l_reg + l_roll)): matrix of resulting residuals between vectorized pairs of X and Y\n",
    "        - betas (numpy array of shape (n_pairs * (l_roll/dt)+1, 1)): beta coefficients for each linear regression\n",
    "        - Y_hat (numpy array of shape (n_pairs * (l_roll/dt)+1, l_reg + l_roll)): predictions using X\n",
    "    '''\n",
    "    \n",
    "    _DAYS_IN_TRADING_YEAR = 252\n",
    "    \n",
    "    # Adjust days in a year so that the number is divisible by dt\n",
    "    _DAYS_IN_TRADING_YEAR = _DAYS_IN_TRADING_YEAR - (_DAYS_IN_TRADING_YEAR % dt)\n",
    "    l_reg_days = int(_DAYS_IN_TRADING_YEAR * l_reg)\n",
    "    l_roll_days = int(_DAYS_IN_TRADING_YEAR * l_roll)\n",
    "    total_days = l_reg_days + l_roll_days\n",
    "    \n",
    "    # Number of regressions for each ticker\n",
    "    n_windows = (l_roll_days // dt) + 1\n",
    "    \n",
    "    # Number of tickers\n",
    "    n_x = X.shape[0]\n",
    "    \n",
    "    # Take the dates, create an empty array for windowed dates\n",
    "    date_index = X.columns[-total_days:]\n",
    "    date_index_windowed = np.empty(shape=(n_x*n_windows, 2), dtype='O')\n",
    "    \n",
    "    # Repeat each ticker name times n_windows\n",
    "    X_index = np.repeat(X.index, n_windows)\n",
    "    Y_index = np.repeat(Y.index, n_windows)\n",
    "    \n",
    "    # X and Y must have the same dates\n",
    "    assert np.array_equal(X.columns, Y.columns)\n",
    "    \n",
    "    X = X.to_numpy(dtype=np.float32)\n",
    "    Y = Y.to_numpy(dtype=np.float32)\n",
    "    \n",
    "    # Rolling window length must be divisible by dt\n",
    "    assert (l_roll_days % dt) == 0\n",
    "    \n",
    "    # There has to be enough days' worth of data in X (and Y) and their shapes must match\n",
    "    assert X.shape == Y.shape and X.shape[1] >= total_days\n",
    "    \n",
    "    # Take the total_days from the end of the arrays (most recent days first, oldest days at the end are cut off)\n",
    "    X = X[:, -total_days:]\n",
    "    Y = Y[:, -total_days:]\n",
    "    \n",
    "    # Create empty arrays that will contain windowed slices of our data\n",
    "    X_windows = np.empty(shape=(n_x*n_windows, l_reg_days))\n",
    "    Y_windows = np.empty(shape=(n_x*n_windows, l_reg_days))\n",
    "    \n",
    "    # Take windowed slices and place them into the created empty arrays\n",
    "    for n in range(n_x):\n",
    "        for i in range(n_windows):\n",
    "            n_i = (n*n_windows)+i\n",
    "            t_i = i*dt\n",
    "            t_y = t_i + l_reg_days\n",
    "            \n",
    "            X_windows[n_i] = X[n, t_i:t_y]\n",
    "            Y_windows[n_i] = Y[n, t_i:t_y]\n",
    "            date_index_windowed[n_i, 0] = date_index[t_i]\n",
    "            date_index_windowed[n_i, 1] = date_index[t_y-1]\n",
    "    \n",
    "    # Make sure we've got the windowing dimensions right\n",
    "    assert X_windows.shape == (n_x*n_windows, l_reg_days) and Y_windows.shape == (n_x*n_windows, l_reg_days)\n",
    "    \n",
    "    # Sanity checks\n",
    "    assert all([\n",
    "        X[0, -1] == X_windows[n_windows-1, -1],\n",
    "        Y[0, -1] == Y_windows[n_windows-1, -1],\n",
    "        X[-1, -1] == X_windows[-1, -1],\n",
    "        Y[-1, -1] == Y_windows[-1, -1],\n",
    "    ])\n",
    "    \n",
    "    # Construct ticker pair index column\n",
    "    pair_index = np.array(pd.DataFrame(np.array([Y_index, X_index])).apply('_'.join, axis=0, raw=True))\n",
    "    \n",
    "    # Construct regression date range index column\n",
    "    date_index = np.array(pd.DataFrame(np.array([date_index_windowed[:, 0], date_index_windowed[:, 1]])).apply('_'.join, axis=0, raw=True))\n",
    "    \n",
    "    # Lengths of indexes must match\n",
    "    assert len(pair_index) == len(date_index)\n",
    "    \n",
    "    # Calculate and return the residuals\n",
    "    res, betas = get_residuals_many(X_windows, Y_windows)\n",
    "    \n",
    "    res = pd.DataFrame(res, index=pair_index)\n",
    "    res.insert(0, 'dates', date_index)\n",
    "    betas = pd.DataFrame(betas, index=pair_index)\n",
    "    betas.insert(0, 'dates', date_index)\n",
    "    \n",
    "    if write_csv:\n",
    "        time = datetime.now().time()\n",
    "        res.to_csv(os.path.join(data_dir, time.strftime('residuals_%H%M%S.csv')), header=False, index=True)\n",
    "        betas.to_csv(os.path.join(data_dir, time.strftime('betas_%H%M%S.csv')), header=False, index=True)\n",
    "        pd.Series(date_index).to_csv(os.path.join(data_dir, time.strftime('dates_%H%M%S.csv')), header=False, index=False)\n",
    "    \n",
    "    return res, betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADF testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adfs(residuals, adf_regression):\n",
    "    # Get ADF test p-values for each row of the residuals array. No autolag (maxlag always used)\n",
    "    return np.apply_along_axis(lambda x: adfuller(x, regression=adf_regression, autolag=None)[1], axis=1, arr=residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregate_adfs(residuals, betas=None, cutoff=0.1, adf_regression='c', write_csv=False, data_dir='data'):\n",
    "    # Get ADF p-values\n",
    "    adfs = get_adfs(residuals.drop(columns='dates').to_numpy(dtype=np.float32), adf_regression=adf_regression).reshape((-1, 1))\n",
    "    \n",
    "    # Add ones to ADF values where betas are negative, if betas are given\n",
    "    if betas is not None:\n",
    "        # Must be the same number of columns\n",
    "        assert adfs.shape[0] == betas.shape[0]\n",
    "        # Residuals and betas must have the same index names\n",
    "        assert np.all(residuals.index == betas.index)\n",
    "        # Add 1's to p-values where betas are negative\n",
    "        adfs = adfs + (betas[0].to_numpy() <= 0).reshape((-1, 1))\n",
    "        \n",
    "    # Make a copy for returning, CSV output\n",
    "    adfs_raw = pd.DataFrame(adfs.copy(), index=residuals.index)\n",
    "    \n",
    "    # All unique ticker pairs, in original order\n",
    "    unique_pairs = residuals.index.unique()\n",
    "    \n",
    "    # Number of regressions for one pair\n",
    "    pairs_per_index = len(residuals) // len(unique_pairs)\n",
    "    \n",
    "    # Reshape into a 3D array for averaging ADF values along the second axis\n",
    "    adfs = adfs.reshape((len(unique_pairs), pairs_per_index, 1))\n",
    "    \n",
    "    # Takes cutoff, averages along the pairs_per_index (second) axis\n",
    "    adfs = (adfs <= cutoff).mean(axis=1)\n",
    "    \n",
    "    # Probably always true, but just in case\n",
    "    assert adfs.shape[0] == len(unique_pairs)\n",
    "        \n",
    "    # Back to a DataFrame with named indexes\n",
    "    adfs = pd.DataFrame(adfs, index=unique_pairs)\n",
    "    \n",
    "    # Output to CSV\n",
    "    if write_csv:\n",
    "        time = datetime.now().time()\n",
    "        adfs.to_csv(os.path.join(data_dir, time.strftime('adfs_%H%M%S.csv')), header=False, index=True)\n",
    "        adfs_raw.to_csv(os.path.join(data_dir, time.strftime('adfs-raw_%H%M%S.csv')), header=False, index=True)\n",
    "        \n",
    "    return adfs, adfs_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardized residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_pairs(pairs):\n",
    "    # Get unique ticker pairs, in preserved order\n",
    "    unique_pairs = pairs.index.unique()\n",
    "    \n",
    "    # Number of samples per ticker pair\n",
    "    pairs_per_index = len(pairs) // len(unique_pairs)\n",
    "    \n",
    "    # Must be an equal number of pairs per index\n",
    "    assert pairs_per_index * len(unique_pairs) == len(pairs)\n",
    "    \n",
    "    # Slice taking only the last regression for each ticker pair\n",
    "    last_pairs = pairs.iloc[pairs_per_index-1:len(pairs):pairs_per_index].copy()\n",
    "    \n",
    "    # Make sure we got the slices right\n",
    "    assert np.all(last_pairs.index == unique_pairs) and np.all(pairs.iloc[-1] == last_pairs.iloc[-1])\n",
    "        \n",
    "    return last_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_standardized_residuals(residuals, write_csv=False, data_dir='data'):\n",
    "    # Dates aren't needed anymore, as we're using the latest regressions\n",
    "    residuals = residuals.drop(columns='dates')\n",
    "    \n",
    "    # Get the last regression for each spread\n",
    "    last_reg_pairs = get_last_pairs(residuals)\n",
    "    \n",
    "    # Get unique ticker pairs\n",
    "    unique_pairs = last_reg_pairs.index\n",
    "    \n",
    "    # Convert to numpy\n",
    "    last_reg_pairs = last_reg_pairs.to_numpy(dtype=np.float32)\n",
    "    \n",
    "    # Standardize\n",
    "    last_reg_pairs = (last_reg_pairs - last_reg_pairs.mean(axis=1, keepdims=True)) / last_reg_pairs.std(axis=1, keepdims=True)\n",
    "    \n",
    "    # Back to a DataFrame with named indexes\n",
    "    last_reg_pairs = pd.DataFrame(last_reg_pairs, index=unique_pairs)\n",
    "    \n",
    "    # Output to CSV\n",
    "    if write_csv:\n",
    "        time = datetime.now().time()\n",
    "        last_reg_pairs.to_csv(os.path.join(data_dir, time.strftime('std-residuals_%H%M%S.csv')), header=False, index=True)\n",
    "        \n",
    "    return last_reg_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean residual magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_residual_magnitude(std_residuals, dt):\n",
    "    # Assume there is enough days' worth of data for averaging over dt days\n",
    "    assert std_residuals.shape[1] >= dt\n",
    "    \n",
    "    # Select the last dt days from the right\n",
    "    std_residuals = std_residuals.to_numpy(dtype=np.float32)[:, -dt:]\n",
    "    \n",
    "    # Take the absolute maximum for each day, over all tickers, mean over the results\n",
    "    mean_magnitude = np.abs(std_residuals).max(axis=0).mean()\n",
    "    \n",
    "    return mean_magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trade returns test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spreads_returns(stonk_prices, tickers_X, tickers_Y, betas_YX, buy_X, date_enter, max_trade_length):\n",
    "    \n",
    "    # Select stock prices starting from trade enter date, up to maximum trade length\n",
    "    stonk_prices = stonk_prices.loc[:, date_enter:].iloc[:, :max_trade_length]\n",
    "\n",
    "    # Save spread indexes for later\n",
    "    pairs_indexes = betas_YX.index\n",
    "    \n",
    "    # Take numpy betas\n",
    "    betas_YX = betas_YX.drop(columns='dates').to_numpy()\n",
    "\n",
    "    # Take numpy buy list for X\n",
    "    buy_X = buy_X.values\n",
    "\n",
    "    # Sanity checks\n",
    "    assert all(\n",
    "        stonk_prices.shape[1] == max_trade_length,\n",
    "        len(buy_X) == len(tickers_X) == len(tickers_Y) == len(betas_YX)\n",
    "    )\n",
    "\n",
    "    # Select stock prices for tickers in trade\n",
    "    prices_X = stonk_prices.loc[tickers_X].to_numpy()\n",
    "    prices_Y = stonk_prices.loc[tickers_Y].to_numpy()\n",
    "\n",
    "    # Save entering prices at t=0\n",
    "    initial_prices_X = prices_X[:, [0]].copy()\n",
    "    initial_prices_Y = prices_Y[:, [0]].copy()\n",
    "\n",
    "    # Initial values of trades at t=0\n",
    "    initial_trade_values = (initial_prices_X * betas_YX) + initial_prices_Y\n",
    "\n",
    "    # Returns for X, Y trades per each day. X tickers always scaled by beta\n",
    "    returns_X = betas_YX * (prices_X - initial_prices_X)\n",
    "    returns_Y = prices_Y - initial_prices_Y\n",
    "\n",
    "    # Negate short trades\n",
    "    returns_X[~buy_X] = -returns_X[~buy_X]\n",
    "    returns_Y[buy_X] = -returns_Y[buy_X]\n",
    "\n",
    "    # Add the trade returns for X, Y, divide by initial investment values to get profit/loss %\n",
    "    trade_returns = (returns_X + returns_Y) / initial_trade_values\n",
    "\n",
    "    # Back to dataframe with indexes\n",
    "    trade_returns = pd.DataFrame(trade_returns, index=pairs_indexes)\n",
    "    \n",
    "    return trade_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_time(func):\n",
    "    t1 = time.time()\n",
    "    ret = func()\n",
    "    t2 = time.time()\n",
    "    print(\"Time: \" + str(int(t2-t1)) + 's')\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_pair_index(indexes):\n",
    "    indexes = pd.Series(indexes)\n",
    "    y = np.array(indexes.apply(lambda x: x.split('_')[0]))\n",
    "    x = np.array(indexes.apply(lambda x: x.split('_')[1]))\n",
    "    return {'y':y, 'x':x}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline example tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Import tickers from given custom list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/selected_spreads.csv'\n",
    "ticker_pairs = pd.read_csv(filename, header=0)\n",
    "ticker_pairs.set_index('spreads', inplace=True)\n",
    "\n",
    "separated_indexes = separate_pair_index(ticker_pairs.index)\n",
    "ticker_pairs['x'] = separated_indexes['x']\n",
    "ticker_pairs['y'] = separated_indexes['y']\n",
    "\n",
    "ticker_pairs = ticker_pairs.loc[~ticker_pairs.index.str.contains(r'CIT|LORL|ENBL|MDP')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Download stock daily prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets all ticker names (no argument given)\n",
    "# ticker_list = get_tickers_by_industry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_list = set(list(ticker_pairs['x']) + list(ticker_pairs['y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific date - 3rd of March 2022 (Y, M, D)\n",
    "# date_to = datetime(2022, 3, 1)\n",
    "# Date of today\n",
    "date_to = datetime.today()\n",
    "# How many years' of data to download (going backwards from date_end). Year can be a floating point number\n",
    "period_years = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download ticker price data for the tickers selected above (saved to .csv automatically)\n",
    "df, df_clean = download_stonk_prices(ticker_list, period_years=period_years, date_to=date_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Read stock data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'technology_hardware_and_equipment'\n",
    "'software_and_services'\n",
    "'media_and_entertainment'\n",
    "'retailing'\n",
    "'automobiles_and_components'\n",
    "'semiconductors_and_semiconductor_equipment'\n",
    "'health_care_equipment_and_services'\n",
    "'banks'\n",
    "'pharmaceuticals_biotechnology_and_life_sciences'\n",
    "'food_and_staples_retailing'\n",
    "'oil_gas_and_consumable_fuels'\n",
    "'food_beverage_and_tobacco'\n",
    "'telecommunication_services'\n",
    "'consumer_durables_and_apparel'\n",
    "'consumer_services'\n",
    "'transportation'\n",
    "'diversified_financials'\n",
    "'utilities'\n",
    "'capital_goods'\n",
    "'insurance'\n",
    "'chemicals'\n",
    "'metals_and_mining'\n",
    "'commercial_and_professional_services'\n",
    "'containers_and_packaging'\n",
    "'energy_equipment_and_services'\n",
    "'construction_materials'\n",
    "'paper_and_forest_products'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stock data of ALL industries (all tickers) - no arguments specified\n",
    "stonks = get_stonk_data_by_industry('2018-03-16', '2022-03-14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stock data from selected industries only\n",
    "# stonks = get_stonk_data_by_industry('2018-03-12', '2022-03-11', industries=['consumer_durables_and_apparel', 'consumer_services'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, Y = combine_stonk_pairs(stonks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Select spreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = stonks.loc[ticker_pairs['x']]\n",
    "Y = stonks.loc[ticker_pairs['y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "_WRITE_RESULTS_TO_CSV = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Calculate rolling residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 8s\n"
     ]
    }
   ],
   "source": [
    "residuals, betas = measure_time(partial(get_rolling_residuals, X=X, Y=Y, l_reg=3, l_roll=1, dt=20, write_csv=_WRITE_RESULTS_TO_CSV))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Calculate ADF test results using the residuals returned above. Betas are optionally given to invalidate ADF test results where betas are negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 32s\n"
     ]
    }
   ],
   "source": [
    "adfs, adfs_raw = measure_time(partial(get_aggregate_adfs, residuals, betas=betas, write_csv=_WRITE_RESULTS_TO_CSV))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Calculate the standardized residuals of the regression from the last time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_residuals = get_standardized_residuals(residuals, write_csv=_WRITE_RESULTS_TO_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Calculate the mean residual trade making magnitude cutoff over the last *dt* days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_residual_magnitude = get_mean_residual_magnitude(std_residuals, dt=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 8. Calculate selected trade returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 8.2. Select which trades to make based on the last standardized residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_YX = std_residuals[std_residuals.iloc[:, -1].abs() >= mean_residual_magnitude]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 8.2. Get betas for the last regressions and for the selected pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_betas = get_last_pairs(betas)\n",
    "betas_YX = last_betas.loc[trade_YX.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 8.3. Select long/short stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_X = trade_YX.iloc[:, -1].apply(lambda x: x > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 8.4. Separate spread pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "separated_pairs = separate_pair_index(trade_YX.index)\n",
    "tickers_X = separated_pairs['x']\n",
    "tickers_Y = separated_pairs['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 8.5. Calculate returns for the trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Slow residual functions (for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_rolling_slow_residuals(X, Y, l_reg, l_roll, dt):\n",
    "#     _DAYS_IN_TRADING_YEAR = (252) - (252 % dt)\n",
    "#     l_reg_days = _DAYS_IN_TRADING_YEAR * l_reg\n",
    "#     l_roll_days = _DAYS_IN_TRADING_YEAR * l_roll\n",
    "#     total_days = l_reg_days + l_roll_days\n",
    "#     n_windows = l_roll_days // dt\n",
    "#     n_x = X.shape[0]\n",
    "    \n",
    "#     assert (l_roll_days % dt) == 0\n",
    "#     assert X.shape[1] >= total_days and Y.shape[1] >= total_days\n",
    "    \n",
    "#     X = X[:, -total_days:]\n",
    "#     Y = Y[:, -total_days:]\n",
    "    \n",
    "#     # First window\n",
    "#     X_windows = np.empty(shape=(n_x*n_windows, l_reg_days))\n",
    "#     Y_windows = np.empty(shape=(n_x*n_windows, l_reg_days))\n",
    "    \n",
    "#     for n in range(n_x):\n",
    "#         for i in range(n_windows):\n",
    "#             X_windows = np.concatenate(( X_windows, X[n, i*dt:l_reg_days+(i*dt)] ))\n",
    "#             Y_windows = np.concatenate(( Y_windows, Y[n, i*dt:l_reg_days+(i*dt)] ))\n",
    "    \n",
    "#     assert X_windows.shape == (n_x*n_windows, l_reg_days) and Y_windows.shape == (n_x*n_windows, l_reg_days)\n",
    "    \n",
    "#     return get_slow_residuals_many(X_windows, Y_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_slow_residuals_many(X, Y, n_jobs=-1):\n",
    "#     lr = LinearRegression(n_jobs=n_jobs, fit_intercept=True)\n",
    "#     X = X.reshape((X.shape[0], X.shape[1], -1))\n",
    "#     Y = Y.reshape((Y.shape[0], Y.shape[1], -1))\n",
    "    \n",
    "#     preds = []\n",
    "#     res = []\n",
    "#     betas = []\n",
    "#     for i in range(X.shape[0]):\n",
    "#         lr.fit(X[i], Y[i])\n",
    "#         preds.append(lr.predict(X[i]).round(2))\n",
    "#         res.append(Y[i]-preds[-1])\n",
    "#         betas.append(lr.coef_[0][0])\n",
    "#     return (np.asarray(res)[:,:,0], np.asarray(preds)[:,:,0], np.asarray(betas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1_fast = time.time()\n",
    "# res, betas, preds  = get_rolling_residuals(X, Y, l_reg=2, l_roll=1, dt=5)\n",
    "# t2_fast = time.time()\n",
    "\n",
    "# t1_slow = time.time()\n",
    "# res_slow, preds_slow = get_rolling_slow_residuals(X, Y, l_reg=2, l_roll=1, dt=5)\n",
    "# t2_slow = time.time()\n",
    "\n",
    "# print(\"Time slow: \" + str(t2_slow-t1_slow))\n",
    "# print(\"Time fast: \" + str(t2_fast-t1_fast))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1_fast = time.time()\n",
    "# res, preds, betas = get_residuals_many(X, Y)\n",
    "# t2_fast = time.time()\n",
    "\n",
    "# t1_slow = time.time()\n",
    "# res_slow, preds_slow, betas_slow = get_slow_residuals_many(X, Y)\n",
    "# t2_slow = time.time()\n",
    "\n",
    "# print(\"Time slow: \" + str(t2_slow-t1_slow))\n",
    "# print(\"Time fast: \" + str(t2_fast-t1_fast))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Stock list preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_stock_list(raw_data_path='data/raw_stonk_list.xls', output_path='data/stonk_list.csv'):\n",
    "    '''\n",
    "    Parses a raw excel file from CapitalIQ containing ticker names and their subindustries, validates\n",
    "    unusual ticker names with Yahoo Finance, saving the processed data in CSV format.\n",
    "\n",
    "        Parameters:\n",
    "            Required:\n",
    "                raw_data_path (string):\n",
    "                    Path to the raw excel file.\n",
    "                output_path (string):\n",
    "                    Path where to save the parsed data.\n",
    "                \n",
    "        Returns:\n",
    "            Nothing\n",
    "    '''\n",
    "    \n",
    "    df = pd.read_excel(io=raw_data_path)\n",
    "    \n",
    "    # Drop NA rows\n",
    "    df.dropna(axis=0, inplace=True)\n",
    "    \n",
    "    # Reset index and drop the first row\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    df.drop(index=0, axis=0, inplace=True)\n",
    "    \n",
    "    # Drop unwanted columns\n",
    "    df.drop(columns=df.columns[[1, 2, 3, 4, 5, 7, 8, 9]], inplace=True)\n",
    "    \n",
    "    # Rename remaining columns\n",
    "    df.columns = ['ticker', 'subindustry']\n",
    "    \n",
    "    # Remove the '(Primary)' tag from subindustries\n",
    "    df['subindustry'] = df['subindustry'].str.replace(r' \\(Primary\\)', '')\n",
    "    \n",
    "    # Remove everything until (and including) the semicolon for tickers\n",
    "    df['ticker'] = df['ticker'].str.replace(r'(.*:)', '')\n",
    "    \n",
    "    df['ticker'] = df['ticker'].str.replace(r' WI', '.VI')\n",
    "    df['ticker'] = df['ticker'].str.replace(r'\\.WI', '.VI')\n",
    "    \n",
    "    # Replace the ticker endings for a Yahoo finance supported format\n",
    "    df['ticker'] = df['ticker'].str.replace(r'\\.PR', '-P')\n",
    "    # df['ticker'] = df['ticker'].str.replace(r' PR', '-P')\n",
    "    \n",
    "    # Take all remaining tickers that have a dot\n",
    "    dotted = df[df['ticker'].str.fullmatch(r'[A-Z]*\\.[A-Z]')]\n",
    "    \n",
    "    # Replace the dots with dashes\n",
    "    dashed = dotted.copy()\n",
    "    dashed['ticker'] = dashed['ticker'].str.replace(r'\\.', '-')\n",
    "    \n",
    "    # Remove the dots\n",
    "    undotted = dotted.copy()\n",
    "    undotted['ticker'] = undotted['ticker'].str.replace(r'\\.', '')\n",
    "\n",
    "    # Combine all variantas together\n",
    "    all_variants = pd.concat([dotted, dashed, undotted])\n",
    "    \n",
    "    # Run all of these through Yahoo finance, get last day's price\n",
    "    stonks = yf.download(list(all_variants['ticker'].astype('string').values), period='1m', interval='1d', group_by='column')\n",
    "    \n",
    "    # Drop all NA tickers (that failed to download)\n",
    "    valid_tickers = stonks['Adj Close'].iloc[-1].dropna(axis=0).to_frame().reset_index()\n",
    "    \n",
    "    # Rename columns\n",
    "    valid_tickers.columns = ['ticker', 'price']\n",
    "    \n",
    "    # Add subindustries to the remaining valid tickers\n",
    "    valid_tickers = valid_tickers.join(all_variants.set_index('ticker'), on='ticker')\n",
    "    \n",
    "    # Drop the price column\n",
    "    valid_tickers.drop(columns=valid_tickers.columns[[1]], inplace=True)\n",
    "    \n",
    "    # Remove all tickers that have a dot from main dataframe\n",
    "    df = df[~df['ticker'].str.fullmatch(r'[A-Z]*\\.[A-Z]')]\n",
    "    \n",
    "    # Add the validated tickers back\n",
    "    df = pd.concat([df, valid_tickers], axis=0, ignore_index=True)\n",
    "    \n",
    "    # Make the subindustry strings more code friendly\n",
    "    df['subindustry'] = df['subindustry'].str.replace(' ', '_')\n",
    "    df['subindustry'] = df['subindustry'].str.lower()\n",
    "    df['subindustry'] = df['subindustry'].str.replace(',', '')\n",
    "    \n",
    "    df.to_csv(path_or_buf=output_path, header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
