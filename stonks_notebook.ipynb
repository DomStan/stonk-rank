{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install yfinance\n",
    "# !pip install pmdarima\n",
    "# !pip install hyperopt\n",
    "# !pip install xgboost\n",
    "# !pip install numpy -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "\n",
    "import typing\n",
    "from typing import Dict\n",
    "from typing import Any\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "import utils\n",
    "import pipelines\n",
    "import processing\n",
    "import evaluate\n",
    "import predict\n",
    "import train\n",
    "import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download stock daily prices & indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gets all ticker names (no argument given)\n",
    "market_cap_min_mm = 1000\n",
    "market_cap_max_mm = None\n",
    "\n",
    "ticker_list = utils.get_ticker_names(\n",
    "    market_cap_min_mm=market_cap_min_mm,\n",
    "    market_cap_max_mm=market_cap_max_mm,\n",
    "    remove_industries=[\n",
    "        # \"diversified_financials\",\n",
    "        \"pharmaceuticals_biotechnology_and_life_sciences\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Specific date - 3rd of March 2022 (Y, M, D)\n",
    "# date_to = datetime(2021, 1, 18)\n",
    "### Date of today\n",
    "date_to = datetime.today()\n",
    "### How many years' of data to download (going backwards from date_end). Year can be a floating point number\n",
    "period_years = 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  2442 of 2442 completed\n",
      "\n",
      "25 Failed downloads:\n",
      "- JOBS: No data found, symbol may be delisted\n",
      "- FB: No data found, symbol may be delisted\n",
      "- RLGY: No data found, symbol may be delisted\n",
      "- POST WI: No data found, symbol may be delisted\n",
      "- BLL: No data found, symbol may be delisted\n",
      "- BIP.PRB: No data found, symbol may be delisted\n",
      "- MIME: No data found, symbol may be delisted\n",
      "- O.WI: No data found, symbol may be delisted\n",
      "- OCDX: No data found, symbol may be delisted\n",
      "- BIP.PRA: No data found, symbol may be delisted\n",
      "- SNX.WI: No data found, symbol may be delisted\n",
      "- APSG: No data found, symbol may be delisted\n",
      "- TSC: No data found, symbol may be delisted\n",
      "- EPAY: No data found, symbol may be delisted\n",
      "- ANAT: No data found, symbol may be delisted\n",
      "- RXN WI: No data found, symbol may be delisted\n",
      "- T WD: No data found, symbol may be delisted\n",
      "- NCBS: No data found, symbol may be delisted\n",
      "- DELL WI: No data found, symbol may be delisted\n",
      "- CERN: No data found, symbol may be delisted\n",
      "- DIDI: No data found, symbol may be delisted\n",
      "- FOE: No data found, symbol may be delisted\n",
      "- ZNGA: No data found, symbol may be delisted\n",
      "- SGMS: No data found, symbol may be delisted\n",
      "- MGP: No data found, symbol may be delisted\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "_, _ = utils.download_stonk_prices(\n",
    "    ticker_list.index, period_years=period_years, date_to=date_to\n",
    ")\n",
    "_, _ = utils.download_stonk_prices(\n",
    "    [\"^VIX\"], period_years=period_years, date_to=date_to, fname_prefix=\"vix\"\n",
    ")\n",
    "_, _ = utils.download_stonk_prices(\n",
    "    [\"^GSPC\"], period_years=period_years, date_to=date_to, fname_prefix=\"sp500\"\n",
    ")\n",
    "_, _ = utils.download_stonk_prices(\n",
    "    [\"CL=F\"], period_years=period_years, date_to=date_to, fname_prefix=\"oil\"\n",
    ")\n",
    "_, _ = utils.download_stonk_prices(\n",
    "    [\"DX=F\"], period_years=period_years, date_to=date_to, fname_prefix=\"usd\"\n",
    ")\n",
    "_, _ = utils.download_stonk_prices(\n",
    "    [\"^TNX\"], period_years=period_years, date_to=date_to, fname_prefix=\"yield\"\n",
    ")\n",
    "_, _ = utils.download_stonk_prices(\n",
    "    [\"HG=F\"], period_years=period_years, date_to=date_to, fname_prefix=\"copper\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "industries = [\n",
    "    # 'health_care_equipment_and_services',\n",
    "    # 'software_and_services',\n",
    "    # 'retailing',\n",
    "    # 'telecommunication_services',\n",
    "    # \"capital_goods\",\n",
    "    # \"energy\",\n",
    "    # # 'pharmaceuticals_biotechnology_and_life_sciences',\n",
    "    # 'consumer_staples',\n",
    "    # 'banks',\n",
    "    # 'diversified_financials',\n",
    "    # 'metals_and_mining',\n",
    "    # 'technology_hardware_and_equipment',\n",
    "    # 'utilities',\n",
    "    # 'chemicals',\n",
    "    # 'automobiles_and_components',\n",
    "    # \"semiconductors_and_semiconductor_equipment\",\n",
    "    # 'media_and_entertainment',\n",
    "    # 'real_estate',\n",
    "    # 'consumer_services',\n",
    "    # 'consumer_durables_and_apparel',\n",
    "    # 'insurance',\n",
    "    # 'transportation',\n",
    "    # 'commercial_and_professional_services',\n",
    "    \"paper_and_forest_products\",\n",
    "    \"containers_and_packaging\",\n",
    "    \"construction_materials\",\n",
    "]\n",
    "\n",
    "l_reg = 3\n",
    "l_roll = 2\n",
    "dt = 10\n",
    "last_residual_cutoff = 2.5\n",
    "adf_pval_cutoff = 0.1\n",
    "adf_pass_rate_filter = 0.5\n",
    "mean_max_residual_dt = 21\n",
    "arima_forecast_months = 3\n",
    "arima_eval_models = 5\n",
    "\n",
    "market_cap_max_string = \"max\" if market_cap_max_mm is None else str(market_cap_max_mm)\n",
    "pipeline_dir = (\n",
    "    \"pipeline_run_\" + str(market_cap_min_mm) + \"_to_\" + market_cap_max_string + \"_cap\"\n",
    ")\n",
    "output_dir = os.path.join(\"data\", pipeline_dir)\n",
    "\n",
    "stonk_model = predict.XGBStonkModel()\n",
    "\n",
    "market_indexes = utils.get_market_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Industry (1/3): paper_and_forest_products\n",
      "Industry (2/3): containers_and_packaging\n",
      "Mean max value for containers_and_packaging: 2.8289999961853027\n",
      "Industry (3/3): construction_materials\n",
      "*** All done ***\n"
     ]
    }
   ],
   "source": [
    "datasets = []\n",
    "i = 0\n",
    "total_industries = len(industries)\n",
    "for industry in industries:\n",
    "    i += 1\n",
    "    stonks = utils.get_stonk_data(\n",
    "        market_cap_min_mm,\n",
    "        market_cap_max_mm,\n",
    "        remove_industries=[industry],\n",
    "        filter_industries=True,\n",
    "    )\n",
    "    X, Y = processing.combine_stonk_pairs(stonks)\n",
    "\n",
    "    print(\"Industry ({0}/{1}): {2}\".format(i, total_industries, industry))\n",
    "\n",
    "    features = pipelines.process_features_from_price_data(\n",
    "        X=X,\n",
    "        Y=Y,\n",
    "        market_indexes=market_indexes,\n",
    "        l_reg=l_reg,\n",
    "        l_roll=l_roll,\n",
    "        dt=dt,\n",
    "        last_residual_cutoff=last_residual_cutoff,\n",
    "        adf_pval_cutoff=adf_pval_cutoff,\n",
    "        adf_pass_rate_filter=adf_pass_rate_filter,\n",
    "        mean_max_residual_dt=mean_max_residual_dt,\n",
    "        arima_forecast_months=arima_forecast_months,\n",
    "        arima_eval_models=arima_eval_models,\n",
    "    )\n",
    "\n",
    "    if len(features) == 0:\n",
    "        print(\"No trades\")\n",
    "        continue\n",
    "\n",
    "    print(\n",
    "        \"Mean max value for {0}: {1}\".format(industry, features[\"residuals_max_mean\"])\n",
    "    )\n",
    "    dataset = utils.build_dataset_from_live_data_by_industry(\n",
    "        std_residuals=features[\"std_residuals\"],\n",
    "        adfs=features[\"adfs\"],\n",
    "        subindustry=industry,\n",
    "        mean_max_residual=features[\"residuals_max_mean\"],\n",
    "        vix_index=market_indexes[\"vix\"].loc[stonks.columns[-1]],\n",
    "        betas_stability_rsquared=features[\"beta_stability_rsquared_vals\"],\n",
    "        arima_forecasts=features[\"arima_forecasts\"],\n",
    "    )\n",
    "\n",
    "    predictions, df_processed = stonk_model.predict(dataset)\n",
    "    datasets.append((dataset, df_processed))\n",
    "    predictions = pd.DataFrame(predictions)\n",
    "    predictions.index = features[\"adfs\"].index\n",
    "\n",
    "    features[\"residuals\"].insert(0, \"dates\", features[\"dates_index\"].values)\n",
    "    features[\"betas\"].insert(0, \"dates\", features[\"dates_index\"].values)\n",
    "\n",
    "    features[\"residuals\"].to_csv(\n",
    "        os.path.join(output_dir, industry + \"_residuals.csv\"),\n",
    "        header=False,\n",
    "        index=True,\n",
    "    )\n",
    "    features[\"betas\"].to_csv(\n",
    "        os.path.join(output_dir, industry + \"_betas.csv\"), header=False, index=True\n",
    "    )\n",
    "    features[\"adfs_raw\"].to_csv(\n",
    "        os.path.join(output_dir, industry + \"_adfs_raw.csv\"), header=False, index=True\n",
    "    )\n",
    "    predictions.to_csv(\n",
    "        os.path.join(output_dir, industry + \"_predictions.csv\"),\n",
    "        header=False,\n",
    "        index=True,\n",
    "    )\n",
    "    features[\"arima_forecasts\"].to_csv(\n",
    "        os.path.join(output_dir, industry + \"_arima.csv\"),\n",
    "        header=False,\n",
    "        index=True,\n",
    "    )\n",
    "    features[\"beta_stability_rsquared_vals\"].to_csv(\n",
    "        os.path.join(output_dir, industry + \"_rsquared.csv\"),\n",
    "        header=False,\n",
    "        index=True,\n",
    "    )\n",
    "    features[\"market_correlations\"].to_csv(\n",
    "        os.path.join(output_dir, industry + \"_correlations.csv\"),\n",
    "        header=True,\n",
    "        index=True,\n",
    "    )\n",
    "\n",
    "print(\"*** All done ***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "stonks = utils.get_stonk_data(disable_filter=True)\n",
    "stonks = stonks.loc[:, :\"2019-07-19\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data windows: 10\n"
     ]
    }
   ],
   "source": [
    "pipelines.data_collection_rolling_pipeline(\n",
    "    stonk_prices=stonks,\n",
    "    l_reg=3,\n",
    "    l_roll=2,\n",
    "    dt=10,\n",
    "    market_cap_min_mm=1000,\n",
    "    market_cap_max_mm=None,\n",
    "    last_residual_cutoff=2.5,\n",
    "    mean_max_residual_dt=21,\n",
    "    adf_pval_cutoff=0.1,\n",
    "    adf_pass_rate_filter=0.5,\n",
    "    arima_forecast_months=3,\n",
    "    arima_eval_models=5,\n",
    "    trade_length_months=3,\n",
    "    trading_interval_weeks=2,\n",
    "    remove_industries=[\n",
    "        \"pharmaceuticals_biotechnology_and_life_sciences\",\n",
    "    ],\n",
    "    first_n_windows=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = utils.ingest_trade_pipeline_outputs(\n",
    "    data_dir=\"data/data_collection_pipeline/1000_to_max/\"\n",
    ")\n",
    "\n",
    "vix = utils.get_stonk_data(fname_prefix=\"vix\", disable_filter=True).iloc[0]\n",
    "dataset[\"vix\"] = dataset[\"trade_date\"].apply(lambda x: vix.loc[x])\n",
    "\n",
    "dataset.to_csv(\"data/dataset_bigcap.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_validation_pipeline(\n",
    "    dataset,\n",
    "    data_window_train_size,\n",
    "    data_window_test_size,\n",
    "    data_window_gap_size,\n",
    "    hp_model_evals,\n",
    "    top_n_best_trades=5,\n",
    "    min_industry_confidence=0.4,\n",
    "    random_noise=0.005,\n",
    "    hp_nth_best_model=10,\n",
    "    data_dir=\"data\",\n",
    "    outputs_dir=\"experiments\",\n",
    "):\n",
    "    random_state = np.random.randint(133742069)\n",
    "    pipeline_dir = os.path.join(data_dir, outputs_dir)\n",
    "\n",
    "    # Total data window size is the size of the standard data window for validation/training plus another gap and test size for final validation\n",
    "    total_data_window_size = (\n",
    "        data_window_train_size + data_window_gap_size + data_window_test_size\n",
    "    ) + (data_window_gap_size + data_window_test_size)\n",
    "\n",
    "    dates_sorted = np.sort(dataset[\"trade_date\"].unique())\n",
    "    total_date_count = len(dates_sorted)\n",
    "\n",
    "    assert total_date_count >= total_data_window_size\n",
    "\n",
    "    data_windows = range(total_date_count, total_data_window_size - 1, -1)\n",
    "\n",
    "    print(\"Total data windows: \" + str(len(list(data_windows))))\n",
    "\n",
    "    all_evaluation_results = {}\n",
    "    for index_end in data_windows:\n",
    "        index_start = index_end - total_data_window_size\n",
    "\n",
    "        current_data_window = dates_sorted[index_start:index_end]\n",
    "\n",
    "        assert len(current_data_window) == total_data_window_size\n",
    "\n",
    "        print(\n",
    "            \"Period \"\n",
    "            + str(current_data_window[0])\n",
    "            + \" to \"\n",
    "            + str(current_data_window[-1])\n",
    "        )\n",
    "\n",
    "        # Separating validation/test data windows\n",
    "        dataset_window_test = (\n",
    "            dataset[dataset.trade_date.isin(current_data_window)]\n",
    "            .copy()\n",
    "            .sample(frac=1, random_state=random_state)\n",
    "        )\n",
    "\n",
    "        current_data_window_validation = current_data_window[\n",
    "            : -(data_window_gap_size + data_window_test_size)\n",
    "        ]\n",
    "\n",
    "        assert len(current_data_window_validation) + (\n",
    "            data_window_gap_size + data_window_test_size\n",
    "        ) == len(current_data_window)\n",
    "\n",
    "        dataset_window_validation = (\n",
    "            dataset_window_test[\n",
    "                dataset_window_test.trade_date.isin(current_data_window_validation)\n",
    "            ]\n",
    "            .copy()\n",
    "            .sample(frac=1, random_state=random_state)\n",
    "        )\n",
    "\n",
    "        validation_splits = preprocessing.split_data(\n",
    "            dataset_window_validation,\n",
    "            date_count_train=data_window_train_size,\n",
    "            date_count_valid=data_window_test_size,\n",
    "            date_count_gap=data_window_gap_size,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "\n",
    "        test_splits = preprocessing.split_data(\n",
    "            dataset_window_test,\n",
    "            date_count_train=data_window_train_size,\n",
    "            date_count_valid=data_window_test_size,\n",
    "            date_count_gap=data_window_gap_size,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "\n",
    "        assert all(\n",
    "            [\n",
    "                len(validation_splits[\"validation\"].trade_date.unique())\n",
    "                == len(test_splits[\"validation\"].trade_date.unique()),\n",
    "                len(validation_splits[\"train\"].trade_date.unique())\n",
    "                == len(test_splits[\"train\"].trade_date.unique()),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Hyperparameter tuning/search, outputs an artifact CSV with results, which is not used\n",
    "        hp_trial_name = \"validation_{}-{}_until_{}\".format(\n",
    "            current_data_window_validation[0],\n",
    "            current_data_window_validation[-1],\n",
    "            current_data_window[-1],\n",
    "        )\n",
    "        df_trial_results = train.model_hp_search(\n",
    "            validation_splits,\n",
    "            n_evals=hp_model_evals,\n",
    "            trial_name=hp_trial_name,\n",
    "            additive_random_noise=random_noise,\n",
    "            write_csv=True,\n",
    "            random_state=random_state,\n",
    "            data_dir=data_dir,\n",
    "            output_dir=outputs_dir,\n",
    "        )\n",
    "\n",
    "        selected_hps = _select_nth_best_trial(\n",
    "            df_trial_results, nth_best=hp_nth_best_model\n",
    "        )\n",
    "\n",
    "        model_params = {\n",
    "            \"colsample_bylevel\": 1,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"subsample\": 1,\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"enable_categorical\": True,\n",
    "            \"max_cat_to_onehot\": 1,\n",
    "            \"eval_metric\": [\"logloss\"],\n",
    "            \"random_state\": random_state,\n",
    "        }\n",
    "        model_params.update(selected_hps)\n",
    "\n",
    "        # Live/production model training with given hyperparameters, outputs model and scalers JSONs\n",
    "        clf_test, scalers_test = train.train_production_xgb(\n",
    "            dataset=test_splits[\"train\"],\n",
    "            params=model_params,\n",
    "            noise_level=random_noise,\n",
    "            verbose=False,\n",
    "            data_dir=pipeline_dir,\n",
    "        )\n",
    "\n",
    "        # Live/production model evaluation on test set\n",
    "        live_model = predict.XGBStonkModel(model_dir=pipeline_dir)\n",
    "\n",
    "        predictions, test_dataset_processed = live_model.predict(\n",
    "            test_splits[\"validation\"]\n",
    "        )\n",
    "\n",
    "        # Model, scalers saving/loading correctly tests, get predictions\n",
    "        assert live_model._model == clf_test\n",
    "        live_model._model = clf_test\n",
    "        live_model._scalers = scalers_test\n",
    "        predictions_fortest, _ = live_model.predict(test_splits[\"validation\"])\n",
    "        assert np.all(predictions == predictions_fortest)\n",
    "\n",
    "        df_test_scores = test_splits[\"validation\"].copy()\n",
    "        df_test_scores[\"score\"] = predictions\n",
    "        df_test_scores[\"prediction\"] = predictions > 0.5\n",
    "\n",
    "        # Aggregate evaluation results, as a whole and by each trade date (as separate rows)\n",
    "        current_period_trade_dates = np.sort(df_test_scores.trade_date.unique())\n",
    "        current_evaluation_period_row_prefix = str(current_period_trade_dates[0]) + str(\n",
    "            current_period_trade_dates[-1]\n",
    "        )\n",
    "        current_evaluation_results = {}\n",
    "\n",
    "        _, results = returns_on_predictions(df_test_scores)\n",
    "        current_evaluation_results.update(results)\n",
    "\n",
    "        results = performance_summary(\n",
    "            y_score=df_test_scores[\"score\"],\n",
    "            y_preds=df_test_scores[\"prediction\"],\n",
    "            y_true=df_test_scores[\"label\"],\n",
    "            auc_cutoff=0.5,\n",
    "        )\n",
    "        current_evaluation_results.update(results)\n",
    "\n",
    "        results = performance_on_trading_use_case(\n",
    "            df=df_test_scores,\n",
    "            top_n_trades=top_n_best_trades,\n",
    "            min_industry_score=min_industry_confidence,\n",
    "        )\n",
    "        current_evaluation_results.update(results)\n",
    "\n",
    "        current_evaluation_results = pd.Series(current_evaluation_results)\n",
    "        all_evaluation_results[\n",
    "            current_evaluation_period_row_prefix + \"_all\"\n",
    "        ] = current_evaluation_results\n",
    "\n",
    "        # By trade date\n",
    "        for date, trades in df_test_scores.groupby(\"trade_date\"):\n",
    "            current_evaluation_results = {}\n",
    "\n",
    "            _, results = returns_on_predictions(trades)\n",
    "            current_evaluation_results.update(results)\n",
    "\n",
    "            results = performance_summary(\n",
    "                y_score=trades[\"score\"],\n",
    "                y_preds=trades[\"prediction\"],\n",
    "                y_true=trades[\"label\"],\n",
    "                auc_cutoff=0.5,\n",
    "            )\n",
    "            current_evaluation_results.update(results)\n",
    "\n",
    "            results = performance_on_trading_use_case(\n",
    "                df=trades,\n",
    "                top_n_trades=top_n_best_trades,\n",
    "                min_industry_score=min_industry_confidence,\n",
    "            )\n",
    "            current_evaluation_results.update(results)\n",
    "\n",
    "            current_evaluation_results = pd.Series(current_evaluation_results)\n",
    "            all_evaluation_results[\n",
    "                current_evaluation_period_row_prefix + \"_\" + str(date)\n",
    "            ] = current_evaluation_results\n",
    "\n",
    "    return pd.DataFrame(all_evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _select_nth_best_trial(df_trials: pd.DataFrame, nth_best: int) -> Dict[str, float]:\n",
    "    assert nth_best > 0 and nth_best <= len(df_trials)\n",
    "    return dict(\n",
    "        df_trials.iloc[nth_best - 1].drop(\n",
    "            columns=[\"f1_score\", \"precision\", \"ap\", \"auc\", \"pos_preds\"]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from hyperopt import STATUS_OK, STATUS_FAIL, Trials, fmin, hp, tpe, atpe, rand\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/dataset_bigcap.csv\")\n",
    "df = df[df.beta > 0]\n",
    "df = df[df.last_residual.abs() >= 2]\n",
    "df = preprocessing.assign_labels(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated_ticker_list = utils.get_ticker_names(\n",
    "#     1000, None, remove_industries=[\"pharmaceuticals_biotechnology_and_life_sciences\"]\n",
    "# )\n",
    "# df = df[df.ticker_x.isin(updated_ticker_list.index)]\n",
    "# df = df[df.ticker_y.isin(updated_ticker_list.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71910\n",
      "0    55982\n",
      "1    15928\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "drop_dates = 26\n",
    "selected_dates = np.sort(df[\"trade_date\"].unique())[drop_dates:]\n",
    "df_prod = df[df.trade_date.isin(selected_dates)].sample(frac=1)\n",
    "print(len(df_prod))\n",
    "print(df_prod[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_prod, scalers_prod = train.train_production_xgb(df_prod, params, noise_level=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'evaluate' from '/home/jupyter/stonk-rank/evaluate.py'>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import importlib\n",
    "# importlib.reload(evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70130\n",
      "2306\n",
      "0    54974\n",
      "1    15156\n",
      "Name: label, dtype: int64\n",
      "0    1817\n",
      "1     489\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "splits = preprocessing.split_data(\n",
    "    df, date_count_train=61, date_count_valid=2, date_count_gap=6, random_state=330544\n",
    ")\n",
    "print(len(splits[\"train\"]))\n",
    "print(len(splits[\"validation\"]))\n",
    "print(splits[\"train\"][\"label\"].value_counts())\n",
    "print(splits[\"validation\"][\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, scalers = preprocessing.transform_features(splits[\"train\"], noise_level=0.005)\n",
    "\n",
    "X_valid, _ = preprocessing.transform_features(\n",
    "    splits[\"validation\"], scalers=scalers, noise_level=0\n",
    ")\n",
    "\n",
    "y_train = splits[\"train\"][\"label\"]\n",
    "y_valid = splits[\"validation\"][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:17<00:00,  2.93trial/s, best loss: -0.41860886570489053]\n"
     ]
    }
   ],
   "source": [
    "df_trials = model_hp_search(\n",
    "    data_splits=splits, n_evals=50, trial_name=\"test\", additive_random_noise=0.005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trials = pd.read_csv(\"data/experiments/data-window-8#2.csv\")\n",
    "df_trials.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trials = pd.read_csv(\"data/experiments/data-window-8#2.csv\")\n",
    "df_trials.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.68477\tvalidation_1-logloss:0.68377\n",
      "[1]\tvalidation_0-logloss:0.67760\tvalidation_1-logloss:0.67579\n",
      "[2]\tvalidation_0-logloss:0.67145\tvalidation_1-logloss:0.66911\n",
      "[3]\tvalidation_0-logloss:0.66613\tvalidation_1-logloss:0.66324\n",
      "[4]\tvalidation_0-logloss:0.66070\tvalidation_1-logloss:0.65848\n",
      "[5]\tvalidation_0-logloss:0.65791\tvalidation_1-logloss:0.65415\n",
      "[6]\tvalidation_0-logloss:0.65539\tvalidation_1-logloss:0.65043\n",
      "[7]\tvalidation_0-logloss:0.64899\tvalidation_1-logloss:0.64680\n",
      "[8]\tvalidation_0-logloss:0.64530\tvalidation_1-logloss:0.64352\n",
      "[9]\tvalidation_0-logloss:0.63902\tvalidation_1-logloss:0.63850\n",
      "[10]\tvalidation_0-logloss:0.63629\tvalidation_1-logloss:0.63635\n",
      "[11]\tvalidation_0-logloss:0.63146\tvalidation_1-logloss:0.63226\n",
      "[12]\tvalidation_0-logloss:0.62623\tvalidation_1-logloss:0.62907\n",
      "[13]\tvalidation_0-logloss:0.62337\tvalidation_1-logloss:0.62682\n",
      "[14]\tvalidation_0-logloss:0.62020\tvalidation_1-logloss:0.62336\n",
      "[15]\tvalidation_0-logloss:0.61407\tvalidation_1-logloss:0.62074\n",
      "[16]\tvalidation_0-logloss:0.61021\tvalidation_1-logloss:0.61845\n",
      "[17]\tvalidation_0-logloss:0.60970\tvalidation_1-logloss:0.61651\n",
      "[18]\tvalidation_0-logloss:0.60912\tvalidation_1-logloss:0.61513\n",
      "[19]\tvalidation_0-logloss:0.60876\tvalidation_1-logloss:0.61278\n",
      "[20]\tvalidation_0-logloss:0.60850\tvalidation_1-logloss:0.61117\n",
      "[21]\tvalidation_0-logloss:0.60854\tvalidation_1-logloss:0.60968\n",
      "[22]\tvalidation_0-logloss:0.60749\tvalidation_1-logloss:0.60828\n",
      "[23]\tvalidation_0-logloss:0.60293\tvalidation_1-logloss:0.60737\n",
      "[24]\tvalidation_0-logloss:0.60120\tvalidation_1-logloss:0.60631\n",
      "[25]\tvalidation_0-logloss:0.60104\tvalidation_1-logloss:0.60396\n",
      "[26]\tvalidation_0-logloss:0.60082\tvalidation_1-logloss:0.60240\n",
      "[27]\tvalidation_0-logloss:0.59989\tvalidation_1-logloss:0.60083\n",
      "[28]\tvalidation_0-logloss:0.59993\tvalidation_1-logloss:0.59911\n",
      "[29]\tvalidation_0-logloss:0.60012\tvalidation_1-logloss:0.59741\n",
      "[30]\tvalidation_0-logloss:0.59964\tvalidation_1-logloss:0.59619\n",
      "[31]\tvalidation_0-logloss:0.59861\tvalidation_1-logloss:0.59533\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"gamma\": 3.740318,\n",
    "    \"scale_pos_weight\": 4.19,\n",
    "    \"max_depth\": 7,\n",
    "    \"min_child_weight\": 7,\n",
    "    \"max_delta_step\": 3,\n",
    "    \"colsample_bylevel\": 1,\n",
    "    \"n_estimators\": 32,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"subsample\": 1,\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"enable_categorical\": True,\n",
    "    \"max_cat_to_onehot\": 1,\n",
    "    \"eval_metric\": [\"logloss\"],\n",
    "    \"random_state\": np.random.randint(999929),\n",
    "}\n",
    "\n",
    "clf = xgb.XGBClassifier(**params)\n",
    "clf = clf.fit(X_train, y_train, eval_set=[(X_valid, y_valid), (X_train, y_train)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Validation**\n",
      "Precision: 0.3192771084337349\n",
      "PR-AUC/AP score: 0.5393574019093184\n",
      "ROC-AUC score: 0.592475292989523\n",
      "Total positive predictions: 332\n",
      "Total positive labels: 489\n",
      "\n",
      "Totals:\n",
      "        prediction\n",
      "result            \n",
      "FN             383\n",
      "FP             226\n",
      "TN            1591\n",
      "TP             106\n",
      "\n",
      "Means:\n",
      "        return_one_month  return_two_month  return_three_month\n",
      "result                                                        \n",
      "FN              0.058138          0.106154            0.109757\n",
      "FP              0.001075          0.003973            0.003894\n",
      "TN             -0.006720         -0.002602           -0.015820\n",
      "TP              0.036755          0.067575            0.089925\n",
      "\n",
      "Std:\n",
      "        return_one_month  return_two_month  return_three_month\n",
      "result                                                        \n",
      "FN              0.093194          0.066535            0.073164\n",
      "FP              0.044058          0.050160            0.059183\n",
      "TN              0.043610          0.054788            0.069945\n",
      "TP              0.060226          0.059581            0.070419\n",
      "\n",
      "Positive predictions:\n",
      "\n",
      "Means:\n",
      "return_one_month      0.012467\n",
      "return_two_month      0.024280\n",
      "return_three_month    0.031361\n",
      "dtype: float64\n",
      "\n",
      "Std:\n",
      "return_one_month      0.012467\n",
      "return_two_month      0.024280\n",
      "return_three_month    0.031361\n",
      "dtype: float64\n",
      "automnents_ntrades             5.000000\n",
      "automnents_top5_ret_1mo        0.057000\n",
      "automnents_top5_ret_2mo        0.055400\n",
      "automnents_top5_ret_3mo        0.072000\n",
      "automnents_top5_ret_std_1mo    0.072512\n",
      "                                 ...   \n",
      "utiliities_top5_ret_2mo        0.040800\n",
      "utiliities_top5_ret_3mo        0.055800\n",
      "utiliities_top5_ret_std_1mo    0.020543\n",
      "utiliities_top5_ret_std_2mo    0.026348\n",
      "utiliities_top5_ret_std_3mo    0.029184\n",
      "Length: 161, dtype: float64\n",
      "\n",
      " automobiles_and_components :\n",
      "Precision: 0.0\n",
      "PR-AUC/AP score: 0\n",
      "ROC-AUC score: 1.0\n",
      "Total positive predictions: 0\n",
      "Total positive labels: 1\n",
      "\n",
      " banks :\n",
      "Precision: 0.047619047619047616\n",
      "PR-AUC/AP score: 0.5\n",
      "ROC-AUC score: 0.49762362220649203\n",
      "Total positive predictions: 21\n",
      "Total positive labels: 58\n",
      "\n",
      " capital_goods :\n",
      "Precision: 0.0\n",
      "PR-AUC/AP score: 0\n",
      "ROC-AUC score: 0.5937451657814676\n",
      "Total positive predictions: 0\n",
      "Total positive labels: 86\n",
      "\n",
      " chemicals :\n",
      "Precision: 0.8888888888888888\n",
      "PR-AUC/AP score: 0.7713789682539682\n",
      "ROC-AUC score: 0.75\n",
      "Total positive predictions: 9\n",
      "Total positive labels: 20\n",
      "\n",
      " commercial_and_professional_services :\n",
      "Precision: 0.14285714285714285\n",
      "PR-AUC/AP score: 0.5\n",
      "ROC-AUC score: 0.5588235294117647\n",
      "Total positive predictions: 7\n",
      "Total positive labels: 4\n",
      "\n",
      " consumer_durables_and_apparel :\n",
      "Precision: 0.0\n",
      "PR-AUC/AP score: nan\n",
      "ROC-AUC score: 0.65\n",
      "Total positive predictions: 3\n",
      "Total positive labels: 2\n",
      "\n",
      " consumer_services :\n",
      "Precision: 0.25\n",
      "PR-AUC/AP score: 0.3333333333333333\n",
      "ROC-AUC score: 0.6888888888888889\n",
      "Total positive predictions: 4\n",
      "Total positive labels: 6\n",
      "\n",
      " consumer_staples :\n",
      "Precision: 0.0\n",
      "PR-AUC/AP score: nan\n",
      "ROC-AUC score: 0.35248447204968947\n",
      "Total positive predictions: 6\n",
      "Total positive labels: 23\n",
      "\n",
      " containers_and_packaging :\n",
      "Precision: 0.0\n",
      "PR-AUC/AP score: nan\n",
      "ROC-AUC score: 0\n",
      "Total positive predictions: 3\n",
      "Total positive labels: 0\n",
      "\n",
      " diversified_financials :\n",
      "Precision: 0.42857142857142855\n",
      "PR-AUC/AP score: 0.5092592592592593\n",
      "ROC-AUC score: 0.7379032258064516\n",
      "Total positive predictions: 14\n",
      "Total positive labels: 8\n",
      "\n",
      " energy :\n",
      "Precision: 0.8571428571428571\n",
      "PR-AUC/AP score: 1.0\n",
      "ROC-AUC score: 0.6706306427705161\n",
      "Total positive predictions: 7\n",
      "Total positive labels: 82\n",
      "\n",
      " health_care_equipment_and_services :\n",
      "Precision: 0.03333333333333333\n",
      "PR-AUC/AP score: 0.045454545454545456\n",
      "ROC-AUC score: 0.6039650696247344\n",
      "Total positive predictions: 30\n",
      "Total positive labels: 19\n",
      "\n",
      " insurance :\n",
      "Precision: 0.0\n",
      "PR-AUC/AP score: nan\n",
      "ROC-AUC score: 0.45329670329670335\n",
      "Total positive predictions: 3\n",
      "Total positive labels: 13\n",
      "\n",
      " media_and_entertainment :\n",
      "Precision: 0.0\n",
      "PR-AUC/AP score: 0\n",
      "ROC-AUC score: 0.46153846153846156\n",
      "Total positive predictions: 0\n",
      "Total positive labels: 1\n",
      "\n",
      " metals_and_mining :\n",
      "Precision: 0.5\n",
      "PR-AUC/AP score: 0.7117460317460317\n",
      "ROC-AUC score: 0.6411764705882353\n",
      "Total positive predictions: 20\n",
      "Total positive labels: 17\n",
      "\n",
      " real_estate :\n",
      "Precision: 0.6666666666666666\n",
      "PR-AUC/AP score: 0.8333333333333333\n",
      "ROC-AUC score: 0.6155844155844156\n",
      "Total positive predictions: 3\n",
      "Total positive labels: 21\n",
      "\n",
      " retailing :\n",
      "Precision: 0.1\n",
      "PR-AUC/AP score: 0.2\n",
      "ROC-AUC score: 0.5303030303030303\n",
      "Total positive predictions: 10\n",
      "Total positive labels: 4\n",
      "\n",
      " semiconductors_and_semiconductor_equipment :\n",
      "Precision: 0.12903225806451613\n",
      "PR-AUC/AP score: 0.3833333333333333\n",
      "ROC-AUC score: 0.5259661835748792\n",
      "Total positive predictions: 31\n",
      "Total positive labels: 12\n",
      "\n",
      " software_and_services :\n",
      "Precision: 0.2608695652173913\n",
      "PR-AUC/AP score: 0.5459693508856203\n",
      "ROC-AUC score: 0.6737127371273712\n",
      "Total positive predictions: 69\n",
      "Total positive labels: 30\n",
      "\n",
      " technology_hardware_and_equipment :\n",
      "Precision: 0.4878048780487805\n",
      "PR-AUC/AP score: 0.6449937509520053\n",
      "ROC-AUC score: 0.6484374999999999\n",
      "Total positive predictions: 41\n",
      "Total positive labels: 32\n",
      "\n",
      " telecommunication_services :\n",
      "Precision: 0.0\n",
      "PR-AUC/AP score: 0\n",
      "ROC-AUC score: 0.4375\n",
      "Total positive predictions: 0\n",
      "Total positive labels: 2\n",
      "\n",
      " transportation :\n",
      "Precision: 0.25\n",
      "PR-AUC/AP score: 1.0\n",
      "ROC-AUC score: 0.6666666666666666\n",
      "Total positive predictions: 4\n",
      "Total positive labels: 3\n",
      "\n",
      " utilities :\n",
      "Precision: 0.5531914893617021\n",
      "PR-AUC/AP score: 0.6064967339423861\n",
      "ROC-AUC score: 0.6620689655172414\n",
      "Total positive predictions: 47\n",
      "Total positive labels: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:864: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:864: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:864: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:864: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n"
     ]
    }
   ],
   "source": [
    "print(\"**Validation**\")\n",
    "y_score = clf.predict_proba(X_valid)[:, 1]\n",
    "thres = 0.5\n",
    "y_preds = y_score > thres\n",
    "\n",
    "df_results_valid = splits[\"validation\"].copy()\n",
    "df_results_valid[\"score\"] = y_score\n",
    "df_results_valid[\"prediction\"] = y_preds\n",
    "\n",
    "evaluate.performance_summary(\n",
    "    y_score=y_score, y_preds=y_preds, y_true=y_valid, auc_cutoff=0.5\n",
    ")\n",
    "\n",
    "df_results_valid, _ = evaluate.returns_on_predictions(df_results_valid)\n",
    "\n",
    "evaluate.performance_on_slice(df_results_valid, \"subindustry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option(\"display.max_rows\", 200)\n",
    "# evaluate.performance_on_trading_use_case(\n",
    "#         df_results_valid, top_n_trades=5, min_industry_score=0.4\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adf_pass_rate 0.05284967\n",
      "last_residual 0.061928246\n",
      "residual_mean_max 0.10744238\n",
      "vix 0.33485457\n",
      "betas_rsquared 0.09239178\n",
      "arima_forecast 0.13933738\n",
      "industry 0.14954282\n",
      "residual_inter 0.061653122\n"
     ]
    }
   ],
   "source": [
    "for name, importance in zip(clf.feature_names_in_, clf.feature_importances_):\n",
    "    print(name, importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_valid[df_results_valid.result == \"FP\"].iloc[:].drop(\n",
    "    columns=[\"beta\", \"intercept\", \"data_window_start\", \"label\", \"prediction\"]\n",
    ").iloc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results_valid[df_results_valid.return_three_month < -0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results_valid[df_results_valid.subindustry == 'consumer_services'].iloc[0:100]"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
