{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from itertools import combinations\n",
    "\n",
    "# Data management\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data fetching\n",
    "import yfinance as yf\n",
    "\n",
    "# Spread generation\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stonk price data download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input ticker names by industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tickers_by_industry(industries=None, data_dir=None, filename=None):\n",
    "    '''\n",
    "    Read the CSV file containing all tickers and their subindustries and return tickers from the selected subindustries in a list.\n",
    "    \n",
    "    -Args:\n",
    "        industries (List(string)): if not given, return all tickers; else the list can contain:\n",
    "            'technology_hardware_and_equipment'\n",
    "            'software_and_services'\n",
    "            'media_and_entertainment'\n",
    "            'retailing'\n",
    "            'automobiles_and_components'\n",
    "            'semiconductors_and_semiconductor_equipment'\n",
    "            'health_care_equipment_and_services'\n",
    "            'banks'\n",
    "            'pharmaceuticals_biotechnology_and_life_sciences'\n",
    "            'food_and_staples_retailing'\n",
    "            'oil_gas_and_consumable_fuels'\n",
    "            'food_beverage_and_tobacco'\n",
    "            'telecommunication_services'\n",
    "            'consumer_durables_and_apparel'\n",
    "            'consumer_services'\n",
    "            'transportation'\n",
    "            'diversified_financials'\n",
    "            'utilities'\n",
    "            'capital_goods'\n",
    "            'insurance'\n",
    "            'chemicals'\n",
    "            'metals_and_mining'\n",
    "            'commercial_and_professional_services'\n",
    "            'containers_and_packaging'\n",
    "            'energy_equipment_and_services'\n",
    "            'construction_materials'\n",
    "            'paper_and_forest_products'\n",
    "    \n",
    "    -Returns:\n",
    "        tickers (pandas Series): list of selected ticker names\n",
    "    '''\n",
    "    filename = 'stonk_list.csv' if filename is None else filename\n",
    "    data_dir = 'data' if data_dir is None else data_dir\n",
    "    \n",
    "    path_to_csv = os.path.join(data_dir, filename)\n",
    "    stonk_list = pd.read_csv(path_to_csv)\n",
    "    return stonk_list.set_index('ticker') if industries is None else stonk_list[stonk_list['subindustry'].isin(industries)].set_index('ticker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_stonk_prices(stonk_list, period_years=3, date_from=None, date_to=None, interval='1d', source='yfinance', data_dir='data', proxy=False):    \n",
    "    '''\n",
    "    Returns historical price data for the selected stonks.\n",
    "\n",
    "    -Args:\n",
    "        stonk_list (List(string)): List of stonk identifiers as strings, case unsensitive\n",
    "        period_years (float): How many years of data to download until date_to, can be a floating point number\n",
    "    -Optional:\n",
    "        date_from (datetime): Start date for stonk data (use instead of period_years)\n",
    "        date_to (datetime): End date for stonk data\n",
    "        interval (string): Valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
    "        source (string): Where to source data from. Valid sources: yfinance\n",
    "        data_dir (string): Folder name where to output downloaded data\n",
    "        file_prefix (string): Prefix of CSV file containing downloaded data inside data_dir\n",
    "        proxy (boolean): Whether to use a proxy connection to avoid API limits/blocks\n",
    "                \n",
    "    -Returns:\n",
    "        stonks (Pandas Dataframe): Pandas Dataframe containing requested ticker prices\n",
    "    '''\n",
    "    \n",
    "    date_to = datetime.now() if date_to is None else date_to\n",
    "    date_from = date_to-(timedelta(days=int(365*period_years))) if date_from is None else date_from\n",
    "    \n",
    "    if source.lower() == 'yfinance':\n",
    "        stonks = yf.download(list(stonk_list), start=date_from, end=date_to, interval=interval, group_by='column', threads=True, rounding=True)['Adj Close']\n",
    "        stonks.dropna(axis=0, how='all', inplace=True)\n",
    "        stonks.sort_values(by='Date', inplace=True)\n",
    "        \n",
    "        stonks.index = pd.to_datetime(stonks.index).date\n",
    "        stonks.index.name = 'date'\n",
    "\n",
    "        clean_stonks = stonks.dropna(axis=1, how='all', thresh=len(stonks.index) * 0.95).copy()\n",
    "        clean_stonks.dropna(axis=0, how='all', thresh=len(clean_stonks.columns) * 0.95, inplace=True)\n",
    "        clean_stonks.fillna(axis=1, method='ffill', inplace=True)\n",
    "        clean_stonks.dropna(axis=1, how='any', inplace=True)\n",
    "        \n",
    "        # Must be no NA values left\n",
    "        assert clean_stonks.isna().sum().sum() == 0\n",
    "    else:\n",
    "        raise ValueError('Unsupported data source')\n",
    "        \n",
    "    def stonks_to_csv(stonks, clean):\n",
    "        from_date_string = stonks.index[0]\n",
    "        to_date_string = stonks.index[-1]\n",
    "\n",
    "        filename = 'stonks_{from_date}_to_{to_date}.csv'.format(from_date=from_date_string, to_date=to_date_string)\n",
    "        \n",
    "        if clean:\n",
    "            filename = 'clean_' + filename\n",
    "            \n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "\n",
    "        stonks.to_csv(path_or_buf=file_path, header=True, index=True, na_rep='NaN')\n",
    "    \n",
    "    stonks_to_csv(stonks, clean=False)\n",
    "    stonks_to_csv(clean_stonks, clean=True)\n",
    "    \n",
    "    return (stonks, clean_stonks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock price data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stonk_data(date_from, date_to, clean=True, date_index=False, data_dir=None):\n",
    "    data_dir = 'data' if data_dir is None else data_dir\n",
    "    data_prefix = 'clean_stonks' if clean else 'stonks'\n",
    "    \n",
    "    path = os.path.join(data_dir, '{}_{}_to_{}.csv'.format(data_prefix, date_from, date_to))\n",
    "    stonks = pd.read_csv(path)\n",
    "    \n",
    "    if clean:\n",
    "        assert stonks.isna().sum().sum() == 0\n",
    "    \n",
    "    if date_index:\n",
    "        return stonks.set_index('date')\n",
    "    else:\n",
    "        return stonks.set_index('date').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stonk_data_by_industry(date_from, date_to, industries=None, stonk_list_filename=None, data_dir=None):\n",
    "    '''\n",
    "    Read the CSV file containing all stonk price data and return the tickers from the selected subindustries.\n",
    "    \n",
    "    -Args:\n",
    "        industries (List(string)): if not given, return all tickers; else the list can contain:\n",
    "            'technology_hardware_and_equipment'\n",
    "            'software_and_services'\n",
    "            'media_and_entertainment'\n",
    "            'retailing'\n",
    "            'automobiles_and_components'\n",
    "            'semiconductors_and_semiconductor_equipment'\n",
    "            'health_care_equipment_and_services'\n",
    "            'banks'\n",
    "            'pharmaceuticals_biotechnology_and_life_sciences'\n",
    "            'food_and_staples_retailing'\n",
    "            'oil_gas_and_consumable_fuels'\n",
    "            'food_beverage_and_tobacco'\n",
    "            'telecommunication_services'\n",
    "            'consumer_durables_and_apparel'\n",
    "            'consumer_services'\n",
    "            'transportation'\n",
    "            'diversified_financials'\n",
    "            'utilities'\n",
    "            'capital_goods'\n",
    "            'insurance'\n",
    "            'chemicals'\n",
    "            'metals_and_mining'\n",
    "            'commercial_and_professional_services'\n",
    "            'containers_and_packaging'\n",
    "            'energy_equipment_and_services'\n",
    "            'construction_materials'\n",
    "            'paper_and_forest_products'\n",
    "    \n",
    "    -Returns:\n",
    "        stonks (pandas DataFrame): list of selected tickers' price data\n",
    "    '''\n",
    "    all_stonks = read_stonk_data(date_from, date_to, date_index=False, data_dir=data_dir, clean=True)\n",
    "    \n",
    "    if industries is None or not industries:\n",
    "        return all_stonks\n",
    "    else: \n",
    "        all_tickers = get_tickers_by_industry(industries=None, data_dir=data_dir, filename=stonk_list_filename)\n",
    "        all_stonks = all_stonks.join(all_tickers, how='inner')\n",
    "        return all_stonks[all_stonks['subindustry'].isin(industries)].drop(columns='subindustry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make combinations with numpy\n",
    "def combine_stonk_pairs(stonks_prices):\n",
    "    # All ticker names must be unique\n",
    "    assert all(stonks_prices.index.unique() == stonks_prices.index)\n",
    "    assert(len(stonks_prices) < 300)\n",
    "    \n",
    "    combs = np.asarray(list(combinations(stonks_prices.index.unique(), 2)))\n",
    "    \n",
    "    return stonks_prices.loc[combs[:, 0]], stonks_prices.loc[combs[:, 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Linear regression residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_residuals_many(X, Y):\n",
    "    '''\n",
    "    Vectorized calculation of residuals from many univariate linear regressions.\n",
    "        Args:\n",
    "        - X (numpy array of shape (n_pairs, d_time)): matrix of LR inputs X, each row represents a different regression, corresponding to the same rows in Y\n",
    "        - Y (numpy array of shape (n_pairs, d_time)): matrix of LR inputs Y, each row represents a different regression, corresponding to the same rows in X\n",
    "        Returns:\n",
    "        - residuals (numpy array of shape (n_pairs, d_time)): matrix of resulting residuals between vectorized pairs of X and Y\n",
    "        - betas (numpy array of shape (n_pairs, 1)): beta coefficients for each linear regression\n",
    "        - Y_hat (numpy array of shape (n_pairs, d_time)): predictions using X\n",
    "    '''\n",
    "    # Stack 2D matrices into 3D matrices\n",
    "    X = X.reshape(np.shape(X)[0], np.shape(X)[1], -1)\n",
    "    Y = Y.reshape(np.shape(Y)[0], np.shape(Y)[1], -1)\n",
    "    \n",
    "    # Add bias/intercept in the form (Xi, 1)\n",
    "    Z = np.concatenate([X, np.ones((np.shape(X)[0], np.shape(X)[1], 1))], axis=2)\n",
    "    \n",
    "    # Save the transpose as it's used a couple of times\n",
    "    Z_t = Z.transpose(0, 2, 1)\n",
    "    \n",
    "    # Linear Regression equation solutions w.r.t. weight matrix\n",
    "    # W contains (beta_coef, a_intercept) for each regression\n",
    "    W = np.matmul(np.linalg.inv(np.matmul(Z_t, Z)),  np.matmul(Z_t, Y))\n",
    "    \n",
    "    # Predictions and residuals\n",
    "    # Y_hat = np.matmul(Z, W).round(2)\n",
    "    residuals = (Y - np.matmul(Z, W)).round(2)\n",
    "    \n",
    "    # Y_hat returned for debugging purposes\n",
    "    # return (residuals[:, :, 0], W[:, 0, 0], Y_hat[:, :, 0])\n",
    "    return (residuals[:, :, 0], W[:, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rolling_residuals(X, Y, l_reg, l_roll, dt, data_dir='data'):\n",
    "    '''\n",
    "    Calculates rolling window residuals in vectorized form. Returns the result as an array that repeats each ticker for the number of regressions calculated.\n",
    "    For example, if the inputs are (Pair A, Pair B, Pair C) and l_roll / dt = 3, then the returned results will have the form as follows:\n",
    "    (Pair A, Pair A, Pair A, Pair B, Pair B, Pair B, Pair C, Pair C, Pair C)\n",
    "    Works best when l_reg and l_roll are integers.\n",
    "        Args:\n",
    "        - X (DataFrame of shape (n_pairs, >= l_reg + l_roll)): matrix of LR inputs X, each row representing not less than complete data period for rolling regressions (can be longer)\n",
    "        - Y (DataFrame of shape (n_pairs, >= l_reg + l_roll)): matrix of LR inputs Y, each row representing not less than complete data period for rolling regressions (can be longer)\n",
    "        - l_reg (float): length of each LR to calculate residuals, in years; will be multiplied by the adjusted number of days in a trading year\n",
    "        - l_roll (float): length of rolling window, in years; will be multipled by the adjusted number of days in a trading year\n",
    "        - dt (int): rolling window step size, in trading days; total trading year days will be reduced to be divisible by dt (by not more than the value of dt)\n",
    "        Returns:\n",
    "        - residuals (numpy array of shape (n_pairs * (l_roll/dt)+1, l_reg + l_roll)): matrix of resulting residuals between vectorized pairs of X and Y\n",
    "        - betas (numpy array of shape (n_pairs * (l_roll/dt)+1, 1)): beta coefficients for each linear regression\n",
    "        - Y_hat (numpy array of shape (n_pairs * (l_roll/dt)+1, l_reg + l_roll)): predictions using X\n",
    "    '''\n",
    "    \n",
    "    _DAYS_IN_TRADING_YEAR = 252\n",
    "    \n",
    "    # Adjust days in a year so that the number is divisible by dt\n",
    "    _DAYS_IN_TRADING_YEAR = _DAYS_IN_TRADING_YEAR - (_DAYS_IN_TRADING_YEAR % dt)\n",
    "    l_reg_days = int(_DAYS_IN_TRADING_YEAR * l_reg)\n",
    "    l_roll_days = int(_DAYS_IN_TRADING_YEAR * l_roll)\n",
    "    total_days = l_reg_days + l_roll_days\n",
    "    n_windows = (l_roll_days // dt) + 1\n",
    "    n_x = X.shape[0]\n",
    "    \n",
    "    X_index = np.repeat(X.index, n_windows)\n",
    "    Y_index = np.repeat(Y.index, n_windows)\n",
    "    \n",
    "    date_index = X.columns[-total_days:]\n",
    "    date_index_windowed = np.empty(shape=(n_x*n_windows, 2), dtype='O')\n",
    "    \n",
    "    X = X.to_numpy().astype(np.float64)\n",
    "    Y = Y.to_numpy().astype(np.float64)\n",
    "    \n",
    "    # Rolling window length must be divisible by dt\n",
    "    assert (l_roll_days % dt) == 0\n",
    "    \n",
    "    # There has to be enough days' worth of data in X (and Y) and their shapes must match\n",
    "    assert X.shape == Y.shape and X.shape[1] >= total_days\n",
    "    \n",
    "    # Take the total_days from the end of the arrays (most recent days first, oldest days at the end are cut off)\n",
    "    X = X[:, -total_days:]\n",
    "    Y = Y[:, -total_days:]\n",
    "    \n",
    "    # Create empty arrays that will contain windowed slices of our data\n",
    "    X_windows = np.empty(shape=(n_x*n_windows, l_reg_days))\n",
    "    Y_windows = np.empty(shape=(n_x*n_windows, l_reg_days))\n",
    "    \n",
    "    # Take windowed slices and place them into the created empty arrays\n",
    "    for n in range(n_x):\n",
    "        for i in range(n_windows):\n",
    "            X_windows[(n*n_windows)+i] = X[n, i*dt:l_reg_days+(i*dt)]\n",
    "            Y_windows[(n*n_windows)+i] = Y[n, i*dt:l_reg_days+(i*dt)]\n",
    "            date_index_windowed[(n*n_windows)+i, 0] = date_index[i*dt]\n",
    "            date_index_windowed[(n*n_windows)+i, 1] = date_index[l_reg_days+(i*dt)-1]\n",
    "    \n",
    "    # Make sure we've got the windowing dimensions right\n",
    "    assert X_windows.shape == (n_x*n_windows, l_reg_days) and Y_windows.shape == (n_x*n_windows, l_reg_days)\n",
    "    \n",
    "    # Sanity check\n",
    "    assert all([\n",
    "        X[0, -1] == X_windows[n_windows-1, -1],\n",
    "        Y[0, -1] == Y_windows[n_windows-1, -1],\n",
    "        X[-1, -1] == X_windows[-1, -1],\n",
    "        Y[-1, -1] == Y_windows[-1, -1],\n",
    "    ])\n",
    "    \n",
    "    # Construct index column\n",
    "    XY_index = pd.DataFrame(np.array([X_index, Y_index, date_index_windowed[:, 0], date_index_windowed[:, 1]])).apply('_'.join, axis=0, raw=True)\n",
    "    \n",
    "    # Calculate and return the residuals\n",
    "    res, betas = get_residuals_many(X_windows, Y_windows)\n",
    "    \n",
    "    res = pd.DataFrame(res, index=XY_index)\n",
    "    betas = pd.DataFrame(betas, index=XY_index)\n",
    "    \n",
    "    time = datetime.now().time()\n",
    "    res.to_csv(os.path.join(data_dir, time.strftime('residuals_%H%M%S.csv')), header=False, index=True)\n",
    "    betas.to_csv(os.path.join(data_dir, time.strftime('betas_%H%M%S.csv')), header=False, index=True)\n",
    "    pd.Series(date_index).to_csv(os.path.join(data_dir, time.strftime('dates_%H%M%S.csv')), header=False, index=False)\n",
    "    \n",
    "    return res, betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline example tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Download stock daily prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets all ticker names (no argument given)\n",
    "ticker_list = get_tickers_by_industry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific date - 3rd of March 2022 (Y, M, D)\n",
    "date_to = datetime(2022, 3, 1)\n",
    "# Date of today\n",
    "date_to = datetime.today()\n",
    "# How many years' of data to download (going backwards from date_end). Year can be a floating point number\n",
    "period_years = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  2283 of 2283 completed\n",
      "\n",
      "9 Failed downloads:\n",
      "- WFC PRN: No data found, symbol may be delisted\n",
      "- ET-PD: No data found for this date range, symbol may be delisted\n",
      "- SNX.VI: No data found, symbol may be delisted\n",
      "- ET-PE: No data found for this date range, symbol may be delisted\n",
      "- FHN PRA: No data found, symbol may be delisted\n",
      "- FTAI-PA: No data found for this date range, symbol may be delisted\n",
      "- ET-PC: No data found for this date range, symbol may be delisted\n",
      "- ALL-PB: No data found for this date range, symbol may be delisted\n",
      "- RXN.VI: No data found, symbol may be delisted\n"
     ]
    }
   ],
   "source": [
    "# Download ticker price data for the tickers selected above (saved to .csv automatically)\n",
    "df, df_clean = download_stonk_prices(ticker_list.index, period_years=period_years, date_to=date_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>AA</th>\n",
       "      <th>AAL</th>\n",
       "      <th>AAON</th>\n",
       "      <th>AAP</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AAWW</th>\n",
       "      <th>AAXJ</th>\n",
       "      <th>AB</th>\n",
       "      <th>ABBV</th>\n",
       "      <th>...</th>\n",
       "      <th>ZGNX</th>\n",
       "      <th>ZION</th>\n",
       "      <th>ZIONO</th>\n",
       "      <th>ZIONP</th>\n",
       "      <th>ZNGA</th>\n",
       "      <th>ZS</th>\n",
       "      <th>ZTS</th>\n",
       "      <th>ZUMZ</th>\n",
       "      <th>ZUO</th>\n",
       "      <th>ZWS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-03-08</th>\n",
       "      <td>67.70</td>\n",
       "      <td>47.37</td>\n",
       "      <td>53.50</td>\n",
       "      <td>35.45</td>\n",
       "      <td>112.53</td>\n",
       "      <td>42.61</td>\n",
       "      <td>60.55</td>\n",
       "      <td>73.75</td>\n",
       "      <td>19.00</td>\n",
       "      <td>94.68</td>\n",
       "      <td>...</td>\n",
       "      <td>44.50</td>\n",
       "      <td>51.12</td>\n",
       "      <td>21.49</td>\n",
       "      <td>24.43</td>\n",
       "      <td>3.71</td>\n",
       "      <td>3.71</td>\n",
       "      <td>80.63</td>\n",
       "      <td>20.25</td>\n",
       "      <td>20.25</td>\n",
       "      <td>29.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-09</th>\n",
       "      <td>68.94</td>\n",
       "      <td>47.76</td>\n",
       "      <td>54.75</td>\n",
       "      <td>36.91</td>\n",
       "      <td>115.56</td>\n",
       "      <td>43.34</td>\n",
       "      <td>61.55</td>\n",
       "      <td>75.37</td>\n",
       "      <td>19.10</td>\n",
       "      <td>96.52</td>\n",
       "      <td>...</td>\n",
       "      <td>45.00</td>\n",
       "      <td>51.98</td>\n",
       "      <td>21.66</td>\n",
       "      <td>24.43</td>\n",
       "      <td>3.72</td>\n",
       "      <td>3.72</td>\n",
       "      <td>82.56</td>\n",
       "      <td>19.55</td>\n",
       "      <td>19.55</td>\n",
       "      <td>30.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-12</th>\n",
       "      <td>68.54</td>\n",
       "      <td>48.25</td>\n",
       "      <td>55.17</td>\n",
       "      <td>37.25</td>\n",
       "      <td>115.12</td>\n",
       "      <td>43.76</td>\n",
       "      <td>61.30</td>\n",
       "      <td>75.60</td>\n",
       "      <td>19.00</td>\n",
       "      <td>95.72</td>\n",
       "      <td>...</td>\n",
       "      <td>44.80</td>\n",
       "      <td>51.56</td>\n",
       "      <td>21.54</td>\n",
       "      <td>24.41</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.75</td>\n",
       "      <td>82.45</td>\n",
       "      <td>19.90</td>\n",
       "      <td>19.90</td>\n",
       "      <td>30.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-13</th>\n",
       "      <td>68.51</td>\n",
       "      <td>48.48</td>\n",
       "      <td>54.91</td>\n",
       "      <td>37.69</td>\n",
       "      <td>115.69</td>\n",
       "      <td>43.33</td>\n",
       "      <td>62.35</td>\n",
       "      <td>75.06</td>\n",
       "      <td>18.75</td>\n",
       "      <td>96.90</td>\n",
       "      <td>...</td>\n",
       "      <td>44.50</td>\n",
       "      <td>50.80</td>\n",
       "      <td>21.64</td>\n",
       "      <td>24.39</td>\n",
       "      <td>3.70</td>\n",
       "      <td>3.70</td>\n",
       "      <td>82.63</td>\n",
       "      <td>20.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-14</th>\n",
       "      <td>67.77</td>\n",
       "      <td>46.47</td>\n",
       "      <td>53.98</td>\n",
       "      <td>37.55</td>\n",
       "      <td>113.81</td>\n",
       "      <td>42.97</td>\n",
       "      <td>62.85</td>\n",
       "      <td>75.33</td>\n",
       "      <td>18.47</td>\n",
       "      <td>96.24</td>\n",
       "      <td>...</td>\n",
       "      <td>44.50</td>\n",
       "      <td>49.92</td>\n",
       "      <td>21.62</td>\n",
       "      <td>24.05</td>\n",
       "      <td>3.77</td>\n",
       "      <td>3.77</td>\n",
       "      <td>82.39</td>\n",
       "      <td>19.60</td>\n",
       "      <td>19.60</td>\n",
       "      <td>29.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-28</th>\n",
       "      <td>130.36</td>\n",
       "      <td>75.34</td>\n",
       "      <td>17.25</td>\n",
       "      <td>58.56</td>\n",
       "      <td>204.48</td>\n",
       "      <td>165.12</td>\n",
       "      <td>78.36</td>\n",
       "      <td>78.62</td>\n",
       "      <td>44.84</td>\n",
       "      <td>147.77</td>\n",
       "      <td>...</td>\n",
       "      <td>26.26</td>\n",
       "      <td>70.89</td>\n",
       "      <td>26.16</td>\n",
       "      <td>23.54</td>\n",
       "      <td>9.08</td>\n",
       "      <td>239.15</td>\n",
       "      <td>193.65</td>\n",
       "      <td>44.49</td>\n",
       "      <td>15.18</td>\n",
       "      <td>32.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-01</th>\n",
       "      <td>131.93</td>\n",
       "      <td>79.78</td>\n",
       "      <td>16.29</td>\n",
       "      <td>57.18</td>\n",
       "      <td>201.17</td>\n",
       "      <td>163.20</td>\n",
       "      <td>80.39</td>\n",
       "      <td>77.91</td>\n",
       "      <td>43.38</td>\n",
       "      <td>147.69</td>\n",
       "      <td>...</td>\n",
       "      <td>26.36</td>\n",
       "      <td>64.94</td>\n",
       "      <td>26.00</td>\n",
       "      <td>22.17</td>\n",
       "      <td>9.00</td>\n",
       "      <td>247.83</td>\n",
       "      <td>192.54</td>\n",
       "      <td>43.04</td>\n",
       "      <td>15.21</td>\n",
       "      <td>31.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-02</th>\n",
       "      <td>133.16</td>\n",
       "      <td>82.56</td>\n",
       "      <td>16.37</td>\n",
       "      <td>55.42</td>\n",
       "      <td>209.07</td>\n",
       "      <td>166.56</td>\n",
       "      <td>80.84</td>\n",
       "      <td>78.03</td>\n",
       "      <td>44.81</td>\n",
       "      <td>149.57</td>\n",
       "      <td>...</td>\n",
       "      <td>26.37</td>\n",
       "      <td>68.38</td>\n",
       "      <td>25.74</td>\n",
       "      <td>22.70</td>\n",
       "      <td>9.12</td>\n",
       "      <td>254.41</td>\n",
       "      <td>196.07</td>\n",
       "      <td>45.00</td>\n",
       "      <td>15.40</td>\n",
       "      <td>33.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-03</th>\n",
       "      <td>137.17</td>\n",
       "      <td>82.89</td>\n",
       "      <td>15.71</td>\n",
       "      <td>54.01</td>\n",
       "      <td>208.27</td>\n",
       "      <td>166.23</td>\n",
       "      <td>80.66</td>\n",
       "      <td>76.85</td>\n",
       "      <td>44.37</td>\n",
       "      <td>150.41</td>\n",
       "      <td>...</td>\n",
       "      <td>26.26</td>\n",
       "      <td>68.00</td>\n",
       "      <td>25.96</td>\n",
       "      <td>22.30</td>\n",
       "      <td>9.14</td>\n",
       "      <td>242.03</td>\n",
       "      <td>195.87</td>\n",
       "      <td>46.01</td>\n",
       "      <td>14.00</td>\n",
       "      <td>33.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-04</th>\n",
       "      <td>133.89</td>\n",
       "      <td>90.66</td>\n",
       "      <td>14.59</td>\n",
       "      <td>54.32</td>\n",
       "      <td>206.62</td>\n",
       "      <td>163.17</td>\n",
       "      <td>82.36</td>\n",
       "      <td>75.19</td>\n",
       "      <td>43.22</td>\n",
       "      <td>150.56</td>\n",
       "      <td>...</td>\n",
       "      <td>26.68</td>\n",
       "      <td>65.39</td>\n",
       "      <td>25.75</td>\n",
       "      <td>22.00</td>\n",
       "      <td>9.14</td>\n",
       "      <td>225.50</td>\n",
       "      <td>196.84</td>\n",
       "      <td>44.50</td>\n",
       "      <td>13.26</td>\n",
       "      <td>32.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1006 rows Ã— 1753 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 A     AA    AAL   AAON     AAP    AAPL   AAWW   AAXJ     AB  \\\n",
       "date                                                                           \n",
       "2018-03-08   67.70  47.37  53.50  35.45  112.53   42.61  60.55  73.75  19.00   \n",
       "2018-03-09   68.94  47.76  54.75  36.91  115.56   43.34  61.55  75.37  19.10   \n",
       "2018-03-12   68.54  48.25  55.17  37.25  115.12   43.76  61.30  75.60  19.00   \n",
       "2018-03-13   68.51  48.48  54.91  37.69  115.69   43.33  62.35  75.06  18.75   \n",
       "2018-03-14   67.77  46.47  53.98  37.55  113.81   42.97  62.85  75.33  18.47   \n",
       "...            ...    ...    ...    ...     ...     ...    ...    ...    ...   \n",
       "2022-02-28  130.36  75.34  17.25  58.56  204.48  165.12  78.36  78.62  44.84   \n",
       "2022-03-01  131.93  79.78  16.29  57.18  201.17  163.20  80.39  77.91  43.38   \n",
       "2022-03-02  133.16  82.56  16.37  55.42  209.07  166.56  80.84  78.03  44.81   \n",
       "2022-03-03  137.17  82.89  15.71  54.01  208.27  166.23  80.66  76.85  44.37   \n",
       "2022-03-04  133.89  90.66  14.59  54.32  206.62  163.17  82.36  75.19  43.22   \n",
       "\n",
       "              ABBV  ...   ZGNX   ZION  ZIONO  ZIONP  ZNGA      ZS     ZTS  \\\n",
       "date                ...                                                     \n",
       "2018-03-08   94.68  ...  44.50  51.12  21.49  24.43  3.71    3.71   80.63   \n",
       "2018-03-09   96.52  ...  45.00  51.98  21.66  24.43  3.72    3.72   82.56   \n",
       "2018-03-12   95.72  ...  44.80  51.56  21.54  24.41  3.75    3.75   82.45   \n",
       "2018-03-13   96.90  ...  44.50  50.80  21.64  24.39  3.70    3.70   82.63   \n",
       "2018-03-14   96.24  ...  44.50  49.92  21.62  24.05  3.77    3.77   82.39   \n",
       "...            ...  ...    ...    ...    ...    ...   ...     ...     ...   \n",
       "2022-02-28  147.77  ...  26.26  70.89  26.16  23.54  9.08  239.15  193.65   \n",
       "2022-03-01  147.69  ...  26.36  64.94  26.00  22.17  9.00  247.83  192.54   \n",
       "2022-03-02  149.57  ...  26.37  68.38  25.74  22.70  9.12  254.41  196.07   \n",
       "2022-03-03  150.41  ...  26.26  68.00  25.96  22.30  9.14  242.03  195.87   \n",
       "2022-03-04  150.56  ...  26.68  65.39  25.75  22.00  9.14  225.50  196.84   \n",
       "\n",
       "             ZUMZ    ZUO    ZWS  \n",
       "date                             \n",
       "2018-03-08  20.25  20.25  29.87  \n",
       "2018-03-09  19.55  19.55  30.68  \n",
       "2018-03-12  19.90  19.90  30.12  \n",
       "2018-03-13  20.00  20.00  30.00  \n",
       "2018-03-14  19.60  19.60  29.68  \n",
       "...           ...    ...    ...  \n",
       "2022-02-28  44.49  15.18  32.52  \n",
       "2022-03-01  43.04  15.21  31.42  \n",
       "2022-03-02  45.00  15.40  33.40  \n",
       "2022-03-03  46.01  14.00  33.21  \n",
       "2022-03-04  44.50  13.26  32.90  \n",
       "\n",
       "[1006 rows x 1753 columns]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Read stock data by industry"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'technology_hardware_and_equipment'\n",
    "'software_and_services'\n",
    "'media_and_entertainment'\n",
    "'retailing'\n",
    "'automobiles_and_components'\n",
    "'semiconductors_and_semiconductor_equipment'\n",
    "'health_care_equipment_and_services'\n",
    "'banks'\n",
    "'pharmaceuticals_biotechnology_and_life_sciences'\n",
    "'food_and_staples_retailing'\n",
    "'oil_gas_and_consumable_fuels'\n",
    "'food_beverage_and_tobacco'\n",
    "'telecommunication_services'\n",
    "'consumer_durables_and_apparel'\n",
    "'consumer_services'\n",
    "'transportation'\n",
    "'diversified_financials'\n",
    "'utilities'\n",
    "'capital_goods'\n",
    "'insurance'\n",
    "'chemicals'\n",
    "'metals_and_mining'\n",
    "'commercial_and_professional_services'\n",
    "'containers_and_packaging'\n",
    "'energy_equipment_and_services'\n",
    "'construction_materials'\n",
    "'paper_and_forest_products'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stock data from selected industries only\n",
    "stonks = get_stonk_data_by_industry('2018-03-08', '2022-03-04', industries=['consumer_durables_and_apparel', 'consumer_services'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stock data of ALL industries (all tickers) - no arguments specified\n",
    "stonks = get_stonk_data_by_industry('2018-03-08', '2022-03-04')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Input list of ticker pairs from CSV and select price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_pairs = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Calculate residuals (exported to CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 4s\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "residuals, betas = get_rolling_residuals(X, Y, l_reg=3, l_roll=1, dt=5)\n",
    "t2 = time.time()\n",
    "print(\"Time: \" + str(int(t2-t1)) + 's') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Slow residual functions (for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_rolling_slow_residuals(X, Y, l_reg, l_roll, dt):\n",
    "#     _DAYS_IN_TRADING_YEAR = (252) - (252 % dt)\n",
    "#     l_reg_days = _DAYS_IN_TRADING_YEAR * l_reg\n",
    "#     l_roll_days = _DAYS_IN_TRADING_YEAR * l_roll\n",
    "#     total_days = l_reg_days + l_roll_days\n",
    "#     n_windows = l_roll_days // dt\n",
    "#     n_x = X.shape[0]\n",
    "    \n",
    "#     assert (l_roll_days % dt) == 0\n",
    "#     assert X.shape[1] >= total_days and Y.shape[1] >= total_days\n",
    "    \n",
    "#     X = X[:, -total_days:]\n",
    "#     Y = Y[:, -total_days:]\n",
    "    \n",
    "#     # First window\n",
    "#     X_windows = np.empty(shape=(n_x*n_windows, l_reg_days))\n",
    "#     Y_windows = np.empty(shape=(n_x*n_windows, l_reg_days))\n",
    "    \n",
    "#     for n in range(n_x):\n",
    "#         for i in range(n_windows):\n",
    "#             X_windows = np.concatenate(( X_windows, X[n, i*dt:l_reg_days+(i*dt)] ))\n",
    "#             Y_windows = np.concatenate(( Y_windows, Y[n, i*dt:l_reg_days+(i*dt)] ))\n",
    "    \n",
    "#     assert X_windows.shape == (n_x*n_windows, l_reg_days) and Y_windows.shape == (n_x*n_windows, l_reg_days)\n",
    "    \n",
    "#     return get_slow_residuals_many(X_windows, Y_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_slow_residuals_many(X, Y, n_jobs=-1):\n",
    "#     lr = LinearRegression(n_jobs=n_jobs, fit_intercept=True)\n",
    "#     X = X.reshape((X.shape[0], X.shape[1], -1))\n",
    "#     Y = Y.reshape((Y.shape[0], Y.shape[1], -1))\n",
    "    \n",
    "#     preds = []\n",
    "#     res = []\n",
    "#     betas = []\n",
    "#     for i in range(X.shape[0]):\n",
    "#         lr.fit(X[i], Y[i])\n",
    "#         preds.append(lr.predict(X[i]).round(2))\n",
    "#         res.append(Y[i]-preds[-1])\n",
    "#         betas.append(lr.coef_[0][0])\n",
    "#     return (np.asarray(res)[:,:,0], np.asarray(preds)[:,:,0], np.asarray(betas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1_fast = time.time()\n",
    "# res, betas, preds  = get_rolling_residuals(X, Y, l_reg=2, l_roll=1, dt=5)\n",
    "# t2_fast = time.time()\n",
    "\n",
    "# t1_slow = time.time()\n",
    "# res_slow, preds_slow = get_rolling_slow_residuals(X, Y, l_reg=2, l_roll=1, dt=5)\n",
    "# t2_slow = time.time()\n",
    "\n",
    "# print(\"Time slow: \" + str(t2_slow-t1_slow))\n",
    "# print(\"Time fast: \" + str(t2_fast-t1_fast))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1_fast = time.time()\n",
    "# res, preds, betas = get_residuals_many(X, Y)\n",
    "# t2_fast = time.time()\n",
    "\n",
    "# t1_slow = time.time()\n",
    "# res_slow, preds_slow, betas_slow = get_slow_residuals_many(X, Y)\n",
    "# t2_slow = time.time()\n",
    "\n",
    "# print(\"Time slow: \" + str(t2_slow-t1_slow))\n",
    "# print(\"Time fast: \" + str(t2_fast-t1_fast))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Stock list preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_stock_list(raw_data_path='data/raw_stonk_list.xls', output_path='data/stonk_list.csv'):\n",
    "    '''\n",
    "    Parses a raw excel file from CapitalIQ containing ticker names and their subindustries, validates\n",
    "    unusual ticker names with Yahoo Finance, saving the processed data in CSV format.\n",
    "\n",
    "        Parameters:\n",
    "            Required:\n",
    "                raw_data_path (string):\n",
    "                    Path to the raw excel file.\n",
    "                output_path (string):\n",
    "                    Path where to save the parsed data.\n",
    "                \n",
    "        Returns:\n",
    "            Nothing\n",
    "    '''\n",
    "    \n",
    "    df = pd.read_excel(io=raw_data_path)\n",
    "    \n",
    "    # Drop NA rows\n",
    "    df.dropna(axis=0, inplace=True)\n",
    "    \n",
    "    # Reset index and drop the first row\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    df.drop(index=0, axis=0, inplace=True)\n",
    "    \n",
    "    # Drop unwanted columns\n",
    "    df.drop(columns=df.columns[[1, 2, 3, 4, 5, 7, 8, 9]], inplace=True)\n",
    "    \n",
    "    # Rename remaining columns\n",
    "    df.columns = ['ticker', 'subindustry']\n",
    "    \n",
    "    # Remove the '(Primary)' tag from subindustries\n",
    "    df['subindustry'] = df['subindustry'].str.replace(r' \\(Primary\\)', '')\n",
    "    \n",
    "    # Remove everything until (and including) the semicolon for tickers\n",
    "    df['ticker'] = df['ticker'].str.replace(r'(.*:)', '')\n",
    "    \n",
    "    df['ticker'] = df['ticker'].str.replace(r' WI', '.VI')\n",
    "    df['ticker'] = df['ticker'].str.replace(r'\\.WI', '.VI')\n",
    "    \n",
    "    # Replace the ticker endings for a Yahoo finance supported format\n",
    "    df['ticker'] = df['ticker'].str.replace(r'\\.PR', '-P')\n",
    "    # df['ticker'] = df['ticker'].str.replace(r' PR', '-P')\n",
    "    \n",
    "    # Take all remaining tickers that have a dot\n",
    "    dotted = df[df['ticker'].str.fullmatch(r'[A-Z]*\\.[A-Z]')]\n",
    "    \n",
    "    # Replace the dots with dashes\n",
    "    dashed = dotted.copy()\n",
    "    dashed['ticker'] = dashed['ticker'].str.replace(r'\\.', '-')\n",
    "    \n",
    "    # Remove the dots\n",
    "    undotted = dotted.copy()\n",
    "    undotted['ticker'] = undotted['ticker'].str.replace(r'\\.', '')\n",
    "\n",
    "    # Combine all variantas together\n",
    "    all_variants = pd.concat([dotted, dashed, undotted])\n",
    "    \n",
    "    # Run all of these through Yahoo finance, get last day's price\n",
    "    stonks = yf.download(list(all_variants['ticker'].astype('string').values), period='1m', interval='1d', group_by='column')\n",
    "    \n",
    "    # Drop all NA tickers (that failed to download)\n",
    "    valid_tickers = stonks['Adj Close'].iloc[-1].dropna(axis=0).to_frame().reset_index()\n",
    "    \n",
    "    # Rename columns\n",
    "    valid_tickers.columns = ['ticker', 'price']\n",
    "    \n",
    "    # Add subindustries to the remaining valid tickers\n",
    "    valid_tickers = valid_tickers.join(all_variants.set_index('ticker'), on='ticker')\n",
    "    \n",
    "    # Drop the price column\n",
    "    valid_tickers.drop(columns=valid_tickers.columns[[1]], inplace=True)\n",
    "    \n",
    "    # Remove all tickers that have a dot from main dataframe\n",
    "    df = df[~df['ticker'].str.fullmatch(r'[A-Z]*\\.[A-Z]')]\n",
    "    \n",
    "    # Add the validated tickers back\n",
    "    df = pd.concat([df, valid_tickers], axis=0, ignore_index=True)\n",
    "    \n",
    "    # Make the subindustry strings more code friendly\n",
    "    df['subindustry'] = df['subindustry'].str.replace(' ', '_')\n",
    "    df['subindustry'] = df['subindustry'].str.lower()\n",
    "    df['subindustry'] = df['subindustry'].str.replace(',', '')\n",
    "    \n",
    "    df.to_csv(path_or_buf=output_path, header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
