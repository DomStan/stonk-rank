{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data management\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data fetching\n",
    "import yfinance as yf\n",
    "\n",
    "# Spread generation\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Backtesting\n",
    "\n",
    "# ML\n",
    "\n",
    "# Utils\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import numba\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stonk_data(stonk_list, period_years=3, date_from=None, date_to=datetime.now(), interval='1d', source='yfinance', data_dir='data', file_prefix='stonks', proxy=False):    \n",
    "    '''\n",
    "    Returns historical price data for the selected stonks.\n",
    "\n",
    "    -Args:\n",
    "        stonk_list (string, list): List of stonk identifiers as strings, case unsensitive\n",
    "        period_years (float): How many years of data to download until date_to, can be a floating point number\n",
    "    -Optional:\n",
    "        date_from (datetime): Start date for stonk data (use instead of period_years)\n",
    "        date_to (datetime): End date for stonk data\n",
    "        interval (string): Valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
    "        source (string): Where to source data from. Valid sources: yfinance\n",
    "        data_dir (string): Folder name where to output downloaded data\n",
    "        file_prefix (string): Prefix of CSV file containing downloaded data inside data_dir\n",
    "        proxy (boolean): Whether to use a proxy connection to avoid API limits/blocks\n",
    "                \n",
    "    -Returns:\n",
    "        stonks (Pandas Dataframe): Pandas Dataframe containing requested ticker prices\n",
    "    '''\n",
    "    \n",
    "    if date_from is None:\n",
    "        date_from = date_to-(timedelta(days=int(365*period_years)))\n",
    "        \n",
    "    if source.lower() == 'yfinance':\n",
    "        stonks = yf.download(list(stonk_list), start=date_from, end=date_to, interval=interval, group_by='column', threads=True, rounding=True)['Adj Close']\n",
    "        stonks.dropna(axis=0, how='all', inplace=True)\n",
    "    else:\n",
    "        raise ValueError('Unsupported data source')\n",
    "        \n",
    "    from_date_string = stonks.index[0].strftime('%Y-%m-%d')\n",
    "    to_date_string = stonks.index[-1].strftime('%Y-%m-%d')\n",
    "    \n",
    "    filename = '{prefix}_{from_date}_to_{to_date}.csv'.format(prefix=file_prefix, from_date=from_date_string, to_date=to_date_string)\n",
    "    file_path = os.path.join(data_dir, filename)\n",
    "    \n",
    "    stonks.to_csv(path_or_buf=file_path, header=True, index=True, na_rep='NaN')\n",
    "    \n",
    "    return stonks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stonks = get_stonk_data([\"googl\", \"tsla\", \"ffs\"], period_years=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "stonk_list = pd.read_csv('data/stonk_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  2283 of 2283 completed\n",
      "\n",
      "12 Failed downloads:\n",
      "- WFC PRN: No data found, symbol may be delisted\n",
      "- SNX.VI: No data found, symbol may be delisted\n",
      "- ET-PE: No data found for this date range, symbol may be delisted\n",
      "- FTAI-PA: No data found for this date range, symbol may be delisted\n",
      "- ET-PD: No data found for this date range, symbol may be delisted\n",
      "- AZEK: Error occurred while retrieving timeseries from Redis, keys: [RedisKey [key=AZEK, cluster=finance]]\n",
      "- ALL-PB: No data found for this date range, symbol may be delisted\n",
      "- FHN PRA: No data found, symbol may be delisted\n",
      "- ET-PC: No data found for this date range, symbol may be delisted\n",
      "- WCC-PA: No data found for this date range, symbol may be delisted\n",
      "- RXN.VI: No data found, symbol may be delisted\n",
      "- NRZ-PD: No data found for this date range, symbol may be delisted\n"
     ]
    }
   ],
   "source": [
    "df = get_stonk_data(stonk_list['ticker'], period_years=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(yf.download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(yf.Ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock list preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_stock_list(raw_data_path='data/raw_stonk_list.xls', output_path='data/stonk_list.csv'):\n",
    "    '''\n",
    "    Parses a raw excel file from CapitalIQ containing ticker names and their subindustries, validates\n",
    "    unusual ticker names with Yahoo Finance, saving the processed data in CSV format.\n",
    "\n",
    "        Parameters:\n",
    "            Required:\n",
    "                raw_data_path (string):\n",
    "                    Path to the raw excel file.\n",
    "                output_path (string):\n",
    "                    Path where to save the parsed data.\n",
    "                \n",
    "        Returns:\n",
    "            Nothing\n",
    "    '''\n",
    "    \n",
    "    df = pd.read_excel(io=raw_data_path)\n",
    "    \n",
    "    # Drop NA rows\n",
    "    df.dropna(axis=0, inplace=True)\n",
    "    \n",
    "    # Reset index and drop the first row\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    df.drop(index=0, axis=0, inplace=True)\n",
    "    \n",
    "    # Drop unwanted columns\n",
    "    df.drop(columns=df.columns[[1, 2, 3, 4, 5, 7, 8, 9]], inplace=True)\n",
    "    \n",
    "    # Rename remaining columns\n",
    "    df.columns = ['ticker', 'subindustry']\n",
    "    \n",
    "    # Remove the '(Primary)' tag from subindustries\n",
    "    df['subindustry'] = df['subindustry'].str.replace(r' \\(Primary\\)', '')\n",
    "    \n",
    "    # Remove everything until (and including) the semicolon for tickers\n",
    "    df['ticker'] = df['ticker'].str.replace(r'(.*:)', '')\n",
    "    \n",
    "    df['ticker'] = df['ticker'].str.replace(r' WI', '.VI')\n",
    "    df['ticker'] = df['ticker'].str.replace(r'\\.WI', '.VI')\n",
    "    \n",
    "    # Replace the ticker endings for a Yahoo finance supported format\n",
    "    df['ticker'] = df['ticker'].str.replace(r'\\.PR', '-P')\n",
    "    # df['ticker'] = df['ticker'].str.replace(r' PR', '-P')\n",
    "    \n",
    "    # Take all remaining tickers that have a dot\n",
    "    dotted = df[df['ticker'].str.fullmatch(r'[A-Z]*\\.[A-Z]')]\n",
    "    \n",
    "    # Replace the dots with dashes\n",
    "    dashed = dotted.copy()\n",
    "    dashed['ticker'] = dashed['ticker'].str.replace(r'\\.', '-')\n",
    "    \n",
    "    # Remove the dots\n",
    "    undotted = dotted.copy()\n",
    "    undotted['ticker'] = undotted['ticker'].str.replace(r'\\.', '')\n",
    "\n",
    "    # Combine all variantas together\n",
    "    all_variants = pd.concat([dotted, dashed, undotted])\n",
    "    \n",
    "    # Run all of these through Yahoo finance, get last day's price\n",
    "    stonks = yf.download(list(all_variants['ticker'].astype('string').values), period='1m', interval='1d', group_by='column')\n",
    "    \n",
    "    # Drop all NA tickers (that failed to download)\n",
    "    valid_tickers = stonks['Adj Close'].iloc[-1].dropna(axis=0).to_frame().reset_index()\n",
    "    \n",
    "    # Rename columns\n",
    "    valid_tickers.columns = ['ticker', 'price']\n",
    "    \n",
    "    # Add subindustries to the remaining valid tickers\n",
    "    valid_tickers = valid_tickers.join(all_variants.set_index('ticker'), on='ticker')\n",
    "    \n",
    "    # Drop the price column\n",
    "    valid_tickers.drop(columns=valid_tickers.columns[[1]], inplace=True)\n",
    "    \n",
    "    # Remove all tickers that have a dot from main dataframe\n",
    "    df = df[~df['ticker'].str.fullmatch(r'[A-Z]*\\.[A-Z]')]\n",
    "    \n",
    "    # Add the validated tickers back\n",
    "    df = pd.concat([df, valid_tickers], axis=0, ignore_index=True)\n",
    "    \n",
    "    # Make the subindustry strings more code friendly\n",
    "    df['subindustry'] = df['subindustry'].str.replace(' ', '_')\n",
    "    df['subindustry'] = df['subindustry'].str.lower()\n",
    "    df['subindustry'] = df['subindustry'].str.replace(',', '')\n",
    "    \n",
    "    df.to_csv(path_or_buf=output_path, header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_residuals_many(X, Y):\n",
    "    '''\n",
    "    Vectorized calculation of residuals from many univariate linear regressions.\n",
    "        Args:\n",
    "        - X (numpy array of shape (n_pairs, d_time)): matrix of LR inputs X, each row represents a different regression, corresponding to the same rows in Y\n",
    "        - Y (numpy array of shape (n_pairs, d_time)): matrix of LR inputs Y, each row represents a different regression, corresponding to the same rows in X\n",
    "        Returns:\n",
    "        - residuals (numpy array of shape (n_pairs, d_time)): matrix of resulting residuals between vectorized pairs of X and Y\n",
    "        - Y_hat (numpy array of shape (n_pairs, d_time)): predictions using X\n",
    "        - betas (numpy array of shape (n_pairs, 1)): beta coefficients for each linear regression\n",
    "    '''\n",
    "    # Stack 2D matrices into 3D matrices\n",
    "    X = X.reshape(np.shape(X)[0], np.shape(X)[1], -1)\n",
    "    Y = Y.reshape(np.shape(Y)[0], np.shape(Y)[1], -1)\n",
    "    \n",
    "    # Add bias/intercept in the form (Xi, 1)\n",
    "    Z = np.concatenate([X, np.ones((np.shape(X)[0], np.shape(X)[1], 1))], axis=2)\n",
    "    \n",
    "    # Save the transpose as it's used a couple of times\n",
    "    Z_t = Z.transpose(0, 2, 1)\n",
    "    \n",
    "    # Linear Regression equation solutions w.r.t. weight matrix\n",
    "    # W contains (beta_coef, a_intercept) for each regression\n",
    "    W = np.matmul(np.linalg.inv(np.matmul(Z_t, Z)),  np.matmul(Z_t, Y))\n",
    "    \n",
    "    # Predictions and residuals\n",
    "    Y_hat = np.matmul(Z, W).round(2)\n",
    "    residuals = (Y - Y_hat)\n",
    "    \n",
    "    # TODO: Y_hat returned for debugging purposes\n",
    "    return (residuals[:, :, 0], Y_hat[:, :, 0], W[:, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rolling_residuals(X, Y, l_reg, l_roll, dt):\n",
    "    _DAYS_IN_TRADING_YEAR = 252\n",
    "    \n",
    "    # Adjust days in a year so that the number is divisible by dt\n",
    "    _DAYS_IN_TRADING_YEAR = _DAYS_IN_TRADING_YEAR - (_DAYS_IN_TRADING_YEAR % dt)\n",
    "    l_reg_days = _DAYS_IN_TRADING_YEAR * l_reg\n",
    "    l_roll_days = _DAYS_IN_TRADING_YEAR * l_roll\n",
    "    total_days = l_reg_days + l_roll_days\n",
    "    n_windows = l_roll_days // dt\n",
    "    n_x = X.shape[0]\n",
    "    \n",
    "    # Rolling window length must be divisible by dt\n",
    "    assert (l_roll_days % dt) == 0\n",
    "    \n",
    "    # The shapes of X and Y must match\n",
    "    assert X.shape == Y.shape\n",
    "    \n",
    "    # There has to be enough days' worth of data in X (and Y)\n",
    "    assert X.shape[1] >= total_days\n",
    "    \n",
    "    # Take the total_days from the end of the arrays (most recent days first, oldest days at the end are cut off)\n",
    "    X = X[:, -total_days:]\n",
    "    Y = Y[:, -total_days:]\n",
    "    \n",
    "    # Take the first first rolling window\n",
    "    X_windows = X[:, :l_reg_days].copy()\n",
    "    Y_windows = Y[:, :l_reg_days].copy()\n",
    "    \n",
    "    # Concatenate all the remaining rolling windows to one big array containing all regressions\n",
    "    for i in range(1, n_windows):\n",
    "        X_windows = np.concatenate(( X_windows, X[:, i*dt:l_reg_days+(i*dt)] ))\n",
    "        Y_windows = np.concatenate(( Y_windows, Y[:, i*dt:l_reg_days+(i*dt)] ))\n",
    "    \n",
    "    assert X_windows.shape == (n_x*n_windows, l_reg_days) and Y_windows.shape == (n_x*n_windows, l_reg_days)\n",
    "    \n",
    "    return get_residuals_many(X_windows, Y_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rolling_slow_residuals(X, Y, l_reg, l_roll, dt):\n",
    "    _DAYS_IN_TRADING_YEAR = (252) - (252 % dt)\n",
    "    l_reg_days = _DAYS_IN_TRADING_YEAR * l_reg\n",
    "    l_roll_days = _DAYS_IN_TRADING_YEAR * l_roll\n",
    "    total_days = l_reg_days + l_roll_days\n",
    "    n_windows = l_roll_days // dt\n",
    "    n_x = X.shape[0]\n",
    "    \n",
    "    assert (l_roll_days % dt) == 0\n",
    "    assert X.shape[1] >= total_days and Y.shape[1] >= total_days\n",
    "    \n",
    "    X = X[:, -total_days:]\n",
    "    Y = Y[:, -total_days:]\n",
    "    \n",
    "    # First window\n",
    "    X_windows = X[:, :l_reg_days].copy()\n",
    "    Y_windows = Y[:, :l_reg_days].copy()\n",
    "    \n",
    "    for i in range(1, n_windows):\n",
    "        X_windows = np.concatenate(( X_windows, X[:, i*dt:l_reg_days+(i*dt)] ))\n",
    "        Y_windows = np.concatenate(( Y_windows, Y[:, i*dt:l_reg_days+(i*dt)] ))\n",
    "    \n",
    "    assert X_windows.shape == (n_x*n_windows, l_reg_days) and Y_windows.shape == (n_x*n_windows, l_reg_days)\n",
    "    \n",
    "    return get_slow_residuals_many(X_windows, Y_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slow_residuals_many(X, Y, n_jobs=-1):\n",
    "    lr = LinearRegression(n_jobs=n_jobs, fit_intercept=True)\n",
    "    X = X.reshape((X.shape[0], X.shape[1], -1))\n",
    "    Y = Y.reshape((Y.shape[0], Y.shape[1], -1))\n",
    "    \n",
    "    preds = []\n",
    "    res = []\n",
    "    betas = []\n",
    "    for i in range(X.shape[0]):\n",
    "        lr.fit(X[i], Y[i])\n",
    "        preds.append(lr.predict(X[i]).round(2))\n",
    "        res.append(Y[i]-preds[-1])\n",
    "        betas.append(lr.coef_[0][0])\n",
    "    return (np.asarray(res)[:,:,0], np.asarray(preds)[:,:,0], np.asarray(betas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stonk_data(date_from, date_to, data_dir='data', data_prefix='stonks'):\n",
    "    path = os.path.join(data_dir, '{}_{}_to_{}.csv'.format(data_prefix, date_from, date_to))\n",
    "    stonks = pd.read_csv(path, index_col=0)\n",
    "    stonks.dropna(axis=1, how='all', thresh=len(stonks) * 0.95, inplace=True)\n",
    "    stonks.dropna(axis=0, how='all', thresh=len(stonks) * 0.95, inplace=True)\n",
    "    stonks.fillna(axis=1, method='ffill', inplace=True)\n",
    "    stonks.dropna(axis=1, how='any', inplace=True)\n",
    "    \n",
    "    assert stonks.isna().sum().sum() == 0\n",
    "    \n",
    "    return stonks.to_numpy().T.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stonks = read_stonk_data('2019-02-25', '2022-02-24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = stonks\n",
    "Y = np.flipud(stonks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_roll, preds_roll, betas_roll = get_rolling_residuals(X, Y, l_reg=2, l_roll=1, dt=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91400, 500)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_roll.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time slow: 73.13934755325317\n",
      "Time fast: 14.310967922210693\n"
     ]
    }
   ],
   "source": [
    "t1_fast = time.time()\n",
    "res, preds, betas = get_rolling_residuals(X, Y, l_reg=2, l_roll=1, dt=5)\n",
    "t2_fast = time.time()\n",
    "\n",
    "t1_slow = time.time()\n",
    "res_slow, preds_slow = get_rolling_slow_residuals(X, Y, l_reg=2, l_roll=1, dt=5)\n",
    "t2_slow = time.time()\n",
    "\n",
    "print(\"Time slow: \" + str(t2_slow-t1_slow))\n",
    "print(\"Time fast: \" + str(t2_fast-t1_fast))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 28.18,  27.99,  28.32, ...,  39.36,  39.35,  39.96],\n",
       "       [ 18.45,  18.2 ,  18.52, ...,  17.62,  17.16,  17.82],\n",
       "       [ 28.83,  28.83,  28.83, ...,  28.77,  28.77,  28.77],\n",
       "       ...,\n",
       "       [ 14.27,  13.69,  13.47, ...,  19.4 ,  20.07,  20.21],\n",
       "       [ 26.19,  23.67,  24.04, ...,  34.63,  37.07,  36.86],\n",
       "       [106.55, 103.58, 101.73, ..., 110.35, 113.83, 112.97]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 28.18,  27.99,  28.32, ...,  39.36,  39.35,  39.96],\n",
       "       [ 18.45,  18.2 ,  18.52, ...,  17.62,  17.16,  17.82],\n",
       "       [ 28.83,  28.83,  28.83, ...,  28.77,  28.77,  28.77],\n",
       "       ...,\n",
       "       [ 14.27,  13.69,  13.47, ...,  19.4 ,  20.07,  20.21],\n",
       "       [ 26.19,  23.67,  24.04, ...,  34.63,  37.07,  36.86],\n",
       "       [106.55, 103.58, 101.73, ..., 110.35, 113.83, 112.97]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, preds, betas = get_residuals(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time slow: 1.513979196548462\n",
      "Time fast: 0.22905278205871582\n"
     ]
    }
   ],
   "source": [
    "t1_fast = time.time()\n",
    "res, preds, betas = get_residuals_many(X, Y)\n",
    "t2_fast = time.time()\n",
    "\n",
    "t1_slow = time.time()\n",
    "res_slow, preds_slow, betas_slow = get_slow_residuals_many(X, Y)\n",
    "t2_slow = time.time()\n",
    "\n",
    "print(\"Time slow: \" + str(t2_slow-t1_slow))\n",
    "print(\"Time fast: \" + str(t2_fast-t1_fast))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.24952022,  0.13838666, -0.08973576, ..., -0.05050888,\n",
       "        2.59664332,  2.56862466])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas_slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.24952022,  0.13838666, -0.08973576, ..., -0.05050888,\n",
       "        2.59664332,  2.56862466])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(betas, betas_slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(np.equal(res, res_slow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: rolling residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
