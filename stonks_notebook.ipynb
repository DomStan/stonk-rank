{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install yfinance\n",
    "# !pip install pmdarima\n",
    "# !pip install hyperopt\n",
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "\n",
    "import typing\n",
    "from typing import Dict\n",
    "from typing import Any\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "import utils\n",
    "import pipelines\n",
    "import processing\n",
    "import evaluate\n",
    "import predict\n",
    "import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download stock daily prices & indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gets all ticker names (no argument given)\n",
    "ticker_list = utils.get_ticker_names(market_cap_min_mm=1000, market_cap_max_mm=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Specific date - 3rd of March 2022 (Y, M, D)\n",
    "# date_to = datetime(2021, 1, 18)\n",
    "### Date of today\n",
    "date_to = datetime.today()\n",
    "### How many years' of data to download (going backwards from date_end). Year can be a floating point number\n",
    "period_years = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  2820 of 2820 completed\n",
      "\n",
      "16 Failed downloads:\n",
      "- DELL WI: No data found, symbol may be delisted\n",
      "- BLL: No data found, symbol may be delisted\n",
      "- RXN WI: No data found, symbol may be delisted\n",
      "- O.WI: No data found, symbol may be delisted\n",
      "- SNX.WI: No data found, symbol may be delisted\n",
      "- FOE: No data found, symbol may be delisted\n",
      "- SGMS: No data found, symbol may be delisted\n",
      "- BIP.PRB: No data found, symbol may be delisted\n",
      "- T WD: No data found, symbol may be delisted\n",
      "- BIP.PRA: No data found, symbol may be delisted\n",
      "- PFE.WI: No data found, symbol may be delisted\n",
      "- EPAY: No data found, symbol may be delisted\n",
      "- JOBS: No data found, symbol may be delisted\n",
      "- MRK.WI: No data found, symbol may be delisted\n",
      "- MGP: No data found, symbol may be delisted\n",
      "- POST WI: No data found, symbol may be delisted\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "df, df_clean = utils.download_stonk_prices(\n",
    "    ticker_list.index, period_years=period_years, date_to=date_to\n",
    ")\n",
    "vix, vix_clean = utils.download_stonk_prices(\n",
    "    [\"^VIX\"], period_years=period_years, date_to=date_to, fname_prefix=\"vix\"\n",
    ")\n",
    "sp500, sp500_clean = utils.download_stonk_prices(\n",
    "    [\"^GSPC\"], period_years=period_years, date_to=date_to, fname_prefix=\"sp500\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "industries = [\n",
    "    # 'health_care_equipment_and_services',\n",
    "    # 'software_and_services',\n",
    "    # 'retailing',\n",
    "    # 'telecommunication_services',\n",
    "    \"capital_goods\",\n",
    "    # 'energy',\n",
    "    # 'pharmaceuticals_biotechnology_and_life_sciences',\n",
    "    # 'consumer_staples',\n",
    "    # 'banks',\n",
    "    # 'diversified_financials',\n",
    "    # 'metals_and_mining',\n",
    "    # 'technology_hardware_and_equipment',\n",
    "    # 'utilities',\n",
    "    # 'chemicals',\n",
    "    # 'automobiles_and_components',\n",
    "    # 'semiconductors_and_semiconductor_equipment',\n",
    "    # 'media_and_entertainment',\n",
    "    # 'real_estate',\n",
    "    # 'consumer_services',\n",
    "    # 'consumer_durables_and_apparel',\n",
    "    # 'insurance',\n",
    "    # 'transportation',\n",
    "    # 'commercial_and_professional_services',\n",
    "    # 'paper_and_forest_products',\n",
    "    # 'containers_and_packaging',\n",
    "    # 'construction_materials'\n",
    "]\n",
    "\n",
    "l_reg = 3\n",
    "l_roll = 2\n",
    "dt = 10\n",
    "\n",
    "output_dir = \"data\"\n",
    "\n",
    "stonk_model = predict.XGBStonkModel()\n",
    "vix = utils.get_stonk_data(fname_prefix=\"vix\", disable_filter=True).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Industry (1/1): capital_goods\n",
      "Processing residuals...\n",
      "Done after: 33s\n",
      "807 trades selected out of 16836 by residual values\n",
      "Processing ADFs...\n",
      "Done after: 80s\n",
      "166 trades selected out of 807 by ADF pass rates\n",
      "Mean max residual value for capital_goods after filtering is 3.869999885559082\n",
      "Preparing data for model...\n",
      "Running model...\n",
      "Writing results to CSV...\n",
      "*** All done ***\n"
     ]
    }
   ],
   "source": [
    "datasets = []\n",
    "i = 1\n",
    "total_industries = len(industries)\n",
    "for industry in industries:\n",
    "    stonks = utils.get_stonk_data(filter_industries=[industry])\n",
    "    X, Y = processing.combine_stonk_pairs(stonks)\n",
    "\n",
    "    print(\"Industry ({0}/{1}): {2}\".format(i, total_industries, industry))\n",
    "\n",
    "    print(\"Processing residuals...\")\n",
    "    residuals, betas, _, date_index = utils.measure_time(\n",
    "        partial(\n",
    "            processing.get_rolling_residuals,\n",
    "            X=X,\n",
    "            Y=Y,\n",
    "            l_reg=l_reg,\n",
    "            l_roll=l_roll,\n",
    "            dt=dt,\n",
    "        )\n",
    "    )\n",
    "    residuals.insert(0, \"dates\", date_index)\n",
    "    betas.insert(0, \"dates\", date_index)\n",
    "\n",
    "    std_residuals, means, stds = processing.get_standardized_residuals(\n",
    "        residuals.drop(columns=\"dates\")\n",
    "    )\n",
    "\n",
    "    trades_before = len(std_residuals)\n",
    "    std_residuals = std_residuals[std_residuals.iloc[:, -1].abs() >= 2.5]\n",
    "    trades_after = len(std_residuals)\n",
    "    print(\n",
    "        \"{0} trades selected out of {1} by residual values\".format(\n",
    "            trades_after, trades_before\n",
    "        )\n",
    "    )\n",
    "    if trades_after == 0:\n",
    "        print(\"No trades left after filtering residuals, skipping this industry...\")\n",
    "        continue\n",
    "    residuals = residuals.loc[std_residuals.index]\n",
    "    betas = betas.loc[std_residuals.index]\n",
    "\n",
    "    print(\"Processing ADFs...\")\n",
    "    adfs, adfs_raw = utils.measure_time(\n",
    "        partial(\n",
    "            processing.get_aggregate_adfs,\n",
    "            residuals.drop(columns=\"dates\"),\n",
    "            betas=betas.drop(columns=\"dates\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    selected_by_adf = (adfs >= 0.5).values\n",
    "    adfs = adfs[selected_by_adf]\n",
    "\n",
    "    trades_before = len(std_residuals)\n",
    "    std_residuals = std_residuals[selected_by_adf]\n",
    "    trades_after = len(std_residuals)\n",
    "    print(\n",
    "        \"{0} trades selected out of {1} by ADF pass rates\".format(\n",
    "            trades_after, trades_before\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if len(std_residuals) == 0:\n",
    "        print(\n",
    "            \"No trades left after filtering ADF pass rates, skipping this industry...\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    betas = betas.loc[adfs.index]\n",
    "    residuals = residuals.loc[adfs.index]\n",
    "    adfs_raw = adfs_raw.loc[adfs.index]\n",
    "    means = means.loc[adfs.index]\n",
    "    stds = stds.loc[adfs.index]\n",
    "\n",
    "    residuals_max_mean = processing.get_mean_residual_magnitude(\n",
    "        std_residuals.to_numpy(), dt=21\n",
    "    )\n",
    "    print(\n",
    "        \"Mean max residual value for {0} after filtering is {1}\".format(\n",
    "            industry, residuals_max_mean\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"Preparing data for model...\")\n",
    "    dataset = utils.build_dataset_from_live_data_by_industry(\n",
    "        std_residuals.to_numpy(),\n",
    "        adfs.to_numpy().ravel(),\n",
    "        industry,\n",
    "        residuals_max_mean,\n",
    "        vix.loc[stonks.columns[-1]],\n",
    "    )\n",
    "\n",
    "    print(\"Running model...\")\n",
    "    predictions, df_processed = stonk_model.predict(dataset)\n",
    "    datasets.append((dataset, df_processed))\n",
    "    predictions = pd.DataFrame(predictions)\n",
    "    predictions.index = adfs.index\n",
    "\n",
    "    print(\"Writing results to CSV...\")\n",
    "    # Very big industry, exceeds Git file size limit\n",
    "    if industry == \"diversified_financials\":\n",
    "        half = len(residuals) // 2\n",
    "        residuals_fst = residuals.iloc[:half]\n",
    "        residuals_snd = residuals.iloc[half:]\n",
    "        residuals_fst.to_csv(\n",
    "            os.path.join(output_dir, industry + \"_one_residuals.csv\"),\n",
    "            header=False,\n",
    "            index=True,\n",
    "        )\n",
    "        residuals_snd.to_csv(\n",
    "            os.path.join(output_dir, industry + \"_two_residuals.csv\"),\n",
    "            header=False,\n",
    "            index=True,\n",
    "        )\n",
    "        del residuals_fst\n",
    "        del residuals_snd\n",
    "    else:\n",
    "        residuals.to_csv(\n",
    "            os.path.join(output_dir, industry + \"_residuals.csv\"),\n",
    "            header=False,\n",
    "            index=True,\n",
    "        )\n",
    "    betas.to_csv(\n",
    "        os.path.join(output_dir, industry + \"_betas.csv\"), header=False, index=True\n",
    "    )\n",
    "    adfs_raw.to_csv(\n",
    "        os.path.join(output_dir, industry + \"_adfs_raw.csv\"), header=False, index=True\n",
    "    )\n",
    "    predictions.to_csv(\n",
    "        os.path.join(output_dir, industry + \"_predictions.csv\"),\n",
    "        header=False,\n",
    "        index=True,\n",
    "    )\n",
    "    i += 1\n",
    "\n",
    "print(\"*** All done ***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stonks = utils.get_stonk_data()\n",
    "# stonks = stonks.loc[:, :\"2022-07-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data windows: 70\n",
      "Period 2017-04-28 to 2022-07-15\n",
      "Industries 1/26\n",
      "Industries 2/26\n",
      "Industries 3/26\n",
      "Industries 4/26\n",
      "Industries 5/26\n",
      "Industries 6/26\n",
      "Industries 7/26\n",
      "Industries 8/26\n"
     ]
    }
   ],
   "source": [
    "pipelines.data_collection_rolling_pipeline(\n",
    "    stonks,\n",
    "    l_reg=3,\n",
    "    l_roll=2,\n",
    "    dt=10,\n",
    "    market_cap_min_mm=1000,\n",
    "    market_cap_max_mm=None,\n",
    "    last_residual_cutoff=2.5,\n",
    "    mean_max_residual_dt=21,\n",
    "    adf_pval_cutoff=0.1,\n",
    "    adf_pass_rate_filter=0.5,\n",
    "    arima_forecast_months=3,\n",
    "    arima_eval_models=5,\n",
    "    trade_length_months=3,\n",
    "    trading_interval_weeks=2,\n",
    "    # first_n_windows=72,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = utils.ingest_trade_pipeline_outputs()\n",
    "\n",
    "vix = utils.get_stonk_data(fname_prefix=\"vix\", disable_filter=True).iloc[0]\n",
    "sp500 = utils.get_stonk_data(fname_prefix=\"sp500\", disable_filter=True).iloc[0]\n",
    "\n",
    "sp500_chg = pd.Series((sp500.iloc[63:].values / sp500.iloc[:-63].values) - 1)\n",
    "sp500_chg.index = sp500.iloc[63:].index\n",
    "\n",
    "dataset[\"vix\"] = dataset[\"trade_date\"].apply(lambda x: vix.loc[x])\n",
    "dataset[\"sp500\"] = dataset[\"trade_date\"].apply(lambda x: sp500_chg.loc[x])\n",
    "dataset.to_csv(\"data/dataset.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from hyperopt import STATUS_OK, STATUS_FAIL, Trials, fmin, hp, tpe, atpe, rand\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_production_xgb(\n",
    "    df: pd.DataFrame, params: Dict[str, Any], noise_level: float = 0\n",
    ") -> Tuple[xgb.XGBClassifier, sklearn.base.TransformerMixin]:\n",
    "    X_train, scalers = preprocessing.transform_features(df, noise_level=noise_level)\n",
    "    y_train = df[\"label\"]\n",
    "\n",
    "    clf = xgb.XGBClassifier(**params)\n",
    "\n",
    "    clf.fit(X_train, y_train, eval_set=[(X_train, y_train)])\n",
    "    clf.save_model(os.path.join(\"data\", \"xgb_classifier.json\"))\n",
    "\n",
    "    with open(os.path.join(\"data\", \"scalers.json\"), \"wb\") as fp:\n",
    "        pickle.dump(scalers, fp)\n",
    "\n",
    "    return clf, scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/dataset.csv\")\n",
    "df = df[df.beta > 0]\n",
    "df = df[df.last_residual.abs() >= 2.5]\n",
    "df = preprocessing.assign_labels(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_dates = 2\n",
    "selected_dates = np.sort(df[\"trade_date\"].unique())[drop_dates:]\n",
    "df_prod = df[df.trade_date.isin(selected_dates)].sample(frac=1)\n",
    "print(len(df_prod))\n",
    "print(df_prod[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_prod, scalers_prod = train_production_xgb(df_prod, params, noise_level=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = preprocessing.split_data(df, 2, 6, 2, random_state=3439)\n",
    "print(len(splits[\"train\"]))\n",
    "print(len(splits[\"validation\"]))\n",
    "print(splits[\"train\"][\"label\"].value_counts())\n",
    "print(splits[\"validation\"][\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_level = 0.005\n",
    "\n",
    "X_train, scalers = preprocessing.transform_features(\n",
    "    splits[\"train\"], noise_level=noise_level\n",
    ")\n",
    "X_valid, _ = preprocessing.transform_features(\n",
    "    splits[\"validation\"], scalers=scalers, noise_level=0\n",
    ")\n",
    "\n",
    "y_train = splits[\"train\"][\"label\"]\n",
    "y_valid = splits[\"validation\"][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_space = {\n",
    "    \"gamma\": hp.uniform(\"gamma\", 0, 5),\n",
    "    \"scale_pos_weight\": hp.uniform(\"scale_pos_weight\", 2, 12),\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 3, 10, 1),\n",
    "    \"min_child_weight\": hp.quniform(\"min_child_weight\", 1, 8, 1),\n",
    "    \"max_delta_step\": hp.quniform(\"max_delta_step\", 1, 4, 1),\n",
    "    \"n_estimators\": hp.quniform(\"n_estimators\", 50, 200, 1),\n",
    "    # \"n_estimators\": hp.choice(\"n_estimators\", np.array([50, 75, 100, 150, 200])),\n",
    "    # \"subsample\": hp.uniform(\"subsample\", 0.5, 1),\n",
    "    # \"colsample_bylevel\" : hp.uniform(\"colsample_bylevel\", 0.5, 1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def optimization_objective(space):\n",
    "    clf = xgb.XGBClassifier(\n",
    "        gamma=space[\"gamma\"],\n",
    "        scale_pos_weight=space[\"scale_pos_weight\"],\n",
    "        #\n",
    "        max_depth=int(space[\"max_depth\"]),\n",
    "        min_child_weight=int(space[\"min_child_weight\"]),\n",
    "        max_delta_step=int(space[\"max_delta_step\"]),\n",
    "        #\n",
    "        # colsample_bylevel = space['colsample_bylevel'],\n",
    "        colsample_bylevel=1,\n",
    "        n_estimators=int(space[\"n_estimators\"]),\n",
    "        learning_rate=0.1,\n",
    "        # subsample = space['subsample'],\n",
    "        subsample=1,\n",
    "        #\n",
    "        tree_method=\"hist\",\n",
    "        enable_categorical=True,\n",
    "        max_cat_to_onehot=1,\n",
    "        random_state=np.random.randint(9999999),\n",
    "    )\n",
    "\n",
    "    clf.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    y_score = clf.predict_proba(X_valid)[:, 1]\n",
    "    y_preds = y_score > 0.5\n",
    "\n",
    "    f1 = f1_score(y_valid, y_preds, zero_division=0)\n",
    "    precision = precision_score(y_valid, y_preds, zero_division=0)\n",
    "    ap = evaluate.average_precision_from_cutoff(y_valid, y_score, 0.4)\n",
    "    roc = roc_auc_score(y_valid, y_score)\n",
    "\n",
    "    pos_preds = int(y_preds.sum())\n",
    "    pos_labels = int(y_valid.sum())\n",
    "\n",
    "    ap = ap if pos_preds >= pos_labels else 0\n",
    "\n",
    "    if f1 == 0 or precision == 0:\n",
    "        return {\n",
    "            \"loss\": 100,\n",
    "            \"precision\": precision,\n",
    "            \"f1_score\": f1,\n",
    "            \"ap\": ap,\n",
    "            \"auc\": roc,\n",
    "            \"pos_preds\": pos_preds,\n",
    "            \"status\": STATUS_FAIL,\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"loss\": -ap,\n",
    "            \"precision\": precision,\n",
    "            \"f1_score\": f1,\n",
    "            \"ap\": ap,\n",
    "            \"auc\": roc,\n",
    "            \"pos_preds\": pos_preds,\n",
    "            \"status\": STATUS_OK,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "\n",
    "best_hyperparams = fmin(\n",
    "    fn=optimization_objective,\n",
    "    space=hyperparameter_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=1000,\n",
    "    trials=trials,\n",
    ")\n",
    "\n",
    "trial_vals = trials.vals\n",
    "trial_vals[\"f1_score\"] = list(map(lambda x: x[\"f1_score\"], trials.results))\n",
    "trial_vals[\"precision\"] = list(map(lambda x: x[\"precision\"], trials.results))\n",
    "trial_vals[\"ap\"] = list(map(lambda x: x[\"ap\"], trials.results))\n",
    "trial_vals[\"auc\"] = list(map(lambda x: x[\"auc\"], trials.results))\n",
    "trial_vals[\"pos_preds\"] = list(map(lambda x: x[\"pos_preds\"], trials.results))\n",
    "\n",
    "df_trials = pd.DataFrame.from_dict(trial_vals)\n",
    "df_trials.to_csv(\"data/data-window-size-2#7.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    # reg def 0\n",
    "    \"gamma\": 3.301387,\n",
    "    # L2 def 1\n",
    "    # \"reg_lambda\" : 1,\n",
    "    # \"reg_alpha\" : 0,\n",
    "    # Class imbalance def 1\n",
    "    \"scale_pos_weight\": 5.568589,\n",
    "    # Integers:\n",
    "    \"max_depth\": 6,\n",
    "    # Reg def 1\n",
    "    \"min_child_weight\": 7,\n",
    "    # Class imbalance def 0\n",
    "    \"max_delta_step\": 3,\n",
    "    # Choice:\n",
    "    \"colsample_bylevel\": 1,\n",
    "    \"n_estimators\": 63,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"subsample\": 1,\n",
    "    # Fixed:\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"enable_categorical\": True,\n",
    "    \"max_cat_to_onehot\": 1,\n",
    "    \"eval_metric\": [\"logloss\"],\n",
    "    \"random_state\": np.random.randint(999929),\n",
    "}\n",
    "\n",
    "clf = xgb.XGBClassifier(**params)\n",
    "\n",
    "clf.fit(X_train, y_train, eval_set=[(X_valid, y_valid), (X_train, y_train)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"**Validation**\")\n",
    "y_score = clf.predict_proba(X_valid)[:, 1]\n",
    "thres = 0.7\n",
    "y_preds = y_score > thres\n",
    "\n",
    "evaluate.performance_summary(y_score, y_preds, y_valid, auc_cutoff=0.4)\n",
    "\n",
    "df_results_valid = evaluate.returns_on_predictions(splits[\"validation\"], y_preds)\n",
    "\n",
    "evaluate.performance_on_slice(\n",
    "    splits[\"validation\"], y_score, y_preds, \"subindustry\", False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results_valid[df_results_valid.result == 'TP'].iloc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results_valid[df_results_valid.subindustry == 'consumer_services'].iloc[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.feature_names_in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trials = pd.read_csv(\"data/last-residual-cutoff-check-2.5#13.csv\")\n",
    "df_trials.sort_values(\"ap\", ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trials = pd.read_csv(\"data/data-window-size-2#7.csv\")\n",
    "df_trials.sort_values(\"ap\", ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trials = pd.read_csv(\"data/data-window-size-2#6.csv\")\n",
    "df_trials.sort_values(\"ap\", ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
