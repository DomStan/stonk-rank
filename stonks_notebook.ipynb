{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data management\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data fetching\n",
    "import yfinance as yf\n",
    "\n",
    "# Spread generation\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Backtesting\n",
    "\n",
    "# ML\n",
    "\n",
    "# Utils\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import numba\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stonk_data(stonk_list, period_years=3, date_from=None, date_to=datetime.now(), interval='1d', source='yfinance', data_dir='data', file_prefix='stonks', proxy=False):    \n",
    "    '''\n",
    "    Returns historical price data for the selected stonks.\n",
    "\n",
    "    -Args:\n",
    "        stonk_list (string, list): List of stonk identifiers as strings, case unsensitive\n",
    "        period_years (float): How many years of data to download until date_to, can be a floating point number\n",
    "    -Optional:\n",
    "        date_from (datetime): Start date for stonk data (use instead of period_years)\n",
    "        date_to (datetime): End date for stonk data\n",
    "        interval (string): Valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
    "        source (string): Where to source data from. Valid sources: yfinance\n",
    "        data_dir (string): Folder name where to output downloaded data\n",
    "        file_prefix (string): Prefix of CSV file containing downloaded data inside data_dir\n",
    "        proxy (boolean): Whether to use a proxy connection to avoid API limits/blocks\n",
    "                \n",
    "    -Returns:\n",
    "        stonks (Pandas Dataframe): Pandas Dataframe containing requested ticker prices\n",
    "    '''\n",
    "    \n",
    "    if date_from is None:\n",
    "        date_from = date_to-(timedelta(days=int(365*period_years)))\n",
    "        \n",
    "    if source.lower() == 'yfinance':\n",
    "        stonks = yf.download(list(stonk_list), start=date_from, end=date_to, interval=interval, group_by='column', threads=True, rounding=True)['Adj Close']\n",
    "        stonks.dropna(axis=0, how='all', inplace=True)\n",
    "    else:\n",
    "        raise ValueError('Unsupported data source')\n",
    "        \n",
    "    from_date_string = stonks.index[0].strftime('%Y-%m-%d')\n",
    "    to_date_string = stonks.index[-1].strftime('%Y-%m-%d')\n",
    "    \n",
    "    filename = '{prefix}_{from_date}_to_{to_date}.csv'.format(prefix=file_prefix, from_date=from_date_string, to_date=to_date_string)\n",
    "    file_path = os.path.join(data_dir, filename)\n",
    "    \n",
    "    stonks.to_csv(path_or_buf=file_path, header=True, index=True, na_rep='NaN')\n",
    "    \n",
    "    return stonks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stonks = get_stonk_data([\"googl\", \"tsla\", \"ffs\"], period_years=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "stonk_list = pd.read_csv('data/stonk_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  2283 of 2283 completed\n",
      "\n",
      "12 Failed downloads:\n",
      "- WFC PRN: No data found, symbol may be delisted\n",
      "- SNX.VI: No data found, symbol may be delisted\n",
      "- ET-PE: No data found for this date range, symbol may be delisted\n",
      "- FTAI-PA: No data found for this date range, symbol may be delisted\n",
      "- ET-PD: No data found for this date range, symbol may be delisted\n",
      "- AZEK: Error occurred while retrieving timeseries from Redis, keys: [RedisKey [key=AZEK, cluster=finance]]\n",
      "- ALL-PB: No data found for this date range, symbol may be delisted\n",
      "- FHN PRA: No data found, symbol may be delisted\n",
      "- ET-PC: No data found for this date range, symbol may be delisted\n",
      "- WCC-PA: No data found for this date range, symbol may be delisted\n",
      "- RXN.VI: No data found, symbol may be delisted\n",
      "- NRZ-PD: No data found for this date range, symbol may be delisted\n"
     ]
    }
   ],
   "source": [
    "df = get_stonk_data(stonk_list['ticker'], period_years=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(yf.download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(yf.Ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock list preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_stock_list(raw_data_path='data/raw_stock_list.xls', output_path='data/stonk_list.csv'):\n",
    "    '''\n",
    "    Parses a raw excel file from CapitalIQ containing ticker names and their subindustries, validates\n",
    "    unusual ticker names with Yahoo Finance, saving the processed data in CSV format.\n",
    "\n",
    "        Parameters:\n",
    "            Required:\n",
    "                raw_data_path (string):\n",
    "                    Path to the raw excel file.\n",
    "                output_path (string):\n",
    "                    Path where to save the parsed data.\n",
    "                \n",
    "        Returns:\n",
    "            Nothing\n",
    "    '''\n",
    "    \n",
    "    df = pd.read_excel(io=raw_data_path)\n",
    "    \n",
    "    # Drop NA rows\n",
    "    df.dropna(axis=0, inplace=True)\n",
    "    \n",
    "    # Reset index and drop the first row\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    df.drop(index=0, axis=0, inplace=True)\n",
    "    \n",
    "    # Drop unwanted columns\n",
    "    df.drop(columns=df.columns[[1, 2, 3, 4, 5, 7, 8, 9]], inplace=True)\n",
    "    \n",
    "    # Rename remaining columns\n",
    "    df.columns = ['ticker', 'subindustry']\n",
    "    \n",
    "    # Remove the '(Primary)' tag from subindustries\n",
    "    df['subindustry'] = df['subindustry'].str.replace(r' \\(Primary\\)', '')\n",
    "    \n",
    "    # Remove everything until (and including) the semicolon for tickers\n",
    "    df['ticker'] = df['ticker'].str.replace(r'(.*:)', '')\n",
    "    \n",
    "    df['ticker'] = df['ticker'].str.replace(r' WI', '.VI')\n",
    "    df['ticker'] = df['ticker'].str.replace(r'\\.WI', '.VI')\n",
    "    \n",
    "    # Replace the ticker endings for a Yahoo finance supported format\n",
    "    df['ticker'] = df['ticker'].str.replace(r'\\.PR', '-P')\n",
    "    \n",
    "#     # Drop tickers with two letters after a dot, unavailable in Yahoo finance\n",
    "#     df = df[~df['ticker'].str.fullmatch(r'[A-Z]*\\.[A-Z]{2}')]\n",
    "    \n",
    "    # Take all remaining tickers that have a dot\n",
    "    dotted = df[df['ticker'].str.fullmatch(r'[A-Z]*\\.[A-Z]')]\n",
    "    \n",
    "    # Replace the dots with dashes\n",
    "    dashed = dotted.copy()\n",
    "    dashed['ticker'] = dashed['ticker'].str.replace(r'\\.', '-')\n",
    "    \n",
    "    # Remove the dots\n",
    "    undotted = dotted.copy()\n",
    "    undotted['ticker'] = undotted['ticker'].str.replace(r'\\.', '')\n",
    "\n",
    "    # Combine all variantas together\n",
    "    all_variants = pd.concat([dotted, dashed, undotted])\n",
    "    \n",
    "    # Run all of these through Yahoo finance, get last day's price\n",
    "    stonks = yf.download(list(all_variants['ticker'].astype('string').values), period='1m', interval='1d', group_by='column')\n",
    "    \n",
    "    # Drop all NA tickers (that failed to download)\n",
    "    valid_tickers = stonks['Adj Close'].iloc[-1].dropna(axis=0).to_frame().reset_index()\n",
    "    \n",
    "    # Rename columns\n",
    "    valid_tickers.columns = ['ticker', 'price']\n",
    "    \n",
    "    # Add subindustries to the remaining valid tickers\n",
    "    valid_tickers = valid_tickers.join(all_variants.set_index('ticker'), on='ticker')\n",
    "    \n",
    "    # Drop the price column\n",
    "    valid_tickers.drop(columns=valid_tickers.columns[[1]], inplace=True)\n",
    "    \n",
    "    # Remove all tickers that have a dot from main dataframe\n",
    "    df = df[~df['ticker'].str.fullmatch(r'[A-Z]*\\.[A-Z]')]\n",
    "    \n",
    "    # Add the validated tickers back\n",
    "    df = pd.concat([df, valid_tickers], axis=0, ignore_index=True)\n",
    "    \n",
    "    # Make the subindustry strings more code friendly\n",
    "    df['subindustry'] = df['subindustry'].str.replace(' ', '_')\n",
    "    df['subindustry'] = df['subindustry'].str.lower()\n",
    "    df['subindustry'] = df['subindustry'].str.replace(',', '')\n",
    "    \n",
    "    df.to_csv(path_or_buf=output_path, header=True, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_residuals(X, Y):\n",
    "    '''\n",
    "    Vectorized calculation of residuals from multiple univariate linear regressions.\n",
    "\n",
    "        Args:\n",
    "        - X (numpy array of shape (n_pairs, d_time)): X variable for LR\n",
    "        - Y (numpy array of shape (n_pairs, d_time)): Y variable for LR           \n",
    "        Returns:\n",
    "        - residuals (numpy array of shape (n_pairs, d_time)): matrix of resulting residuals between vectorized pairs of X and Y\n",
    "        - Y_hat (numpy array of shape (n_pairs, d_time)): predictions of Y using X\n",
    "    '''\n",
    "    # Stack 2D matrices into 3D matrices\n",
    "    X = X.reshape(np.shape(X)[0], np.shape(X)[1], -1)\n",
    "    Y = Y.reshape(np.shape(Y)[0], np.shape(Y)[1], -1)\n",
    "    \n",
    "    # Add bias\n",
    "    Z = np.concatenate([X, np.ones((np.shape(X)[0], np.shape(X)[1], 1))], axis=2)\n",
    "    \n",
    "    Z_t = Z.transpose(0, 2, 1)\n",
    "    W = np.matmul(np.linalg.inv(np.matmul(Z_t, Z)),  np.matmul(Z_t, Y))\n",
    "    \n",
    "    # Predictions and residuals\n",
    "    Y_hat = np.matmul(Z, W).round(2)\n",
    "    residuals = (Y - Y_hat)\n",
    "    \n",
    "    return (residuals[:, :, 0], Y_hat[:, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slow_residuals(X, Y, n_jobs=None):\n",
    "    lr = LinearRegression(n_jobs=n_jobs, fit_intercept=True)\n",
    "    X = X.reshape((X.shape[0], X.shape[1], -1))\n",
    "    Y = Y.reshape((Y.shape[0], Y.shape[1], -1))\n",
    "    \n",
    "    preds = []\n",
    "    res = []\n",
    "    for i in range(X.shape[0]):\n",
    "        lr.fit(X[i], Y[i])\n",
    "        preds.append(lr.predict(X[i]).round(2))\n",
    "        res.append(Y[i]-preds[-1])\n",
    "    return (np.asarray(res)[:,:,0], np.asarray(preds)[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1815, 757)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stonk_data(date_from, date_to, data_dir='data', data_prefix='stonks'):\n",
    "    path = os.path.join(data_dir, '{}_{}_to_{}.csv'.format(data_prefix, date_from, date_to))\n",
    "    stonks = pd.read_csv(path, index_col=0)\n",
    "    stonks.dropna(axis=1, how='all', thresh=len(stonks) * 0.95, inplace=True)\n",
    "    stonks.dropna(axis=0, how='all', thresh=len(stonks) * 0.95, inplace=True)\n",
    "    stonks.fillna(axis=1, method='ffill', inplace=True)\n",
    "    return stonks.to_numpy().T.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "stonks = read_stonk_data('2019-02-25', '2022-02-24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = stonks\n",
    "Y = np.flipud(stonks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time slow: 1.8449993133544922\n",
      "Time fast: 0.2609989643096924\n"
     ]
    }
   ],
   "source": [
    "t1_fast = time.time()\n",
    "res, preds = get_residuals(X, Y)\n",
    "t2_fast = time.time()\n",
    "\n",
    "t1_slow = time.time()\n",
    "res_slow, preds_slow = get_slow_residuals(X, Y)\n",
    "t2_slow = time.time()\n",
    "\n",
    "print(\"Time slow: \" + str(t2_slow-t1_slow))\n",
    "print(\"Time fast: \" + str(t2_fast-t1_fast))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(np.equal(res, res_slow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: check which inputs/outputs from X, Y generate mismatching results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
