{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "\n",
    "import typing\n",
    "from typing import Dict\n",
    "from typing import Any\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "import utils\n",
    "import pipelines\n",
    "import processing\n",
    "import evaluate\n",
    "import predict\n",
    "import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download stock daily prices & VIX index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gets all ticker names (no argument given)\n",
    "ticker_list = utils.get_ticker_names(market_cap_min_mm=1000, market_cap_max_mm=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Specific date - 3rd of March 2022 (Y, M, D)\n",
    "# date_to = datetime(2021, 1, 18)\n",
    "### Date of today\n",
    "date_to = datetime.today()\n",
    "### How many years' of data to download (going backwards from date_end). Year can be a floating point number\n",
    "period_years = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  2820 of 2820 completed\n",
      "\n",
      "13 Failed downloads:\n",
      "- MRK.WI: No data found, symbol may be delisted\n",
      "- FOE: No data found, symbol may be delisted\n",
      "- PFE.WI: No data found, symbol may be delisted\n",
      "- MGP: No data found, symbol may be delisted\n",
      "- BIP.PRB: No data found, symbol may be delisted\n",
      "- POST WI: No data found, symbol may be delisted\n",
      "- RXN WI: No data found, symbol may be delisted\n",
      "- BIP.PRA: No data found, symbol may be delisted\n",
      "- SGMS: No data found, symbol may be delisted\n",
      "- SNX.WI: No data found, symbol may be delisted\n",
      "- O.WI: No data found, symbol may be delisted\n",
      "- T WD: No data found, symbol may be delisted\n",
      "- DELL WI: No data found, symbol may be delisted\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "df, df_clean = utils.download_stonk_prices(ticker_list.index, period_years=period_years, date_to=date_to)\n",
    "vix, vix_clean = utils.download_stonk_prices([\"^VIX\"], period_years=period_years, date_to=date_to, fname_prefix=\"vix\")\n",
    "sp500, sp500_clean = utils.download_stonk_prices([\"^GSPC\"], period_years=period_years, date_to=date_to, fname_prefix=\"sp500\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run data pipeline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# For convenience to copy down below\n",
    "industries = [\n",
    "    'health_care_equipment_and_services',\n",
    "    'software_and_services',\n",
    "    'retailing',\n",
    "    'telecommunication_services',\n",
    "    'capital_goods',\n",
    "    'energy',\n",
    "    'pharmaceuticals_biotechnology_and_life_sciences',\n",
    "    'consumer_staples',\n",
    "    'banks',\n",
    "    'diversified_financials',\n",
    "    'metals_and_mining',\n",
    "    'technology_hardware_and_equipment',\n",
    "    'utilities',\n",
    "    'chemicals',\n",
    "    'automobiles_and_components',\n",
    "    'semiconductors_and_semiconductor_equipment',\n",
    "    'media_and_entertainment',\n",
    "    'real_estate',\n",
    "    'consumer_services',\n",
    "    'consumer_durables_and_apparel',\n",
    "    'insurance',\n",
    "    'transportation',\n",
    "    'commercial_and_professional_services',\n",
    "    'paper_and_forest_products',\n",
    "    'containers_and_packaging',\n",
    "    'construction_materials'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "### All industries:\n",
    "# industries = ticker_list['subindustry'].unique()\n",
    "### Custom list of industries:\n",
    "industries = [\n",
    "    # 'health_care_equipment_and_services',\n",
    "    # 'software_and_services',\n",
    "    # 'retailing',\n",
    "    # 'telecommunication_services',\n",
    "    # 'capital_goods',\n",
    "    # 'energy',\n",
    "    # 'pharmaceuticals_biotechnology_and_life_sciences',\n",
    "    # 'consumer_staples',\n",
    "    # 'banks',\n",
    "    'diversified_financials',\n",
    "    # 'metals_and_mining',\n",
    "    # 'technology_hardware_and_equipment',\n",
    "    # 'utilities',\n",
    "    # 'chemicals',\n",
    "    # 'automobiles_and_components',\n",
    "    # 'semiconductors_and_semiconductor_equipment',\n",
    "    # 'media_and_entertainment',\n",
    "    # 'real_estate',\n",
    "    # 'consumer_services',\n",
    "    # 'consumer_durables_and_apparel',\n",
    "    # 'insurance',\n",
    "    # 'transportation',\n",
    "    # 'commercial_and_professional_services',\n",
    "    # 'paper_and_forest_products',\n",
    "    # 'containers_and_packaging',\n",
    "    # 'construction_materials'\n",
    "    ]\n",
    "\n",
    "l_reg = 3\n",
    "l_roll = 2\n",
    "dt = 10\n",
    "\n",
    "output_dir = 'data'\n",
    "\n",
    "stonk_model = predict.XGBStonkModel()\n",
    "vix = utils.get_stonk_data(fname_prefix='vix', disable_filter=True).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Industry (1/1): diversified_financials\n",
      "Processing residuals...\n",
      "Done after: 59s\n",
      "5420 trades selected out of 30876 by residual values\n",
      "Processing ADFs...\n",
      "Done after: 420s\n",
      "506 trades selected out of 5420 by ADF pass rates\n",
      "Mean max residual value for diversified_financials after filtering is 4.170000076293945\n",
      "Preparing data for model...\n",
      "Running model...\n",
      "Writing results to CSV...\n",
      "*** All done ***\n"
     ]
    }
   ],
   "source": [
    "datasets = []\n",
    "i = 1\n",
    "total_industries = len(industries)\n",
    "for industry in industries:\n",
    "    stonks = utils.get_stonk_data(filter_industries=[industry])\n",
    "    X, Y = processing.combine_stonk_pairs(stonks)\n",
    "    \n",
    "    print('Industry ({0}/{1}): {2}'.format(i, total_industries, industry))\n",
    "    \n",
    "    print('Processing residuals...')\n",
    "    residuals, betas, _, date_index = utils.measure_time(partial(processing.get_rolling_residuals, X=X, Y=Y, l_reg=l_reg, l_roll=l_roll, dt=dt))\n",
    "    residuals.insert(0, \"dates\", date_index)\n",
    "    betas.insert(0, \"dates\", date_index)\n",
    "    \n",
    "    std_residuals, _, _ = processing.get_standardized_residuals(residuals.drop(columns=\"dates\"))\n",
    "\n",
    "    trades_before = len(std_residuals)\n",
    "    std_residuals = std_residuals[std_residuals.iloc[:, -1].abs() >= 2]\n",
    "    trades_after = len(std_residuals)\n",
    "    print('{0} trades selected out of {1} by residual values'.format(trades_after, trades_before))\n",
    "    if trades_after == 0:\n",
    "        print('No trades left after filtering residuals, skipping this industry...')\n",
    "        continue\n",
    "    residuals = residuals.loc[std_residuals.index]\n",
    "    betas = betas.loc[std_residuals.index]\n",
    "    \n",
    "    print('Processing ADFs...')\n",
    "    adfs, adfs_raw = utils.measure_time(partial(processing.get_aggregate_adfs, residuals.drop(columns=\"dates\"), betas=betas.drop(columns=\"dates\")))\n",
    "    \n",
    "    selected_by_adf = (adfs >= 0.5).values\n",
    "    adfs = adfs[selected_by_adf]\n",
    "    \n",
    "    trades_before = len(std_residuals)\n",
    "    std_residuals = std_residuals[selected_by_adf]\n",
    "    trades_after = len(std_residuals)\n",
    "    print('{0} trades selected out of {1} by ADF pass rates'.format(trades_after, trades_before))\n",
    "    \n",
    "    if len(std_residuals) == 0:\n",
    "        print('No trades left after filtering ADF pass rates, skipping this industry...')\n",
    "        continue\n",
    "\n",
    "    betas = betas.loc[adfs.index]\n",
    "    residuals = residuals.loc[adfs.index]\n",
    "    adfs_raw = adfs_raw.loc[adfs.index]\n",
    "    \n",
    "    residuals_max_mean = processing.get_mean_residual_magnitude(std_residuals.to_numpy(), dt=21)\n",
    "    print('Mean max residual value for {0} after filtering is {1}'.format(industry, residuals_max_mean))\n",
    "    \n",
    "    print('Preparing data for model...')\n",
    "    dataset = utils.build_dataset_from_live_data_by_industry(std_residuals.to_numpy(), adfs.to_numpy().ravel(), industry, residuals_max_mean, vix.loc[stonks.columns[-1]])\n",
    "    \n",
    "    print('Running model...')\n",
    "    predictions, df_processed = stonk_model.predict(dataset)\n",
    "    datasets.append((dataset, df_processed))\n",
    "    predictions = pd.DataFrame(predictions)\n",
    "    predictions.index = adfs.index\n",
    "    \n",
    "    print('Writing results to CSV...')\n",
    "    # Very big industry, exceeds Git file size limit\n",
    "    if industry == \"diversified_financials\":\n",
    "        half = len(residuals) // 2\n",
    "        residuals_fst = residuals.iloc[:half]\n",
    "        residuals_snd = residuals.iloc[half:]\n",
    "        residuals_fst.to_csv(os.path.join(output_dir, industry + '_one_residuals.csv'), header=False, index=True)\n",
    "        residuals_snd.to_csv(os.path.join(output_dir, industry + '_two_residuals.csv'), header=False, index=True)\n",
    "        del residuals_fst\n",
    "        del residuals_snd\n",
    "    else:\n",
    "        residuals.to_csv(os.path.join(output_dir, industry + '_residuals.csv'), header=False, index=True)\n",
    "    betas.to_csv(os.path.join(output_dir, industry + '_betas.csv'), header=False, index=True)\n",
    "    adfs_raw.to_csv(os.path.join(output_dir, industry + '_adfs_raw.csv'), header=False, index=True)\n",
    "    predictions.to_csv(os.path.join(output_dir, industry + '_predictions.csv'), header=False, index=True)\n",
    "    i+= 1\n",
    "    \n",
    "print('*** All done ***')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets[-23][0].head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stonks = utils.get_stonk_data()\n",
    "stonks = stonks.loc[:, :'2022-07-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_list = utils.get_ticker_names(market_cap_min_mm=1000, market_cap_max_mm=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines.data_collection_rolling_pipeline(stonks, first_n_windows=1, industries=list(ticker_list['subindustry'].unique()), l_reg=3, l_roll=2, dt=10, market_cap_min_mm=1000, market_cap_max_mm=None, adf_pval_cutoff=0.1, adf_pass_rate_filter=0.5, trade_length_months=3, trading_interval_weeks=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = utils.ingest_trade_pipeline_outputs()\n",
    "\n",
    "vix = utils.get_stonk_data(fname_prefix='vix', disable_filter=True).iloc[0]\n",
    "sp500 = utils.get_stonk_data(fname_prefix='sp500', disable_filter=True).iloc[0]\n",
    "\n",
    "sp500_chg = pd.Series((sp500.iloc[63:].values / sp500.iloc[:-63].values) - 1)\n",
    "sp500_chg.index = sp500.iloc[63:].index\n",
    "\n",
    "dataset['vix'] = dataset['trade_date'].apply(lambda x: vix.loc[x])\n",
    "dataset['sp500'] = dataset['trade_date'].apply(lambda x: sp500_chg.loc[x])\n",
    "dataset.to_csv('data/dataset.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from hyperopt import STATUS_OK, STATUS_FAIL, Trials, fmin, hp, tpe\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_production_xgb(df: pd.DataFrame, params: Dict[str, Any], noise_level: float = 0) -> Tuple[xgb.XGBClassifier, sklearn.base.TransformerMixin]:\n",
    "    X_train, scalers = preprocessing.transform_features(df, noise_level=noise_level)\n",
    "    y_train = df['label']\n",
    "    \n",
    "    clf = xgb.XGBClassifier(\n",
    "        **params\n",
    "    )\n",
    "\n",
    "    clf.fit(X_train, y_train, eval_set=[(X_train, y_train)])\n",
    "    clf.save_model(os.path.join('data', 'xgb_classifier.json'))\n",
    "                   \n",
    "    with open(os.path.join('data', 'scalers.json'), 'wb') as fp:\n",
    "        pickle.dump(scalers, fp)\n",
    "                   \n",
    "    return clf, scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/dataset.csv')\n",
    "df = df[df.beta > 0]\n",
    "df = preprocessing.assign_labels(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178269\n",
      "0    157307\n",
      "1     20962\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "drop_dates = 2\n",
    "selected_dates = np.sort(df['trade_date'].unique())[drop_dates:]\n",
    "df_prod = df[df.trade_date.isin(selected_dates)].sample(frac=1)\n",
    "print(len(df_prod))\n",
    "print(df_prod['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.67140\n",
      "[1]\tvalidation_0-logloss:0.65293\n",
      "[2]\tvalidation_0-logloss:0.63693\n",
      "[3]\tvalidation_0-logloss:0.62322\n",
      "[4]\tvalidation_0-logloss:0.61151\n",
      "[5]\tvalidation_0-logloss:0.60124\n",
      "[6]\tvalidation_0-logloss:0.59231\n",
      "[7]\tvalidation_0-logloss:0.58433\n",
      "[8]\tvalidation_0-logloss:0.57747\n",
      "[9]\tvalidation_0-logloss:0.57135\n",
      "[10]\tvalidation_0-logloss:0.56582\n",
      "[11]\tvalidation_0-logloss:0.56105\n",
      "[12]\tvalidation_0-logloss:0.55677\n",
      "[13]\tvalidation_0-logloss:0.55298\n",
      "[14]\tvalidation_0-logloss:0.54943\n",
      "[15]\tvalidation_0-logloss:0.54620\n",
      "[16]\tvalidation_0-logloss:0.54340\n",
      "[17]\tvalidation_0-logloss:0.54101\n",
      "[18]\tvalidation_0-logloss:0.53880\n",
      "[19]\tvalidation_0-logloss:0.53681\n",
      "[20]\tvalidation_0-logloss:0.53498\n",
      "[21]\tvalidation_0-logloss:0.53299\n",
      "[22]\tvalidation_0-logloss:0.53149\n",
      "[23]\tvalidation_0-logloss:0.52969\n",
      "[24]\tvalidation_0-logloss:0.52845\n",
      "[25]\tvalidation_0-logloss:0.52712\n",
      "[26]\tvalidation_0-logloss:0.52608\n",
      "[27]\tvalidation_0-logloss:0.52505\n",
      "[28]\tvalidation_0-logloss:0.52418\n",
      "[29]\tvalidation_0-logloss:0.52328\n",
      "[30]\tvalidation_0-logloss:0.52251\n",
      "[31]\tvalidation_0-logloss:0.52168\n",
      "[32]\tvalidation_0-logloss:0.52108\n",
      "[33]\tvalidation_0-logloss:0.52033\n",
      "[34]\tvalidation_0-logloss:0.51971\n",
      "[35]\tvalidation_0-logloss:0.51921\n",
      "[36]\tvalidation_0-logloss:0.51858\n",
      "[37]\tvalidation_0-logloss:0.51811\n",
      "[38]\tvalidation_0-logloss:0.51766\n",
      "[39]\tvalidation_0-logloss:0.51725\n",
      "[40]\tvalidation_0-logloss:0.51696\n",
      "[41]\tvalidation_0-logloss:0.51636\n",
      "[42]\tvalidation_0-logloss:0.51592\n",
      "[43]\tvalidation_0-logloss:0.51548\n",
      "[44]\tvalidation_0-logloss:0.51520\n",
      "[45]\tvalidation_0-logloss:0.51489\n",
      "[46]\tvalidation_0-logloss:0.51461\n",
      "[47]\tvalidation_0-logloss:0.51436\n",
      "[48]\tvalidation_0-logloss:0.51400\n",
      "[49]\tvalidation_0-logloss:0.51354\n"
     ]
    }
   ],
   "source": [
    "clf_prod, scalers_prod = train_production_xgb(df_prod, params, noise_level=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154428\n",
      "6719\n",
      "0    136722\n",
      "1     17706\n",
      "Name: label, dtype: int64\n",
      "0    5802\n",
      "1     917\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "splits = preprocessing.split_data(df, 2, 6, 2, random_state=9999)\n",
    "print(len(splits['train']))\n",
    "print(len(splits['validation']))\n",
    "print(splits['train']['label'].value_counts())\n",
    "print(splits['validation']['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_level = 0.005\n",
    "\n",
    "X_train, scalers = preprocessing.transform_features(splits['train'], noise_level=noise_level)\n",
    "X_valid, _ = preprocessing.transform_features(splits['validation'], scalers=scalers, noise_level=0)\n",
    "\n",
    "y_train = splits['train']['label']\n",
    "y_valid = splits['validation']['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_space = {\n",
    "    \"gamma\": hp.uniform(\"gamma\", 0, 5),\n",
    "    \"scale_pos_weight\" : hp.uniform(\"scale_pos_weight\", 2, 12),\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 3, 10, 1),\n",
    "    \"min_child_weight\" : hp.quniform(\"min_child_weight\", 1, 8, 1),\n",
    "    \"max_delta_step\" : hp.quniform(\"max_delta_step\", 1, 4, 1),\n",
    "    \"n_estimators\": hp.choice(\"n_estimators\", np.array([50, 100, 150, 200])),\n",
    "    # \"subsample\": hp.uniform(\"subsample\", 0.5, 1),\n",
    "    # \"colsample_bylevel\" : hp.uniform(\"colsample_bylevel\", 0.5, 1),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def optimization_objective(space):\n",
    "    clf = xgb.XGBClassifier(\n",
    "        gamma = space['gamma'],\n",
    "        scale_pos_weight = space['scale_pos_weight'],\n",
    "        #\n",
    "        max_depth = int(space['max_depth']),\n",
    "        min_child_weight = int(space['min_child_weight']),\n",
    "        max_delta_step = int(space['max_delta_step']),\n",
    "        #\n",
    "        # colsample_bylevel = space['colsample_bylevel'],\n",
    "        colsample_bylevel = 1,\n",
    "        n_estimators = int(space['n_estimators']), \n",
    "        learning_rate = 0.1,\n",
    "        # subsample = space['subsample'],\n",
    "        subsample = 1,\n",
    "        #\n",
    "        tree_method = \"hist\",\n",
    "        enable_categorical = True,\n",
    "        max_cat_to_onehot = 1,\n",
    "        random_state = np.random.randint(9999999),\n",
    "    )\n",
    "    \n",
    "    clf.fit(\n",
    "        X_train, y_train,\n",
    "        verbose=False,\n",
    "    )\n",
    "    \n",
    "    y_score = clf.predict_proba(X_valid)[:, 1]\n",
    "    y_preds = y_score > 0.5\n",
    "    \n",
    "    f1 = f1_score(y_valid, y_preds, zero_division=0)\n",
    "    precision = precision_score(y_valid, y_preds, zero_division=0)\n",
    "    ap = evaluate.average_precision_from_cutoff(y_valid, y_score, 0.55)\n",
    "    roc = roc_auc_score(y_valid, y_score)\n",
    "    \n",
    "    pos_preds = int(y_preds.sum())\n",
    "    pos_labels = int(y_valid.sum())\n",
    "    \n",
    "    ap = ap if pos_preds >= pos_labels else 0\n",
    "    \n",
    "    if f1 == 0 or precision == 0:\n",
    "        return {'loss': 0, 'precision': precision, 'f1_score': f1, 'ap': ap, 'auc': roc, 'pos_preds': pos_preds, 'status': STATUS_FAIL}\n",
    "    else:\n",
    "        return {'loss': -ap, 'precision': precision, 'f1_score': f1, 'ap': ap, 'auc': roc, 'pos_preds': pos_preds, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 126/400 [01:40<04:10,  1.09trial/s, best loss: -0.4217972539875966]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:864: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [04:12<00:00,  1.58trial/s, best loss: -0.4217972539875966]\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "\n",
    "best_hyperparams = fmin(\n",
    "    fn = optimization_objective,\n",
    "    space = hyperparameter_space,\n",
    "    algo = tpe.suggest,\n",
    "    max_evals = 400,\n",
    "    trials = trials\n",
    ")\n",
    "\n",
    "trial_vals = trials.vals\n",
    "trial_vals['f1_score'] = list(map(lambda x: x['f1_score'], trials.results))\n",
    "trial_vals['precision'] = list(map(lambda x: x['precision'], trials.results))\n",
    "trial_vals['ap'] = list(map(lambda x: x['ap'], trials.results))\n",
    "trial_vals['auc'] = list(map(lambda x: x['auc'], trials.results))\n",
    "trial_vals['pos_preds'] = list(map(lambda x: x['pos_preds'], trials.results))\n",
    "\n",
    "df_trials = pd.DataFrame.from_dict(trial_vals)\n",
    "df_trials.to_csv('data/final-check#5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.67072\tvalidation_1-logloss:0.66868\n",
      "[1]\tvalidation_0-logloss:0.64960\tvalidation_1-logloss:0.64802\n",
      "[2]\tvalidation_0-logloss:0.63276\tvalidation_1-logloss:0.63028\n",
      "[3]\tvalidation_0-logloss:0.61845\tvalidation_1-logloss:0.61519\n",
      "[4]\tvalidation_0-logloss:0.60578\tvalidation_1-logloss:0.60215\n",
      "[5]\tvalidation_0-logloss:0.59348\tvalidation_1-logloss:0.59045\n",
      "[6]\tvalidation_0-logloss:0.58441\tvalidation_1-logloss:0.58050\n",
      "[7]\tvalidation_0-logloss:0.57504\tvalidation_1-logloss:0.57164\n",
      "[8]\tvalidation_0-logloss:0.56745\tvalidation_1-logloss:0.56373\n",
      "[9]\tvalidation_0-logloss:0.56087\tvalidation_1-logloss:0.55700\n",
      "[10]\tvalidation_0-logloss:0.55783\tvalidation_1-logloss:0.55098\n",
      "[11]\tvalidation_0-logloss:0.55437\tvalidation_1-logloss:0.54550\n",
      "[12]\tvalidation_0-logloss:0.55005\tvalidation_1-logloss:0.54082\n",
      "[13]\tvalidation_0-logloss:0.54743\tvalidation_1-logloss:0.53649\n",
      "[14]\tvalidation_0-logloss:0.54466\tvalidation_1-logloss:0.53283\n",
      "[15]\tvalidation_0-logloss:0.54074\tvalidation_1-logloss:0.52937\n",
      "[16]\tvalidation_0-logloss:0.53927\tvalidation_1-logloss:0.52609\n",
      "[17]\tvalidation_0-logloss:0.53784\tvalidation_1-logloss:0.52319\n",
      "[18]\tvalidation_0-logloss:0.53523\tvalidation_1-logloss:0.52079\n",
      "[19]\tvalidation_0-logloss:0.53214\tvalidation_1-logloss:0.51842\n",
      "[20]\tvalidation_0-logloss:0.53030\tvalidation_1-logloss:0.51651\n",
      "[21]\tvalidation_0-logloss:0.52948\tvalidation_1-logloss:0.51459\n",
      "[22]\tvalidation_0-logloss:0.52790\tvalidation_1-logloss:0.51297\n",
      "[23]\tvalidation_0-logloss:0.52600\tvalidation_1-logloss:0.51152\n",
      "[24]\tvalidation_0-logloss:0.52465\tvalidation_1-logloss:0.51017\n",
      "[25]\tvalidation_0-logloss:0.52361\tvalidation_1-logloss:0.50896\n",
      "[26]\tvalidation_0-logloss:0.52327\tvalidation_1-logloss:0.50789\n",
      "[27]\tvalidation_0-logloss:0.52187\tvalidation_1-logloss:0.50685\n",
      "[28]\tvalidation_0-logloss:0.52106\tvalidation_1-logloss:0.50599\n",
      "[29]\tvalidation_0-logloss:0.51972\tvalidation_1-logloss:0.50515\n",
      "[30]\tvalidation_0-logloss:0.51938\tvalidation_1-logloss:0.50441\n",
      "[31]\tvalidation_0-logloss:0.51891\tvalidation_1-logloss:0.50365\n",
      "[32]\tvalidation_0-logloss:0.51890\tvalidation_1-logloss:0.50289\n",
      "[33]\tvalidation_0-logloss:0.51843\tvalidation_1-logloss:0.50233\n",
      "[34]\tvalidation_0-logloss:0.51808\tvalidation_1-logloss:0.50175\n",
      "[35]\tvalidation_0-logloss:0.51824\tvalidation_1-logloss:0.50083\n",
      "[36]\tvalidation_0-logloss:0.51860\tvalidation_1-logloss:0.50028\n",
      "[37]\tvalidation_0-logloss:0.52118\tvalidation_1-logloss:0.49965\n",
      "[38]\tvalidation_0-logloss:0.52016\tvalidation_1-logloss:0.49918\n",
      "[39]\tvalidation_0-logloss:0.51989\tvalidation_1-logloss:0.49880\n",
      "[40]\tvalidation_0-logloss:0.51945\tvalidation_1-logloss:0.49840\n",
      "[41]\tvalidation_0-logloss:0.52163\tvalidation_1-logloss:0.49800\n",
      "[42]\tvalidation_0-logloss:0.52154\tvalidation_1-logloss:0.49766\n",
      "[43]\tvalidation_0-logloss:0.52117\tvalidation_1-logloss:0.49710\n",
      "[44]\tvalidation_0-logloss:0.52141\tvalidation_1-logloss:0.49678\n",
      "[45]\tvalidation_0-logloss:0.52147\tvalidation_1-logloss:0.49649\n",
      "[46]\tvalidation_0-logloss:0.52087\tvalidation_1-logloss:0.49620\n",
      "[47]\tvalidation_0-logloss:0.52079\tvalidation_1-logloss:0.49592\n",
      "[48]\tvalidation_0-logloss:0.52005\tvalidation_1-logloss:0.49560\n",
      "[49]\tvalidation_0-logloss:0.51998\tvalidation_1-logloss:0.49538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=True,\n",
       "              eval_metric=['logloss'], gamma=2.695439, gpu_id=-1,\n",
       "              grow_policy='depthwise', importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.1, max_bin=256,\n",
       "              max_cat_to_onehot=1, max_delta_step=4, max_depth=4, max_leaves=0,\n",
       "              min_child_weight=8, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=50, n_jobs=0, num_parallel_tree=1, predictor='auto',\n",
       "              random_state=361398, reg_alpha=0, reg_lambda=1, ...)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = { \n",
    "    # reg def 0\n",
    "    \"gamma\": 2.695439,\n",
    "    # L2 def 1\n",
    "    # \"reg_lambda\" : 1,\n",
    "    # \"reg_alpha\" : 0,\n",
    "    # Class imbalance def 1\n",
    "    \"scale_pos_weight\" : 5.427564,\n",
    "    # Integers:\n",
    "    \"max_depth\": 4,\n",
    "    # Reg def 1\n",
    "    \"min_child_weight\" : 8,\n",
    "    # Class imbalance def 0\n",
    "    \"max_delta_step\" : 4,\n",
    "    # Choice:\n",
    "    \"colsample_bylevel\" : 1,\n",
    "    \"n_estimators\": 50,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"subsample\": 1,\n",
    "    # Fixed:\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"enable_categorical\": True,\n",
    "    \"max_cat_to_onehot\": 1,\n",
    "    \"eval_metric\": [\"logloss\"],\n",
    "    \"random_state\": np.random.randint(999929)\n",
    "}\n",
    "\n",
    "clf = xgb.XGBClassifier(\n",
    "        **params\n",
    "    )\n",
    "\n",
    "clf.fit(X_train, y_train, eval_set=[(X_valid, y_valid), (X_train, y_train)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Validation**\n",
      "Precision: 0.2536057692307692\n",
      "PR-AUC/AP score: 0.4066814800623557\n",
      "ROC-AUC score: 0.5758143038706992\n",
      "Total positive predictions: 832\n",
      "\n",
      "Totals:\n",
      "        prediction\n",
      "result            \n",
      "FN             706\n",
      "FP             621\n",
      "TN            5181\n",
      "TP             211\n",
      "\n",
      "Means:\n",
      "        return_one_month  return_two_month  return_three_month\n",
      "result                                                        \n",
      "FN              0.042707          0.113528            0.132232\n",
      "FP             -0.008454          0.004337            0.001369\n",
      "TN             -0.012911         -0.012254           -0.001703\n",
      "TP              0.053194          0.130005            0.144474\n",
      "\n",
      "Stds:\n",
      "        return_one_month  return_two_month  return_three_month\n",
      "result                                                        \n",
      "FN              0.046833          0.075181            0.068605\n",
      "FP              0.063445          0.069864            0.078964\n",
      "TN              0.048132          0.064799            0.067417\n",
      "TP              0.053686          0.063912            0.070619\n",
      "\n",
      "Positive predictions:\n",
      "\n",
      "Means:\n",
      "return_one_month      0.007180\n",
      "return_two_month      0.036207\n",
      "return_three_month    0.037661\n",
      "dtype: float64\n",
      "\n",
      "Stds:\n",
      "return_one_month      0.066722\n",
      "return_two_month      0.087561\n",
      "return_three_month    0.098962\n",
      "dtype: float64\n",
      "\n",
      " automobiles_and_components :\n",
      "Precision: 0.5\n",
      "PR-AUC/AP score: 1.0\n",
      "ROC-AUC score: 0.7941176470588236\n",
      "Total positive predictions: 2\n",
      "\n",
      " banks :\n",
      "Precision: 0.3333333333333333\n",
      "PR-AUC/AP score: 0\n",
      "ROC-AUC score: 0.5617130265567766\n",
      "Total positive predictions: 12\n",
      "\n",
      " capital_goods :\n",
      "Precision: 0.2\n",
      "PR-AUC/AP score: 0\n",
      "ROC-AUC score: 0.574113299589202\n",
      "Total positive predictions: 5\n",
      "\n",
      " chemicals :\n",
      "Precision: 0.3157894736842105\n",
      "PR-AUC/AP score: 0.16666666666666666\n",
      "ROC-AUC score: 0.6681818181818182\n",
      "Total positive predictions: 38\n",
      "\n",
      " commercial_and_professional_services :\n",
      "Precision: 0.0\n",
      "PR-AUC/AP score: 0\n",
      "ROC-AUC score: 0.4750000000000001\n",
      "Total positive predictions: 0\n",
      "\n",
      " consumer_durables_and_apparel :\n",
      "Precision: 0.0\n",
      "PR-AUC/AP score: 0\n",
      "ROC-AUC score: 0.2604166666666667\n",
      "Total positive predictions: 1\n",
      "\n",
      " consumer_services :\n",
      "Precision: 0.5\n",
      "PR-AUC/AP score: 0.5\n",
      "ROC-AUC score: 0.6818181818181819\n",
      "Total positive predictions: 16\n",
      "\n",
      " consumer_staples :\n",
      "Precision: 0.5\n",
      "PR-AUC/AP score: 0\n",
      "ROC-AUC score: 0.539951017909077\n",
      "Total positive predictions: 6\n",
      "\n",
      " containers_and_packaging :\n",
      "Precision: 1.0\n",
      "PR-AUC/AP score: 0\n",
      "ROC-AUC score: 1.0\n",
      "Total positive predictions: 1\n",
      "\n",
      " diversified_financials :\n",
      "Precision: 0.35294117647058826\n",
      "PR-AUC/AP score: 0\n",
      "ROC-AUC score: 0.6458306320323868\n",
      "Total positive predictions: 17\n",
      "\n",
      " energy :\n",
      "Precision: 0.0\n",
      "PR-AUC/AP score: 0\n",
      "ROC-AUC score: 0.549411162314388\n",
      "Total positive predictions: 0\n",
      "\n",
      " health_care_equipment_and_services :\n",
      "Precision: 0.06060606060606061\n",
      "PR-AUC/AP score: 0\n",
      "ROC-AUC score: 0.433753179120746\n",
      "Total positive predictions: 33\n",
      "\n",
      " insurance :\n",
      "Precision: 0.0\n",
      "PR-AUC/AP score: 0\n",
      "ROC-AUC score: 0.22865853658536586\n",
      "Total positive predictions: 0\n",
      "\n",
      " media_and_entertainment :\n",
      "Precision: 0.0\n",
      "PR-AUC/AP score: 0\n",
      "ROC-AUC score: 0.4835443037974684\n",
      "Total positive predictions: 0\n",
      "\n",
      " metals_and_mining :\n",
      "Precision: 0.3673469387755102\n",
      "PR-AUC/AP score: 0.7456709956709957\n",
      "ROC-AUC score: 0.6571637426900585\n",
      "Total positive predictions: 49\n",
      "\n",
      " pharmaceuticals_biotechnology_and_life_sciences :\n",
      "Precision: 0.14285714285714285\n",
      "PR-AUC/AP score: 0\n",
      "ROC-AUC score: 0.5939965483234714\n",
      "Total positive predictions: 21\n",
      "\n",
      " real_estate :\n",
      "Precision: 0.0\n",
      "PR-AUC/AP score: 0\n",
      "ROC-AUC score: 0.47475226649799707\n",
      "Total positive predictions: 0\n",
      "\n",
      " retailing :\n",
      "Precision: 0.25\n",
      "PR-AUC/AP score: 1.0\n",
      "ROC-AUC score: 0.4746093750000001\n",
      "Total positive predictions: 4\n",
      "\n",
      " semiconductors_and_semiconductor_equipment :\n",
      "Precision: 0.31736526946107785\n",
      "PR-AUC/AP score: 0.5304813888503368\n",
      "ROC-AUC score: 0.5931810658722277\n",
      "Total positive predictions: 167\n",
      "\n",
      " software_and_services :\n",
      "Precision: 0.1881720430107527\n",
      "PR-AUC/AP score: 0.4242880317835946\n",
      "ROC-AUC score: 0.6164871582435791\n",
      "Total positive predictions: 372\n",
      "\n",
      " technology_hardware_and_equipment :\n",
      "Precision: 0.3492063492063492\n",
      "PR-AUC/AP score: 0.3486402486402487\n",
      "ROC-AUC score: 0.6146926536731633\n",
      "Total positive predictions: 63\n",
      "\n",
      " telecommunication_services :\n",
      "Precision: 0.0\n",
      "PR-AUC/AP score: 0\n",
      "ROC-AUC score: 0.4431818181818182\n",
      "Total positive predictions: 0\n",
      "\n",
      " transportation :\n",
      "Precision: 0.20833333333333334\n",
      "PR-AUC/AP score: 0\n",
      "ROC-AUC score: 0.5018028846153847\n",
      "Total positive predictions: 24\n",
      "\n",
      " utilities :\n",
      "Precision: 1.0\n",
      "PR-AUC/AP score: 0\n",
      "ROC-AUC score: 0.5204545454545455\n",
      "Total positive predictions: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"**Validation**\")\n",
    "y_score = clf.predict_proba(X_valid)[:, 1]\n",
    "thres = 0.5\n",
    "y_preds = y_score > thres\n",
    "\n",
    "evaluate.performance_summary(y_score, y_preds, y_valid, auc_cutoff=0.55)\n",
    "\n",
    "df_results_valid = evaluate.returns_on_predictions(splits['validation'], y_preds)\n",
    "\n",
    "evaluate.performance_on_slice(splits['validation'], y_score, y_preds, 'subindustry', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results_valid[df_results_valid.result == 'FP'].iloc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_valid[df_results_valid.subindustry == 'consumer_services'].iloc[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.feature_names_in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trials = pd.read_csv('data/final-check#1.csv')\n",
    "df_trials.sort_values('ap', ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trials = pd.read_csv('data/sp500-check-no#1.csv')\n",
    "df_trials.sort_values('ap', ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trials = pd.read_csv('data/optimise-data-window-size-dynamic-cutoff_2_6_0#6.csv')\n",
    "df_trials.sort_values('ap', ascending=False).head(10)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
